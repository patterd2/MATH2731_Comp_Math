[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "",
    "text": "Introduction\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nWelcome to Computational Mathematics II!\nThis course aims to help you build skills and knowledge in using modern computational methods to do and apply mathematics. It will involve a blend of hands-on computing work and mathematical theory—this theory will include aspects of numerical analysis, computational algebra, and other topics within scientific computing. These areas consist of studying the mathematical properties of the computational representations of mathematical objects (numerical values as well as symbolic manipulations). The computing skills developed in this module will be valuable in all subsequent courses in your degree at Durham and well beyond. We will also introduce you to the use (and abuse) of various computational tools invaluable for doing mathematics, such as AI and searchable websites. While we will encourage you throughout to use all the tools at your disposal, it is imperative that you understand the details and scope of what you are doing! You will also develop your communication, presentation, and group-work skills through the various assessments involved in the course – more on that below!\nThis module has no final exam. In fact, there are no exams of any kind. Instead, the summative assessment and associated final grade are entirely based on coursework undertaken during the term. This means that you should expect to spend more time on this course during the term relative to your other modules. We believe this workload distribution is a better way to train the skills we are trying to develop, and as a bonus, you will not need to worry about this course any further once the term ends!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Content",
    "text": "Content\nThe module’s content is divided into six chapters of roughly equal length; some will focus slightly more on theory, while others have a more practical and hands-on nature.\n\nChapter 1: Introduction to Computational Mathematics\n\nProgramming basics (including GitHub, and numerical versus symbolic computation)\nLaTeX, Overleaf, and presenting lab reports\nFinite-precision arithmetic, rounding error, symbolic representations\n\nChapter 2: Continuous Functions\n\nInterpolation using polynomials – fitting curves to data (Lagrange polynomials, error estimates, convergence, and Chebyshev nodes)\nSolving nonlinear equations (bisection, fixed-point iteration, Newton’s method)\n\nChapter 3: Linear Algebra\n\nSolving linear systems numerically (LU decomposition, Gaussian elimination, conditioning) and symbolically\nApplications: PageRank, computer graphics\n\nChapter 4: Calculus\n\nNumerical differentiation (finite differences)\nNumerical integration (quadrature rules, Newton-Cotes formulae)\n\nChapter 5: Ordinary Differential Equations (ODEs)\n\nNumerically approximating solutions of ODEs\nTimestepping: explicit and implicit methods\nStability and convergence order\n\nChapter 6: Selected Further Topics\n\nIntro. to random numbers and stochastic processes\nIntro. to partial differential equations",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#weekly-workflow-and-summative-assessment",
    "href": "index.html#weekly-workflow-and-summative-assessment",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Weekly workflow and summative assessment",
    "text": "Weekly workflow and summative assessment\nThe final grade for this module is determined as follows:\n\nWeekly lab reports (weeks 1-6) – 20%\nWeekly e-assessments (weeks 1-6) – 30%\nProject (weeks 7-10) – 50%\n\n\nLab reports\nEach week for the first six weeks of the course, we will release a short set of exercises based on the lectures from the previous week. Students will be expected to submit a brief report (1-2 pages A4, including figures) with their solutions to the set of exercises – the report will consist of written answers and figures/plots. The reports will be evaluated for correctness and quality of the presentation and communication (quality of figures, clarity of argumentation, etc.).\nThe lab report for a given week will be due at noon on Monday of the following week (e.g., week one’s lab report is due on Monday of week two and so on). Solutions and generalised feedback will be provided to the class on common mistakes and issues arising in each report. Students can also seek detailed feedback on their submission from the lecturers during drop-in sessions and office hours. There will be six lab reports in total, and your mark is based on your four highest-scoring submissions.\n\n\nE-assessments\nEach week for the first six weeks of the course, we will release an e-assessment based on the lectures from the previous week. These exercises are designed to complement the lab reports by focusing exclusively on coding skills. The e-assessments will involve submitting code auto-marked by an online grading tool, and hence give immediate feedback. As with the lab reports, the e-assessment for a given week will be due at noon on Monday of the following week. There will be six e-assessments in total, and your mark is based on your four highest-scoring submissions.\n\n\nProject\nThe single largest component of the assessment for this module is the project. Weeks 7-10 of this course focus exclusively on project work with lectures ending in Week 6. We will be releasing more detailed instructions on the project submission format and assessment criteria separately, but briefly, the main aspects of the project are as follows:\n\nThere will be approximately eight different project options to choose from across different areas of mathematics (e.g., pure, applied, probability, mathematical physics, etc.); each project has a distinct member of the Maths Department as supervisor.\nStudents will submit their preferred project options (ranked choice preferences) in Week 4 of the term and be allocated to projects by the end of Week 6 (there are maximum subscription numbers for each option to ensure equity of supervision).\nEach project consists of two parts: a guided component that is completed as part of a small group and an extension component that is open-ended and completed as an individual. Group allocations will be done by the lecturers.\nEach group will jointly submit a five-page report for the guided component of the project, and this is worth 60% of the project grade.\nEach student will also submit a three-page report and a six-minute video presentation on their extension component. This submission is worth 40% of the project grade.\n\nIn Weeks 7-10 of the term, lectures will be replaced by project workshop sessions during which students can discuss their project with the designated supervisor. This will be an opportunity to discuss progress, ask questions, and seek clarification. Each student only needs to attend the one project drop-in weekly session relevant to their project. Computing drop-in sessions will continue as scheduled in the first six weeks to provide additional support for coding pertinent tasks for the projects – there will be two timetabled computing drop-ins per week and students are encouraged to attend at least one of them.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#lectures-computing-drop-ins-project-workshops",
    "href": "index.html#lectures-computing-drop-ins-project-workshops",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Lectures, computing drop-ins & project workshops",
    "text": "Lectures, computing drop-ins & project workshops\nLectures will primarily present, explain, and discuss new material (especially theory), but will also feature computer demonstrations of the algorithms and numerical methods. As such, students are encouraged to bring their laptops to lectures to run the examples themselves. Students must bring a laptop or device capable of running code to the computer drop-ins to work on the e-assessments and lab reports.\n\n\n\n\n\nActivities\nContent\n\n\n\n\nWeek 1\nIntroductory lecture, 2 lectures\nChapter 1\n\n\nWeek 2\n3 lectures, 1 computing drop-in\nChapter 2\n\n\nWeek 3\n3 lectures, 1 computing drop-in\nChapter 3\n\n\nWeek 4\n3 lectures, 1 computing drop-in\nChapter 4\n\n\nWeek 5\n3 lectures, 1 computing drop-in\nChapter 5\n\n\nWeek 6\n3 lectures, 1 computing drop-in\nChapter 5/6\n\n\nWeek 7\n0 lectures, 1 project workshop\nProject\n\n\nWeek 8\n0 lectures, 1 project workshop\nProject\n\n\nWeek 9\n0 lectures, 1 project workshop\nProject\n\n\nWeek 10\n0 lectures, 1 project workshop\nProject",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#contact-details-and-reading-materials",
    "href": "index.html#contact-details-and-reading-materials",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Contact details and Reading Materials",
    "text": "Contact details and Reading Materials\nIf you have questions or need clarification on any of the above, please speak to us during lectures, drop-in sessions, or office hours. Alternatively, email one or both of us at denis.d.patterson@durham.ac.uk or andrew.krause@durham.ac.uk.\nThe lecture notes are designed to be sufficient and self-contained. Hence, students do not need to purchase a textbook to complete the course successfully. References for additional reading will also be given at the end of each chapter.\nThe following texts may be useful supplementary references for students wishing to read further into topics from the course:\n\nBurden, R. L., & Faires, J. D. (1997). Numerical Analysis (6th ed.). Pacific Grove, CA: Brooks/Cole Publishing Company.\nSüli, E., & Mayers, D. F. (2003). An Introduction to Numerical Analysis. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chap-two.html",
    "href": "chap-two.html",
    "title": "2  Continuous Functions",
    "section": "",
    "text": "Polynomial Interpolation: Motivation\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nThe goal of this chapter is to explore and begin to answer the following question:\nIf \\(f\\) is a polynomial of degree \\(n\\), \\[\nf(x) = p_n(x) = a_0 + a_1x + \\ldots + a_nx^n,\n\\] then we only need to store the \\(n+1\\) coefficients \\(a_0,\\ldots,a_n\\). Operations such as taking the derivative or integrating \\(f\\) are also convenient. The idea in this chapter is to find a polynomial that approximates a general function \\(f\\). For a continuous function \\(f\\) on a bounded interval, this is always possible if you take a high enough degree polynomial:\nIf \\(f\\) is not continuous, then something other than a polynomial is required, since polynomials can’t handle asymptotic behaviour.\nIn this chapter, we look for a suitable polynomial \\(p_n\\) by interpolation—that is, requiring \\(p_n(x_i) = f(x_i)\\) at a finite set of points \\(x_i\\), usually called nodes. Sometimes we will also require the derivative(s) of \\(p_n\\) to match those of \\(f\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#content",
    "href": "chap-two.html#content",
    "title": "1  Continuous Functions",
    "section": "1.2 Content",
    "text": "1.2 Content\nThe module’s content is divided into six chapters of roughly equal length; some will focus slightly more on theory, while others have a more practical and hands-on nature.\n\nChapter 1: Introduction to Computational Mathematics\n\nProgramming basics (including GitHub, and numerical versus symbolic computation)\nLaTeX, Overleaf, and presenting lab reports\nFinite-precision arithmetic, rounding error, symbolic representations\n\nChapter 2: Continuous Functions\n\nInterpolation using polynomials – fitting curves to data (Lagrange polynomials, error estimates, convergence, and Chebyshev nodes)\nSolving nonlinear equations (bisection, fixed-point iteration, Newton’s method)\n\nChapter 3: Linear Algebra\n\nSolving linear systems numerically (LU decomposition, Gaussian elimination, conditioning) and symbolically\nApplications: PageRank, computer graphics\n\nChapter 4: Calculus\n\nNumerical differentiation (finite differences)\nNumerical integration (quadrature rules, Newton-Cotes formulae)\n\nChapter 5: Ordinary Differential Equations (ODEs)\n\nNumerically approximating solutions of ODEs\nTimestepping: explicit and implicit methods\nStability and convergence order\n\nChapter 6: Selected Further Topics\n\nIntro. to random numbers and stochastic processes\nIntro. to partial differential equations",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#weekly-workflow-and-summative-assessment",
    "href": "chap-two.html#weekly-workflow-and-summative-assessment",
    "title": "1  Continuous Functions",
    "section": "1.3 Weekly workflow and summative assessment",
    "text": "1.3 Weekly workflow and summative assessment\nThe final grade for this module is determined as follows:\n\nWeekly lab reports (weeks 1-6) – 20%\nWeekly e-assessments (weeks 1-6) – 30%\nProject (weeks 7-10) – 50%\n\n\n1.3.1 Lab reports\nEach week for the first six weeks of the course, we will release a short set of exercises based on the lectures from the previous week. Students will be expected to submit a brief report (1-2 pages A4, including figures) with their solutions to the set of exercises – the report will consist of written answers and figures/plots. The reports will be evaluated for correctness and quality of the presentation and communication (quality of figures, clarity of argumentation, etc.).\nThe lab report for a given week will be due at noon on Monday of the following week (e.g., week one’s lab report is due on Monday of week two and so on). Solutions and generalised feedback will be provided to the class on common mistakes and issues arising in each report. Students can also seek detailed feedback on their submission from the lecturers during drop-in sessions and office hours. There will be six lab reports in total, and your mark is based on your four highest-scoring submissions.\n\n\n1.3.2 E-assessments\nEach week for the first six weeks of the course, we will release an e-assessment based on the lectures from the previous week. These exercises are designed to complement the lab reports by focusing exclusively on coding skills. The e-assessments will involve submitting code auto-marked by an online grading tool, and hence give immediate feedback. As with the lab reports, the e-assessment for a given week will be due at noon on Monday of the following week. There will be six e-assessments in total, and your mark is based on your four highest-scoring submissions.\n\n\n1.3.3 Project\nThe single largest component of the assessment for this module is the project. Weeks 7-10 of this course focus exclusively on project work with lectures ending in Week 6. We will be releasing more detailed instructions on the project submission format and assessment criteria separately, but briefly, the main aspects of the project are as follows:\n\nThere will be approximately eight different project options to choose from across different areas of mathematics (e.g., pure, applied, probability, mathematical physics, etc.); each project has a distinct member of the Maths Department as supervisor.\nStudents will submit their preferred project options (ranked choice preferences) in Week 4 of the term and be allocated to projects by the end of Week 6 (there are maximum subscription numbers for each option to ensure equity of supervision).\nEach project consists of two parts: a guided component that is completed as part of a small group and an extension component that is open-ended and completed as an individual. Group allocations will be done by the lecturers.\nEach group will jointly submit a five-page report for the guided component of the project, and this is worth 60% of the project grade.\nEach student will also submit a three-page report and a six-minute video presentation on their extension component. This submission is worth 40% of the project grade.\n\nIn Weeks 7-10 of the term, lectures will be replaced by project workshop sessions during which students can discuss their project with the designated supervisor. This will be an opportunity to discuss progress, ask questions, and seek clarification. Each student only needs to attend the one project drop-in weekly session relevant to their project. Computing drop-in sessions will continue as scheduled in the first six weeks to provide additional support for coding pertinent tasks for the projects – there will be two timetabled computing drop-ins per week and students are encouraged to attend at least one of them.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#lectures-computing-drop-ins-project-workshops",
    "href": "chap-two.html#lectures-computing-drop-ins-project-workshops",
    "title": "1  Continuous Functions",
    "section": "1.4 Lectures, computing drop-ins & project workshops",
    "text": "1.4 Lectures, computing drop-ins & project workshops\nLectures will primarily present, explain, and discuss new material (especially theory), but will also feature computer demonstrations of the algorithms and numerical methods. As such, students are encouraged to bring their laptops to lectures to run the examples themselves. Students must bring a laptop or device capable of running code to the computer drop-ins to work on the e-assessments and lab reports.\n\n\n\n\n\nActivities\nContent\n\n\n\n\nWeek 1\nIntroductory lecture, 2 lectures\nChapter 1\n\n\nWeek 2\n3 lectures, 1 computing drop-in\nChapter 2\n\n\nWeek 3\n3 lectures, 1 computing drop-in\nChapter 3\n\n\nWeek 4\n3 lectures, 1 computing drop-in\nChapter 4\n\n\nWeek 5\n3 lectures, 1 computing drop-in\nChapter 5\n\n\nWeek 6\n3 lectures, 1 computing drop-in\nChapter 5/6\n\n\nWeek 7\n0 lectures, 1 project workshop\nProject\n\n\nWeek 8\n0 lectures, 1 project workshop\nProject\n\n\nWeek 9\n0 lectures, 1 project workshop\nProject\n\n\nWeek 10\n0 lectures, 1 project workshop\nProject",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#contact-details-and-reading-materials",
    "href": "chap-two.html#contact-details-and-reading-materials",
    "title": "1  Continuous Functions",
    "section": "1.5 Contact details and Reading Materials",
    "text": "1.5 Contact details and Reading Materials\nIf you have questions or need clarification on any of the above, please speak to us during lectures, drop-in sessions, or office hours. Alternatively, email one or both of us at denis.d.patterson@durham.ac.uk or andrew.krause@durham.ac.uk.\nThe lecture notes are designed to be sufficient and self-contained. Hence, students do not need to purchase a textbook to complete the course successfully. References for additional reading will also be given at the end of each chapter.\nThe following texts may be useful supplementary references for students wishing to read further into topics from the course:\n\nBurden, R. L., & Faires, J. D. (1997). Numerical Analysis (6th ed.). Pacific Grove, CA: Brooks/Cole Publishing Company.\nSüli, E., & Mayers, D. F. (2003). An Introduction to Numerical Analysis. Cambridge: Cambridge University Press.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#knowledge-checklist",
    "href": "chap-two.html#knowledge-checklist",
    "title": "2  Continuous Functions",
    "section": "Knowledge checklist",
    "text": "Knowledge checklist\nKey topics:\n\nPolynomial interpolation\n\nKey skills:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#polynomial-interpolation",
    "href": "chap-two.html#polynomial-interpolation",
    "title": "2  Continuous Functions",
    "section": "2.1 Polynomial interpolation",
    "text": "2.1 Polynomial interpolation\nThe classical problem of polynomial interpolation is to find a polynomial \\[\np_n(x) = a_0 + a_1x + \\ldots + a_n x^n = \\sum_{k=0}^n a_k x^k\n\\] that interpolates our function \\(f\\) at a finite set of nodes \\(\\{x_0, x_1, \\ldots, x_m\\}\\). In other words, \\(p_n(x_i)=f(x_i)\\) at each of the nodes \\(x_i\\). Since the polynomial has \\(n+1\\) unknown coefficients, we expect to need \\(n+1\\) distinct nodes, so let us assume that \\(m=n\\).\n\nHere we have two nodes \\(x_0\\), \\(x_1\\), and seek a polynomial \\(p_1(x) = a_0 + a_1x\\). Then the interpolation conditions require that \\[\n\\begin{cases}\np_1(x_0) = a_0 + a_1x_0 = f(x_0)\\\\\np_1(x_1) = a_0 + a_1x_1 = f(x_1)\n\\end{cases}\n\\implies\\quad\np_1(x) = \\frac{x_1f(x_0) - x_0f(x_1)}{x_1 - x_0} + \\frac{f(x_1) - f(x_0)}{x_1 - x_0}x.\n\\]\n\nFor general \\(n\\), the interpolation conditions require \\[\n\\begin{matrix}\na_0 &+ a_1x_0 &+ a_2x_0^2 &+ \\ldots &+ a_nx_0^n &= f(x_0),\\\\\na_0 &+ a_1x_1 &+ a_2x_1^2 &+ \\ldots &+ a_nx_1^n &= f(x_1),\\\\\n\\vdots  & \\vdots  & \\vdots     &        &\\vdots      & \\vdots\\\\\na_0 &+ a_1x_n &+ a_2x_n^2 &+ \\ldots &+ a_nx_n^n &= f(x_n),\n\\end{matrix}\n\\] so we have to solve \\[\n\\begin{pmatrix}\n1 & x_0 & x_0^2 & \\cdots & x_0^n\\\\\n1 & x_1 & x_1^2 & \\cdots & x_1^n\\\\\n\\vdots & \\vdots &\\vdots& & \\vdots\\\\\n1 & x_n & x_n^2 & \\cdots & x_n^n\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\na_0\\\\ a_1\\\\ \\vdots\\\\ a_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf(x_0)\\\\ f(x_1)\\\\ \\vdots\\\\ f(x_n)\n\\end{pmatrix}.\n\\] This is called a Vandermonde matrix. The determinant of this matrix is \\[\n\\det(A) = \\prod_{0\\leq i &lt; j\\leq n} (x_j - x_i),\n\\] which is non-zero provided the nodes are all distinct. This establishes an important result, where \\(\\mathcal{P}_n\\) denotes the space of all real polynomials of degree \\(\\leq n\\).\n\nTheorem 2.4: Existence/uniquenessGiven \\(n+1\\) distinct nodes \\(x_0, x_1, \\ldots, x_n\\), there is a unique polynomial \\(p_n\\in\\mathcal{P}_n\\) that interpolates \\(f(x)\\) at these nodes.\n\n\nWe may also prove uniqueness by the following elegant argument.\nProof (Uniqueness part of Existence/Uniqueness Theorem):\nSuppose that in addition to \\(p_n\\) there is another interpolating polynomial \\(q_n\\in\\mathcal{P}_n\\). Then the difference \\(r_n := p_n - q_n\\) is also a polynomial with degree \\(\\leq n\\). But we have \\[\nr_n(x_i) = p_n(x_i) - q_n(x_i) = f(x_i)-f(x_i)=0 \\quad \\textrm{for $i=0,\\ldots,n$},\n\\] so \\(r_n(x)\\) has \\(n+1\\) roots. From the Fundamental Theorem of Algebra, this is possible only if \\(r_n(x)\\equiv 0\\), which implies that \\(q_n=p_n\\).\n\n\n\n\n\n\nNote that the unique polynomial through \\(n+1\\) points may have degree \\(&lt; n\\). This happens when \\(a_0=0\\) in the solution to the Vandermonde system above.\n\n\n\n\nWe have \\(x_0=0\\), \\(x_1=\\tfrac{\\pi}{2}\\), \\(x_2=\\pi\\), so \\(f(x_0)=1\\), \\(f(x_1)=0\\), \\(f(x_2)=-1\\). Clearly the unique interpolant is a straight line \\(p_2(x) = 1 - \\tfrac2\\pi x\\).\nIf we took the nodes \\(\\{0,2\\pi,4\\pi\\}\\), we would get a constant function \\(p_2(x)=1\\).\n\n\n\n\n\n\nOne way to compute the interpolating polynomial would be to solve the Vandermonde system above, e.g. by Gaussian elimination. However, we will see (next term) that this is not recommended. In practice, we choose a different basis for \\(p_n\\). There are two common choices, due to Lagrange and Newton.\n\n\n\n\n\n\nThe Vandermonde matrix arises when we write \\(p_n\\) in the natural basis \\(\\{1,x,x^2,\\ldots\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#taylor-series",
    "href": "chap-two.html#taylor-series",
    "title": "2  Continuous Functions",
    "section": "Taylor series",
    "text": "Taylor series\nA truncated Taylor series is (in some sense) the simplest interpolating polynomial since it uses only a single node \\(x_0\\), although it does require \\(p_n\\) to match both \\(f\\) and some of its derivatives.\n\nWe can approximate this using a Taylor series about the point \\(x_0=0\\), which is \\[\n\\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\ldots.\n\\] This comes from writing \\[\nf(x) = a_0 + a_1(x-x_0) + a_2(x-x_0)^2 + \\ldots,\n\\] then differentiating term-by-term and matching values at \\(x_0\\): \\[\\begin{align*}\nf(x_0) &= a_0,\\\\\nf'(x_0) &= a_1,\\\\\nf''(x_0) &= 2a_2,\\\\\nf'''(x_0) &= 3(2)a_3,\\\\\n&\\vdots\\\\\n\\implies f(x) &= f(x_0) + f'(x_0)(x-x_0) + \\frac{f''(x_0)}{2!}(x-x_0)^2 + \\frac{f'''(x_0)}{3!}(x-x_0)^3 + \\ldots.\n\\end{align*}\\] So \\[\\begin{align*}\n\\textrm{1 term} \\;&\\implies\\; f(0.1) \\approx 0.1,\\\\\n\\textrm{2 terms} \\;&\\implies\\; f(0.1) \\approx 0.1 - \\frac{0.1^3}{6} = 0.099833\\ldots,\\\\\n\\textrm{3 terms} \\;&\\implies\\; f(0.1) \\approx 0.1 - \\frac{0.1^3}{6} + \\frac{0.1^5}{120} = 0.09983341\\ldots.\\\\\n\\end{align*}\\] The next term will be \\(-0.1^7/7! \\approx -10^{-7}/10^3 = -10^{-10}\\), which won’t change the answer to 6 s.f.\n\n\n\n\n\n\n\nThe exact answer is \\(\\sin(0.1)=0.09983341\\).\n\n\n\nMathematically, we can write the remainder as follows.\n\nTheorem 2.2: Taylor’s TheoremLet \\(f\\) be \\(n+1\\) times differentiable on \\((a,b)\\), and let \\(f^{(n)}\\) be continuous on \\([a,b]\\). If \\(x,x_0\\in[a,b]\\) then there exists \\(\\xi \\in (a,b)\\) such that \\[\nf(x) = \\sum_{k=0}^n\\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k \\; + \\; \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)^{n+1}.\n\\]\n\n\nThe sum is called the Taylor polynomial of degree \\(n\\), and the last term is called the Lagrange form of the remainder. Note that the unknown number \\(\\xi\\) depends on \\(x\\).\n\nFor \\(f(x)=\\sin(x)\\), we found the Taylor polynomial \\(p_6(x) = x - x^3/3! + x^5/5!\\), and \\(f^{(7)}(x)=-\\sin(x)\\). So we have \\[\n\\big|f(x) - p_6(x)\\big| = \\left|\\frac{f^{(7)}(\\xi)}{7!}(x-x_0)^7\\right|\n\\] for some \\(\\xi\\) between \\(x_0\\) and \\(x\\). For \\(x=0.1\\), we have \\[\n\\big|f(0.1) - p_6(0.1)\\big| = \\frac{1}{5040}(0.1)^7\\big|f^{(7)}(\\xi)\\big| \\quad \\textrm{for some $\\xi\\in[0,0.1]$}.\n\\] Since \\(\\big|f^{(7)}(\\xi)\\big| = \\big|\\sin(\\xi)\\big| \\leq 1\\), we can say, before calculating, that the error satisfies \\[\n\\big|f(0.1) - p_6(0.1)\\big| \\leq 1.984\\times 10^{-11}.\n\\]\n\n\n\n\n\n\n\nThe actual error is \\(1.983\\times 10^{-11}\\), so this is a tight estimate.\n\n\n\nSince this error arises from approximating \\(f\\) with a truncated series, rather than due to rounding, it is known as truncation error. Note that it tends to be lower if you use more terms (larger \\(n\\)), or if the function oscillates less (smaller \\(f^{(n+1)}\\) on the interval \\((x_0,x)\\)).\nError estimates like the Lagrange remainder play an important role in numerical analysis and computation, so it is important to understand where it comes from. The number \\(\\xi\\) will ultimately come from Rolle’s theorem, which is a special case of the mean value theorem from first-year calculus:\n\nTheorem 2.3: Rolle’s TheoremIf \\(f\\) is continuous on \\([a,b]\\) and differentiable on \\((a,b)\\), with \\(f(a)=f(b)=0\\), then there exists \\(\\xi\\in(a,b)\\) with \\(f'(\\xi)=0\\).\n\n\nProof of Lagrange remainder (Taylor’s Theorem)\nThe argument goes as follows:\n\nDefine the “auxiliary” function\n\\[\ng(t) = f(t) - p_n(t) - M(t-x_0)^{n+1},\n\\] where \\(p_n\\) is the Taylor polynomial. By construction, this function satisfies \\[\n\\begin{aligned}\ng(x_0) &= f(x_0) - p_n(x_0) - M(0)^{n+1} = 0,\\\\\ng'(x_0) &= f'(x_0) - p_n'(x_0) - (n+1)M(0)^{n} = 0,\\\\\ng''(x_0) &= f''(x_0) - p_n''(x_0) - n(n+1)M(0)^{n-1} = 0,\\\\\n&\\vdots\\\\\ng^{(n)}(x_0) &= f^{(n)}(x_0) - p_n^{(n)}(x_0) - (n+1)!M(0) = 0.\n\\end{aligned}\n\\]\nBy a suitable choice of \\(M\\), we can make \\(g(x)=0\\) too. Put\n\\[\nM = \\frac{f(x) - p_n(x)}{(x-x_0)^{n+1}},\n\\] then \\(g(x) = f(x) - p_n(x) - M(x-x_0)^{n+1} = 0\\).\nSince \\(g(x_0)=g(x)=0\\) and \\(x\\neq x_0\\), Rolle’s theorem implies that there exists \\(\\xi_0\\) between \\(x_0\\) and \\(x\\) such that \\(g'(\\xi_0)=0\\). But we already know that \\(g'(x_0)=0\\), so \\(g'\\) has two distinct roots and we can apply Rolle’s theorem again. Hence there exists \\(\\xi_1\\) between \\(x_0\\) and \\(\\xi_0\\) such that \\(g''(\\xi_1)=0\\). We can keep repeating this argument until we get \\(\\xi_{n+1}\\equiv \\xi\\) such that \\(g^{(n+1)}(\\xi)=0\\).\nWe can differentiate \\(g(t)\\) to see that\n\\[\ng^{(n+1)}(t) = f^{(n+1)}(t) - p_n^{(n+1)}(t) - M \\frac{\\mathrm{d}^{n+1}}{\\mathrm{d}t^{n+1}}\\big[(t-x_0)^{n+1}\\big] = f^{(n+1)}(t) - M(n+1)!\n\\] Substituting \\(\\xi\\) and our chosen \\(M\\) gives\n\\[\n0 = g^{(n+1)}(\\xi) = f^{(n+1)}(\\xi) - \\frac{f(x) - p_n(x)}{(x-x_0)^{n+1}}(n+1)!\n\\] which rearranges to give the formula in Taylor’s Theorem.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#polynomial-interpolation-motivation",
    "href": "chap-two.html#polynomial-interpolation-motivation",
    "title": "2  Continuous Functions",
    "section": "",
    "text": "Theorem 2.1: Weierstrass Approximation Theorem (1885)For any \\(f\\in C([0,1])\\) and any \\(\\epsilon &gt; 0\\), there exists a polynomial \\(p(x)\\) such that \\[\n\\max_{0\\leq x\\leq 1}\\big|f(x) - p(x)\\big| \\leq \\epsilon.\n\\]\n\n\n\n\n\n\n\n\nThis may be proved using an explicit sequence of polynomials, called Bernstein polynomials.\n\n\n\n\n\n\n\n\n\n\nTo approximate functions like \\(1/x\\), there is a well-developed theory of rational function interpolation, which is beyond the scope of this course.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#weierstrass-approximation-theorem-1885",
    "href": "chap-two.html#weierstrass-approximation-theorem-1885",
    "title": "1  Continuous Functions",
    "section": "1.1 Weierstrass Approximation Theorem (1885)",
    "text": "1.1 Weierstrass Approximation Theorem (1885)\nFor any \\(f\\in C([0,1])\\) and any \\(\\epsilon &gt; 0\\), there exists a polynomial \\(p(x)\\) such that \\[\n\\max_{0\\leq x\\leq 1}\\big|f(x) - p(x)\\big| \\leq \\epsilon.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-one.html",
    "href": "chap-one.html",
    "title": "1  Floating Point Arithmetic",
    "section": "",
    "text": "1.1 Fixed-point numbers\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nThe goal of this chapter is to explore and begin to answer the following question:\nIntegers can be represented exactly, up to some maximum size.\nBy contrast, only a subset of real numbers within any given interval can be represented exactly.\nIn everyday life, we tend to use a fixed point representation \\[\nx = \\pm (d_1d_2\\cdots d_{k-1}.d_k\\cdots d_n)_\\beta, \\quad \\textrm{where} \\quad d_1,\\ldots,d_n\\in\\{0,1,\\ldots,\\beta - 1\\}.\n\\] Here \\(\\beta\\) is the base (e.g. 10 for decimal arithmetic or 2 for binary).\nIf we require that \\(d_1\\neq 0\\) unless \\(k=2\\), then every number has a unique representation of this form, except for infinite trailing sequences of digits \\(\\beta - 1\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#fixed-point-numbers",
    "href": "chap-one.html#fixed-point-numbers",
    "title": "1  Floating Point Arithmetic",
    "section": "1.1 Fixed-point numbers",
    "text": "1.1 Fixed-point numbers\nIn everyday life, we tend to use a fixed point representation \\[\nx = \\pm (d_1d_2\\cdots d_{k-1}.d_k\\cdots d_n)_\\beta, \\quad \\textrm{where} \\quad d_1,\\ldots,d_n\\in\\{0,1,\\ldots,\\beta - 1\\}.\n\\] Here \\(\\beta\\) is the base (e.g. 10 for decimal arithmetic or 2 for binary).\n\n\n\nIf we require that \\(d_1\\neq 0\\) unless \\(k=2\\), then every number has a unique representation of this form, except for infinite trailing sequences of digits \\(\\beta - 1\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#floating-point-numbers",
    "href": "chap-one.html#floating-point-numbers",
    "title": "1  Floating Point Arithmetic",
    "section": "1.2 Floating-point numbers",
    "text": "1.2 Floating-point numbers\nComputers use a floating-point representation. Only numbers in a floating-point number system \\(F\\subset\\mathbb{R}\\) can be represented exactly, where \\[\nF = \\big\\{ \\pm (0.d_1d_2\\cdots d_{m})_\\beta\\beta^e \\;| \\;  \\beta, d_i, e \\in \\mathbb{Z}, \\;0 \\leq d_i \\leq \\beta-1, \\;e_{\\rm min} \\leq e \\leq e_{\\rm max}\\big\\}.\n\\] Here \\((0.d_1d_2\\cdots d_{m})_\\beta\\) is called the fraction (or significand or mantissa), \\(\\beta\\) is the base, and \\(e\\) is the exponent. This can represent a much larger range of numbers than a fixed-point system of the same size, although at the cost that the numbers are not equally spaced. If \\(d_1\\neq 0\\) then each number in \\(F\\) has a unique representation and \\(F\\) is called normalised.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that the spacing between numbers jumps by a factor \\(\\beta\\) at each power of \\(\\beta\\). The largest possible number is \\((0.111)_22^2 = (\\tfrac12 + \\tfrac14 + \\tfrac18)(4) = \\tfrac72\\). The smallest non-zero number is \\((0.100)_22^{-1}=\\tfrac12(\\tfrac12) = \\tfrac14\\).\n\n\n\n\nHere \\(\\beta=2\\), and there are 52 bits for the fraction, 11 for the exponent, and 1 for the sign. The actual format used is \\[\n\\pm (1.d_1\\cdots d_{52})_22^{e-1023} = \\pm (0.1d_1\\cdots d_{52})_22^{e-1022}, \\quad e = (e_1e_2\\cdots e_{11})_2.\n\\] When \\(\\beta=2\\), the first digit of a normalized number is always \\(1\\), so doesn’t need to be stored in memory. The exponent bias of 1022 means that the actual exponents are in the range \\(-1022\\) to \\(1025\\), since \\(e\\in[0,2047]\\). Actually the exponents \\(-1022\\) and \\(1025\\) are used to store \\(\\pm 0\\) and \\(\\pm\\infty\\) respectively.\nThe smallest non-zero number in this system is \\((0.1)_22^{-1021} \\approx 2.225\\times 10^{-308}\\), and the largest number is \\((0.1\\cdots 1)_22^{1024} \\approx 1.798\\times 10^{308}\\).\n\n\n\n\n\n\n\nIEEE stands for Institute of Electrical and Electronics Engineers. Matlab uses the IEEE 754 standard for floating point arithmetic. The automatic 1 is sometimes called the “hidden bit”. The exponent bias avoids the need to store the sign of the exponent.\n\n\n\nNumbers outside the finite set \\(F\\) cannot be represented exactly. If a calculation falls below the lower non-zero limit (in absolute value), it is called underflow, and usually set to 0. If it falls above the upper limit, it is called overflow, and usually results in a floating-point exception.\n\n\n\n\n\n\nAriane 5 rocket failure (1996): The maiden flight ended in failure. Only 40 seconds after initiation, at altitude 3700m, the launcher veered off course and exploded. The cause was a software exception during data conversion from a 64-bit float to a 16-bit integer. The converted number was too large to be represented, causing an exception.\n\n\n\n\n\n\n\n\n\nIn IEEE arithmetic, some numbers in the “zero gap” can be represented using \\(e=0\\), since only two possible fraction values are needed for \\(\\pm 0\\). The other fraction values may be used with first (hidden) bit 0 to store a set of so-called subnormal numbers.\n\n\n\nThe mapping from \\(\\mathbb{R}\\) to \\(F\\) is called rounding and denoted \\(\\mathrm{fl}(x)\\). Usually it is simply the nearest number in \\(F\\) to \\(x\\). If \\(x\\) lies exactly midway between two numbers in \\(F\\), a method of breaking ties is required. The IEEE standard specifies round to nearest even—i.e., take the neighbour with last digit 0 in the fraction.\n\n\n\n\n\n\nThis avoids statistical bias or prolonged drift.\n\n\n\n\n\n\n\n\n\n\\(\\tfrac98 = (1.001)_2\\) has neighbours \\(1 = (0.100)_22^1\\) and \\(\\tfrac54 = (0.101)_22^1\\), so is rounded down to \\(1\\).\n\\(\\tfrac{11}{8} = (1.011)_2\\) has neighbours \\(\\tfrac54 = (0.101)_22^1\\) and \\(\\tfrac32=(0.110)_22^1\\), so is rounded up to \\(\\tfrac32\\).\n\n\n\n\n\n\n\nVancouver stock exchange index: In 1982, the index was established at 1000. By November 1983, it had fallen to 520, even though the exchange seemed to be doing well. Explanation: the index was rounded down to 3 digits at every recomputation. Since the errors were always in the same direction, they added up to a large error over time. Upon recalculation, the index doubled!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#significant-figures",
    "href": "chap-one.html#significant-figures",
    "title": "1  Floating Point Arithmetic",
    "section": "1.3 Significant figures",
    "text": "1.3 Significant figures\nWhen doing calculations without a computer, we often use the terminology of significant figures. To count the number of significant figures in a number \\(x\\), start with the first non-zero digit from the left, and count all the digits thereafter, including final zeros if they are after the decimal point.\n\n\n\nTo round \\(x\\) to \\(n\\) s.f., replace \\(x\\) by the nearest number with \\(n\\) s.f. An approximation \\(\\hat{x}\\) of \\(x\\) is “correct to \\(n\\) s.f.” if both \\(\\hat{x}\\) and \\(x\\) round to the same number to \\(n\\) s.f.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#rounding-error",
    "href": "chap-one.html#rounding-error",
    "title": "1  Floating Point Arithmetic",
    "section": "1.4 Rounding error",
    "text": "1.4 Rounding error\nIf \\(|x|\\) lies between the smallest non-zero number in \\(F\\) and the largest number in \\(F\\), then \\[\n\\mathrm{fl}(x) = x(1+\\delta),\n\\] where the relative error incurred by rounding is \\[\n|\\delta| = \\frac{|\\mathrm{fl}(x) - x|}{|x|}.\n\\]\n\n\n\n\n\n\nRelative errors are often more useful because they are scale invariant. E.g., an error of 1 hour is irrelevant in estimating the age of this lecture theatre, but catastrophic in timing your arrival at the lecture.\n\n\n\nNow \\(x\\) may be written as \\(x=(0.d_1d_2\\cdots)_\\beta\\beta^e\\) for some \\(e\\in[e_{\\rm min},e_{\\rm max}]\\), but the fraction will not terminate after \\(m\\) digits if \\(x\\notin F\\). However, this fraction will differ from that of \\(\\mathrm{fl}(x)\\) by at most \\(\\tfrac12\\beta^{-m}\\), so \\[\n|\\mathrm{fl}(x) - x| \\leq \\tfrac12\\beta^{-m}\\beta^e \\quad \\implies \\quad |\\delta| \\leq \\tfrac12\\beta^{1-m}.\n\\] Here we used that the fractional part of \\(|x|\\) is at least \\((0.1)_\\beta \\equiv \\beta^{-1}\\). The number \\(\\epsilon_{\\rm M} = \\tfrac12\\beta^{1-m}\\) is called the machine epsilon (or unit roundoff), and is independent of \\(x\\). So the relative rounding error satisfies \\[\n|\\delta| \\leq \\epsilon_{\\rm M}.\n\\]\n\n\n\n\n\n\nTo check the machine epsilon value in Matlab you can just type ‘eps’ in the command line, which will return the value 2.2204e-16.\n\n\n\n\n\n\n\n\n\nThe name “unit roundoff” arises because \\(\\beta^{1-m}\\) is the distance between 1 and the next number in the system.\n\n\n\n\n\n\nWhen adding/subtracting/multiplying/dividing two numbers in \\(F\\), the result will not be in \\(F\\) in general, so must be rounded.\n\nLet us multiply \\(x=\\tfrac58\\) and \\(y=\\tfrac78\\). We have \\[\nxy = \\tfrac{35}{64} = \\tfrac12 + \\tfrac1{32} + \\tfrac1{64} = (0.100011)_2.\n\\] This has too many significant digits to represent in our system, so the best we can do is round the result to \\(\\mathrm{fl}(xy) = (0.100)_2 = \\tfrac12\\).\n\n\n\n\n\n\n\nTypically additional digits are used during the computation itself, as in our example.\n\n\n\nFor \\({\\circ} = +,-,\\times, \\div\\), IEEE standard arithmetic requires rounded exact operations, so that \\[\n\\mathrm{fl}(x {\\,\\circ\\,} y) = (x {\\,\\circ\\,} y)(1+\\delta), \\quad |\\delta|\\leq\\epsilon_{\\rm M}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#loss-of-significance",
    "href": "chap-one.html#loss-of-significance",
    "title": "1  Floating Point Arithmetic",
    "section": "1.5 Loss of significance",
    "text": "1.5 Loss of significance\nYou might think that the above guarantees the accuracy of calculations to within \\(\\epsilon_{\\rm M}\\), but this is true only if \\(x\\) and \\(y\\) are themselves exact. In reality, we are probably starting from \\(\\bar{x}=x(1+\\delta_1)\\) and \\(\\bar{y}=y(1 + \\delta_2)\\), with \\(|\\delta_1|, |\\delta_2| \\leq \\epsilon_{\\rm M}\\). In that case, there is an error even before we round the result, since \\[\n\\begin{aligned}\n\\bar{x} \\pm \\bar{y} &= x(1+ \\delta_1) \\pm y(1 + \\delta_2)\\\\\n&= (x\\pm y)\\left(1 + \\frac{x\\delta_1 \\pm y\\delta_2}{x\\pm y}\\right).\n\\end{aligned}\n\\] If the correct answer \\(x\\pm y\\) is very small, then there can be an arbitrarily large relative error in the result, compared to the errors in the initial \\(\\bar{x}\\) and \\(\\bar{y}\\). In particular, this relative error can be much larger than \\(\\epsilon_{\\rm M}\\). This is called loss of significance, and is a major cause of errors in floating-point calculations.\n\nTo 4 s.f., the roots are \\[\nx_1 = 28 + \\sqrt{783} = 55.98, \\quad x_2 = 28-\\sqrt{783} = 0.01786.\n\\] However, working to 4 s.f. we would compute \\(\\sqrt{783} = 27.98\\), which would lead to the results \\[\n\\bar{x}_1 = 55.98, \\quad \\bar{x}_2 = 0.02000.\n\\] The smaller root is not correct to 4 s.f., because of cancellation error. One way around this is to note that \\(x^2 - 56x + 1 = (x-x_1)(x-x_2)\\), and compute \\(x_2\\) from \\(x_2 = 1/x_1\\), which gives the correct answer.\n\n\n\n\n\n\n\nNote that the error crept in when we rounded \\(\\sqrt{783}\\) to \\(27.98\\), because this removed digits that would otherwise have been significant after the subtraction.\n\n\n\n\nLet us plot this function in the range \\(-5\\times 10^{-8}\\leq x \\leq 5\\times 10^{-8}\\) – even in IEEE double precision arithmetic we find significant errors, as shown by the blue curve:\n\n\n\n\n\nThe red curve shows the correct result approximated using the Taylor series \\[\n\\begin{aligned}\nf(x) &= \\left(1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\ldots\\right) - \\left( 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\ldots\\right) - x\\\\\n&\\approx x^2 + \\frac{x^3}{6}.\n\\end{aligned}\n\\] This avoids subtraction of nearly equal numbers.\n\n\n\n\n\n\n\nWe will look in more detail at polynomial approximations in the next section.\n\n\n\nNote that floating-point arithmetic violates many of the usual rules of real arithmetic, such as \\((a+b)+c = a + (b+c)\\).\n\n\\[\n\\begin{aligned}\n\\mathrm{fl}\\big[(5.9 + 5.5) + 0.4\\big] &= \\mathrm{fl}\\big[\\mathrm{fl}(11.4) + 0.4\\big] = \\mathrm{fl}(11.0 + 0.4) = 11.0,\\\\\n\\mathrm{fl}\\big[5.9 + (5.5 + 0.4)\\big] &= \\mathrm{fl}\\big[5.9 + 5.9 \\big] = \\mathrm{fl}(11.8) = 12.0.\n\\end{aligned}\n\\]\n\n\nIn \\(\\mathbb{R}\\), the average of two numbers always lies between the numbers. But if we work to 3 decimal digits, \\[\n\\mathrm{fl}\\left(\\frac{5.01 + 5.02}{2}\\right) = \\frac{\\mathrm{fl}(10.03)}{2} = \\frac{10.0}{2} = 5.0.\n\\]\n\nThe moral of the story is that sometimes care is needed to ensure that we carry out a calculation accurately and as intended!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#knowledge-checklist",
    "href": "chap-one.html#knowledge-checklist",
    "title": "1  Floating Point Arithmetic",
    "section": "Knowledge checklist",
    "text": "Knowledge checklist\nKey topics:\n\nInteger and floating point representations of real numbers on computers.\nOverflow, underflow and loss of significance.\n\nKey skills:\n\nUnderstanding and distinguishing integer, fixed-point, and floating-point representations.\nAnalyzing the effects of rounding and machine epsilon in calculations.\nDiagnosing and managing rounding errors, overflow, and underflow.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe are indebted to Prof. Anthony Yeates (Durham) who numerical analysis notes formed the basis of several chapters of the coures notes.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chap-one.html#s-float",
    "href": "chap-one.html#s-float",
    "title": "1  Floating Point Arithmetic",
    "section": "",
    "text": "How do we represent numbers on a computer?\n\n\n\nIf 1 bit (binary digit) is used to store the sign \\(\\pm\\), the largest possible number is \\[\n1\\times 2^{62} +1\\times 2^{61} + \\ldots + 1\\times 2^{1} + 1\\times 2^{0} = 2^{63}-1.\n\\]\n\n\n\n\n\n\n\nSome modern languages (such as Python) automatically promote large integers to arbitrary precision (“long”), but most statically-typed languages (C, Java, Matlab, etc.) do not; an overflow will occur and the type remains fixed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#floating-point-arithmetic",
    "href": "chap-one.html#floating-point-arithmetic",
    "title": "1  Floating Point Arithmetic",
    "section": "",
    "text": "How do we represent numbers on a computer?\n\n\n\nIf 1 bit (binary digit) is used to store the sign \\(\\pm\\), the largest possible number is \\[\n1\\times 2^{62} +1\\times 2^{61} + \\ldots + 1\\times 2^{1} + 1\\times 2^{0} = 2^{63}-1.\n\\]\n\n\n\n\n\n\n\nSome modern languages (such as Python) automatically promote large integers to arbitrary precision (“long”), but most statically-typed languages (C, Java, Matlab, etc.) do not; an overflow will occur and the type remains fixed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  }
]