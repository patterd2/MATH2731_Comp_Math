[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "",
    "text": "Introduction\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nWelcome to Computational Mathematics II!\nThis course aims to help you build skills and knowledge in using modern computational methods to do and apply mathematics. It will involve a blend of hands-on computing work and mathematical theory—this theory will include aspects of numerical analysis, computational algebra, and other topics within scientific computing. These areas consist of studying the mathematical properties of the computational representations of mathematical objects (numerical values as well as symbolic manipulations). The computing skills developed in this module will be valuable in all subsequent courses in your degree at Durham and well beyond. We will also introduce you to the use (and abuse) of various computational tools invaluable for doing mathematics, such as AI and searchable websites. While we will encourage you throughout to use all the tools at your disposal, it is imperative that you understand the details and scope of what you are doing! You will also develop your communication, presentation, and group-work skills through the various assessments involved in the course – more on that below!\nThis module has no final exam. In fact, there are no exams of any kind. Instead, the summative assessment and associated final grade are entirely based on coursework undertaken during the term. This means that you should expect to spend more time on this course during the term relative to your other modules. We believe this workload distribution is a better way to train the skills we are trying to develop, and as a bonus, you will not need to worry about this course any further once the term ends!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Content",
    "text": "Content\nThe module’s content is divided into six chapters of roughly equal length; some will focus slightly more on theory, while others have a more practical and hands-on nature.\n\nChapter 1: Introduction to Computational Mathematics\n\nProgramming basics (including GitHub, and numerical versus symbolic computation)\nLaTeX, Overleaf, and presenting lab reports\nFinite-precision arithmetic, rounding error, symbolic representations\n\nChapter 2: Continuous Functions\n\nInterpolation using polynomials – fitting curves to data (Lagrange polynomials, error estimates, convergence, and Chebyshev nodes)\nSolving nonlinear equations (bisection, fixed-point iteration, Newton’s method)\n\nChapter 3: Linear Algebra\n\nSolving linear systems numerically (LU decomposition, Gaussian elimination, conditioning) and symbolically\nApplications: PageRank, computer graphics\n\nChapter 4: Calculus\n\nNumerical differentiation (finite differences)\nNumerical integration (quadrature rules, Newton-Cotes formulae)\n\nChapter 5: Ordinary Differential Equations (ODEs)\n\nNumerically approximating solutions of ODEs\nTimestepping: explicit and implicit methods\nStability and convergence order\n\nChapter 6: Selected Further Topics\n\nIntro. to random numbers and stochastic processes\nIntro. to partial differential equations",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#weekly-workflow-and-summative-assessment",
    "href": "index.html#weekly-workflow-and-summative-assessment",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Weekly workflow and summative assessment",
    "text": "Weekly workflow and summative assessment\nThe final grade for this module is determined as follows:\n\nWeekly lab reports (weeks 1-6) – 20%\nWeekly e-assessments (weeks 1-6) – 30%\nProject (weeks 7-10) – 50%\n\n\nLab reports\nEach week for the first six weeks of the course, we will release a short set of exercises based on the lectures from the previous week. Students will be expected to submit a brief report (1-2 pages A4, including figures) with their solutions to the set of exercises – the report will consist of written answers and figures/plots. The reports will be evaluated for correctness and quality of the presentation and communication (quality of figures, clarity of argumentation, etc.).\nThe lab report for a given week will be due at noon on Monday of the following week (e.g., week one’s lab report is due on Monday of week two and so on). Solutions and generalised feedback will be provided to the class on common mistakes and issues arising in each report. Students can also seek detailed feedback on their submission from the lecturers during drop-in sessions and office hours. There will be six lab reports in total, and your mark is based on your four highest-scoring submissions.\n\n\nE-assessments\nEach week for the first six weeks of the course, we will release an e-assessment based on the lectures from the previous week. These exercises are designed to complement the lab reports by focusing exclusively on coding skills. The e-assessments will involve submitting code auto-marked by an online grading tool, and hence give immediate feedback. As with the lab reports, the e-assessment for a given week will be due at noon on Monday of the following week. There will be six e-assessments in total, and your mark is based on your four highest-scoring submissions.\n\n\nProject\nThe single largest component of the assessment for this module is the project. Weeks 7-10 of this course focus exclusively on project work with lectures ending in Week 6. We will be releasing more detailed instructions on the project submission format and assessment criteria separately, but briefly, the main aspects of the project are as follows:\n\nThere will be approximately eight different project options to choose from across different areas of mathematics (e.g., pure, applied, probability, mathematical physics, etc.); each project has a distinct member of the Maths Department as supervisor.\nStudents will submit their preferred project options (ranked choice preferences) in Week 4 of the term and be allocated to projects by the end of Week 6 (there are maximum subscription numbers for each option to ensure equity of supervision).\nEach project consists of two parts: a guided component that is completed as part of a small group and an extension component that is open-ended and completed as an individual. Group allocations will be done by the lecturers.\nEach group will jointly submit a five-page report for the guided component of the project, and this is worth 60% of the project grade.\nEach student will also submit a three-page report and a six-minute video presentation on their extension component. This submission is worth 40% of the project grade.\n\nIn Weeks 7-10 of the term, lectures will be replaced by project workshop sessions during which students can discuss their project with the designated supervisor. This will be an opportunity to discuss progress, ask questions, and seek clarification. Each student only needs to attend the one project drop-in weekly session relevant to their project. Computing drop-in sessions will continue as scheduled in the first six weeks to provide additional support for coding pertinent tasks for the projects – there will be two timetabled computing drop-ins per week and students are encouraged to attend at least one of them.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#lectures-computing-drop-ins-project-workshops",
    "href": "index.html#lectures-computing-drop-ins-project-workshops",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Lectures, computing drop-ins & project workshops",
    "text": "Lectures, computing drop-ins & project workshops\nLectures will primarily present, explain, and discuss new material (especially theory), but will also feature computer demonstrations of the algorithms and numerical methods. As such, students are encouraged to bring their laptops to lectures to run the examples themselves. Students must bring a laptop or device capable of running code to the computer drop-ins to work on the e-assessments and lab reports.\n\n\n\n\n\nActivities\nContent\n\n\n\n\nWeek 1\nIntroductory lecture, 2 lectures\nChapter 1\n\n\nWeek 2\n3 lectures, 1 computing drop-in\nChapter 2\n\n\nWeek 3\n3 lectures, 1 computing drop-in\nChapter 3\n\n\nWeek 4\n3 lectures, 1 computing drop-in\nChapter 4\n\n\nWeek 5\n3 lectures, 1 computing drop-in\nChapter 5\n\n\nWeek 6\n3 lectures, 1 computing drop-in\nChapter 5/6\n\n\nWeek 7\n0 lectures, 1 project workshop\nProject\n\n\nWeek 8\n0 lectures, 1 project workshop\nProject\n\n\nWeek 9\n0 lectures, 1 project workshop\nProject\n\n\nWeek 10\n0 lectures, 1 project workshop\nProject",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#contact-details-and-reading-materials",
    "href": "index.html#contact-details-and-reading-materials",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Contact details and Reading Materials",
    "text": "Contact details and Reading Materials\nIf you have questions or need clarification on any of the above, please speak to us during lectures, drop-in sessions, or office hours. Alternatively, email one or both of us at denis.d.patterson@durham.ac.uk or andrew.krause@durham.ac.uk.\nThe lecture notes are designed to be sufficient and self-contained. Hence, students do not need to purchase a textbook to complete the course successfully. References for additional reading will also be given at the end of each chapter.\nThe following texts may be useful supplementary references for students wishing to read further into topics from the course:\n\nBurden, R. L., & Faires, J. D. (1997). Numerical Analysis (6th ed.). Pacific Grove, CA: Brooks/Cole Publishing Company.\nSüli, E., & Mayers, D. F. (2003). An Introduction to Numerical Analysis. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe are indebted to Prof. Anthony Yeates (Durham) whose numerical analysis notes formed the basis of several chapters of the coures notes.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chap-one.html",
    "href": "chap-one.html",
    "title": "1  Floating Point Arithmetic",
    "section": "",
    "text": "1.1 Fixed-point numbers\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nThe goal of this chapter is to explore and begin to answer the following question:\nIntegers can be represented exactly, up to some maximum size.\nIn contrast to the integers, only a subset of real numbers within any given interval can be represented exactly.\nIn everyday life, we tend to use a fixed point representation \\[\nx = \\pm (d_1d_2\\cdots d_{k-1}.d_k\\cdots d_n)_\\beta, \\quad \\textrm{where} \\quad d_1,\\ldots,d_n\\in\\{0,1,\\ldots,\\beta - 1\\}.\n\\] Here \\(\\beta\\) is the base (e.g. 10 for decimal arithmetic or 2 for binary).\nIf we require that \\(d_1\\neq 0\\) unless \\(k=2\\), then every number has a unique representation of this form, except for infinite trailing sequences of digits \\(\\beta - 1\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#floating-point-numbers",
    "href": "chap-one.html#floating-point-numbers",
    "title": "1  Floating Point Arithmetic",
    "section": "1.2 Floating-point numbers",
    "text": "1.2 Floating-point numbers\nComputers use a floating-point representation. Only numbers in a floating-point number system \\(F\\subset\\mathbb{R}\\) can be represented exactly, where \\[\nF = \\big\\{ \\pm (0.d_1d_2\\cdots d_{m})_\\beta\\beta^e \\;| \\;  \\beta, d_i, e \\in \\mathbb{Z}, \\;0 \\leq d_i \\leq \\beta-1, \\;e_{\\rm min} \\leq e \\leq e_{\\rm max}\\big\\}.\n\\] Here \\((0.d_1d_2\\cdots d_{m})_\\beta\\) is called the fraction (or significand or mantissa), \\(\\beta\\) is the base, and \\(e\\) is the exponent. This can represent a much larger range of numbers than a fixed-point system of the same size, although at the cost that the numbers are not equally spaced. If \\(d_1\\neq 0\\) then each number in \\(F\\) has a unique representation and \\(F\\) is called normalised.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that the spacing between numbers jumps by a factor \\(\\beta\\) at each power of \\(\\beta\\). The largest possible number is \\((0.111)_22^2 = (\\tfrac12 + \\tfrac14 + \\tfrac18)(4) = \\tfrac72\\). The smallest non-zero number is \\((0.100)_22^{-1}=\\tfrac12(\\tfrac12) = \\tfrac14\\).\n\n\n\n\nHere \\(\\beta=2\\), and there are 52 bits for the fraction, 11 for the exponent, and 1 for the sign. The actual format used is \\[\n\\pm (1.d_1\\cdots d_{52})_22^{e-1023} = \\pm (0.1d_1\\cdots d_{52})_22^{e-1022}, \\quad e = (e_1e_2\\cdots e_{11})_2.\n\\] When \\(\\beta=2\\), the first digit of a normalized number is always \\(1\\), so doesn’t need to be stored in memory. The exponent bias of 1022 means that the actual exponents are in the range \\(-1022\\) to \\(1025\\), since \\(e\\in[0,2047]\\). Actually the exponents \\(-1022\\) and \\(1025\\) are used to store \\(\\pm 0\\) and \\(\\pm\\infty\\) respectively.\nThe smallest non-zero number in this system is \\((0.1)_22^{-1021} \\approx 2.225\\times 10^{-308}\\), and the largest number is \\((0.1\\cdots 1)_22^{1024} \\approx 1.798\\times 10^{308}\\).\n\n\n\n\n\n\n\nIEEE stands for Institute of Electrical and Electronics Engineers. Matlab uses the IEEE 754 standard for floating point arithmetic. The automatic 1 is sometimes called the “hidden bit”. The exponent bias avoids the need to store the sign of the exponent.\n\n\n\nNumbers outside the finite set \\(F\\) cannot be represented exactly. If a calculation falls below the lower non-zero limit (in absolute value), it is called underflow, and usually set to 0. If it falls above the upper limit, it is called overflow, and usually results in a floating-point exception.\n\n\n\n\n\n\nAriane 5 rocket failure (1996): The maiden flight ended in failure. Only 40 seconds after initiation, at altitude 3700m, the launcher veered off course and exploded. The cause was a software exception during data conversion from a 64-bit float to a 16-bit integer. The converted number was too large to be represented, causing an exception.\n\n\n\n\n\n\n\n\n\nIn IEEE arithmetic, some numbers in the “zero gap” can be represented using \\(e=0\\), since only two possible fraction values are needed for \\(\\pm 0\\). The other fraction values may be used with first (hidden) bit 0 to store a set of so-called subnormal numbers.\n\n\n\nThe mapping from \\(\\mathbb{R}\\) to \\(F\\) is called rounding and denoted \\(\\mathrm{fl}(x)\\). Usually it is simply the nearest number in \\(F\\) to \\(x\\). If \\(x\\) lies exactly midway between two numbers in \\(F\\), a method of breaking ties is required. The IEEE standard specifies round to nearest even—i.e., take the neighbour with last digit 0 in the fraction.\n\n\n\n\n\n\nThis avoids statistical bias or prolonged drift.\n\n\n\n\n\n\n\n\n\n\\(\\tfrac98 = (1.001)_2\\) has neighbours \\(1 = (0.100)_22^1\\) and \\(\\tfrac54 = (0.101)_22^1\\), so is rounded down to \\(1\\).\n\\(\\tfrac{11}{8} = (1.011)_2\\) has neighbours \\(\\tfrac54 = (0.101)_22^1\\) and \\(\\tfrac32=(0.110)_22^1\\), so is rounded up to \\(\\tfrac32\\).\n\n\n\n\n\n\n\nVancouver stock exchange index: In 1982, the index was established at 1000. By November 1983, it had fallen to 520, even though the exchange seemed to be doing well. Explanation: the index was rounded down to 3 digits at every recomputation. Since the errors were always in the same direction, they added up to a large error over time. Upon recalculation, the index doubled!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#significant-figures",
    "href": "chap-one.html#significant-figures",
    "title": "1  Floating Point Arithmetic",
    "section": "1.3 Significant figures",
    "text": "1.3 Significant figures\nWhen doing calculations without a computer, we often use the terminology of significant figures. To count the number of significant figures in a number \\(x\\), start with the first non-zero digit from the left, and count all the digits thereafter, including final zeros if they are after the decimal point.\n\n\n\nTo round \\(x\\) to \\(n\\) s.f., replace \\(x\\) by the nearest number with \\(n\\) s.f. An approximation \\(\\hat{x}\\) of \\(x\\) is “correct to \\(n\\) s.f.” if both \\(\\hat{x}\\) and \\(x\\) round to the same number to \\(n\\) s.f.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#rounding-error",
    "href": "chap-one.html#rounding-error",
    "title": "1  Floating Point Arithmetic",
    "section": "1.4 Rounding error",
    "text": "1.4 Rounding error\nIf \\(|x|\\) lies between the smallest non-zero number in \\(F\\) and the largest number in \\(F\\), then \\[\n\\mathrm{fl}(x) = x(1+\\delta),\n\\] where the relative error incurred by rounding is \\[\n|\\delta| = \\frac{|\\mathrm{fl}(x) - x|}{|x|}.\n\\]\n\n\n\n\n\n\nRelative errors are often more useful because they are scale invariant. E.g., an error of 1 hour is irrelevant in estimating the age of this lecture theatre, but catastrophic in timing your arrival at the lecture.\n\n\n\nNow \\(x\\) may be written as \\(x=(0.d_1d_2\\cdots)_\\beta\\beta^e\\) for some \\(e\\in[e_{\\rm min},e_{\\rm max}]\\), but the fraction will not terminate after \\(m\\) digits if \\(x\\notin F\\). However, this fraction will differ from that of \\(\\mathrm{fl}(x)\\) by at most \\(\\tfrac12\\beta^{-m}\\), so \\[\n|\\mathrm{fl}(x) - x| \\leq \\tfrac12\\beta^{-m}\\beta^e \\quad \\implies \\quad |\\delta| \\leq \\tfrac12\\beta^{1-m}.\n\\] Here we used that the fractional part of \\(|x|\\) is at least \\((0.1)_\\beta \\equiv \\beta^{-1}\\). The number \\(\\epsilon_{\\rm M} = \\tfrac12\\beta^{1-m}\\) is called the machine epsilon (or unit roundoff), and is independent of \\(x\\). So the relative rounding error satisfies \\[\n|\\delta| \\leq \\epsilon_{\\rm M}.\n\\]\n\n\n\n\n\n\nTo check the machine epsilon value in Matlab you can just type ‘eps’ in the command line, which will return the value 2.2204e-16.\n\n\n\n\n\n\n\n\n\nThe name “unit roundoff” arises because \\(\\beta^{1-m}\\) is the distance between 1 and the next number in the system.\n\n\n\n\n\n\nWhen adding/subtracting/multiplying/dividing two numbers in \\(F\\), the result will not be in \\(F\\) in general, so must be rounded.\n\nLet us multiply \\(x=\\tfrac58\\) and \\(y=\\tfrac78\\). We have \\[\nxy = \\tfrac{35}{64} = \\tfrac12 + \\tfrac1{32} + \\tfrac1{64} = (0.100011)_2.\n\\] This has too many significant digits to represent in our system, so the best we can do is round the result to \\(\\mathrm{fl}(xy) = (0.100)_2 = \\tfrac12\\).\n\n\n\n\n\n\n\nTypically additional digits are used during the computation itself, as in our example.\n\n\n\nFor \\({\\circ} = +,-,\\times, \\div\\), IEEE standard arithmetic requires rounded exact operations, so that \\[\n\\mathrm{fl}(x {\\,\\circ\\,} y) = (x {\\,\\circ\\,} y)(1+\\delta), \\quad |\\delta|\\leq\\epsilon_{\\rm M}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#loss-of-significance",
    "href": "chap-one.html#loss-of-significance",
    "title": "1  Floating Point Arithmetic",
    "section": "1.5 Loss of significance",
    "text": "1.5 Loss of significance\nYou might think that the above guarantees the accuracy of calculations to within \\(\\epsilon_{\\rm M}\\), but this is true only if \\(x\\) and \\(y\\) are themselves exact. In reality, we are probably starting from \\(\\bar{x}=x(1+\\delta_1)\\) and \\(\\bar{y}=y(1 + \\delta_2)\\), with \\(|\\delta_1|, |\\delta_2| \\leq \\epsilon_{\\rm M}\\). In that case, there is an error even before we round the result, since \\[\n\\begin{aligned}\n\\bar{x} \\pm \\bar{y} &= x(1+ \\delta_1) \\pm y(1 + \\delta_2)\\\\\n&= (x\\pm y)\\left(1 + \\frac{x\\delta_1 \\pm y\\delta_2}{x\\pm y}\\right).\n\\end{aligned}\n\\] If the correct answer \\(x\\pm y\\) is very small, then there can be an arbitrarily large relative error in the result, compared to the errors in the initial \\(\\bar{x}\\) and \\(\\bar{y}\\). In particular, this relative error can be much larger than \\(\\epsilon_{\\rm M}\\). This is called loss of significance, and is a major cause of errors in floating-point calculations.\n\nTo 4 s.f., the roots are \\[\nx_1 = 28 + \\sqrt{783} = 55.98, \\quad x_2 = 28-\\sqrt{783} = 0.01786.\n\\] However, working to 4 s.f. we would compute \\(\\sqrt{783} = 27.98\\), which would lead to the results \\[\n\\bar{x}_1 = 55.98, \\quad \\bar{x}_2 = 0.02000.\n\\] The smaller root is not correct to 4 s.f., because of cancellation error. One way around this is to note that \\(x^2 - 56x + 1 = (x-x_1)(x-x_2)\\), and compute \\(x_2\\) from \\(x_2 = 1/x_1\\), which gives the correct answer.\n\n\n\n\n\n\n\nNote that the error crept in when we rounded \\(\\sqrt{783}\\) to \\(27.98\\), because this removed digits that would otherwise have been significant after the subtraction.\n\n\n\n\nLet us plot this function in the range \\(-5\\times 10^{-8}\\leq x \\leq 5\\times 10^{-8}\\) – even in IEEE double precision arithmetic we find significant errors, as shown by the blue curve:\n\n\n\n\n\nThe red curve shows the correct result approximated using the Taylor series \\[\n\\begin{aligned}\nf(x) &= \\left(1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\ldots\\right) - \\left( 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\ldots\\right) - x\\\\\n&\\approx x^2 + \\frac{x^3}{6}.\n\\end{aligned}\n\\] This avoids subtraction of nearly equal numbers.\n\n\n\n\n\n\n\nWe will look in more detail at polynomial approximations in the next section.\n\n\n\nNote that floating-point arithmetic violates many of the usual rules of real arithmetic, such as \\((a+b)+c = a + (b+c)\\).\n\n\\[\n\\begin{aligned}\n\\mathrm{fl}\\big[(5.9 + 5.5) + 0.4\\big] &= \\mathrm{fl}\\big[\\mathrm{fl}(11.4) + 0.4\\big] = \\mathrm{fl}(11.0 + 0.4) = 11.0,\\\\\n\\mathrm{fl}\\big[5.9 + (5.5 + 0.4)\\big] &= \\mathrm{fl}\\big[5.9 + 5.9 \\big] = \\mathrm{fl}(11.8) = 12.0.\n\\end{aligned}\n\\]\n\n\nIn \\(\\mathbb{R}\\), the average of two numbers always lies between the numbers. But if we work to 3 decimal digits, \\[\n\\mathrm{fl}\\left(\\frac{5.01 + 5.02}{2}\\right) = \\frac{\\mathrm{fl}(10.03)}{2} = \\frac{10.0}{2} = 5.0.\n\\]\n\nThe moral of the story is that sometimes care is needed to ensure that we carry out a calculation accurately and as intended!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-one.html#knowledge-checklist",
    "href": "chap-one.html#knowledge-checklist",
    "title": "1  Floating Point Arithmetic",
    "section": "Knowledge checklist",
    "text": "Knowledge checklist\nKey topics:\n\nInteger and floating point representations of real numbers on computers.\nOverflow, underflow and loss of significance.\n\nKey skills:\n\nUnderstanding and distinguishing integer, fixed-point, and floating-point representations.\nAnalyzing the effects of rounding and machine epsilon in calculations.\nDiagnosing and managing rounding errors, overflow, and underflow.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Floating Point Arithmetic</span>"
    ]
  },
  {
    "objectID": "chap-two.html",
    "href": "chap-two.html",
    "title": "2  Continuous Functions",
    "section": "",
    "text": "2.1 Interpolation\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nThe goal of this chapter is to explore and begin to answer the following question:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#polynomial-interpolation-motivation",
    "href": "chap-two.html#polynomial-interpolation-motivation",
    "title": "2  Continuous Functions",
    "section": "3.1 Polynomial Interpolation: Motivation",
    "text": "3.1 Polynomial Interpolation: Motivation\nIf \\(f\\) is a polynomial of degree \\(n\\), \\[\nf(x) = p_n(x) = a_0 + a_1x + \\ldots + a_nx^n,\n\\] then we only need to store the \\(n+1\\) coefficients \\(a_0,\\ldots,a_n\\). Operations such as taking the derivative or integrating \\(f\\) are also convenient. The idea in this chapter is to find a polynomial that approximates a general function \\(f\\). For a continuous function \\(f\\) on a bounded interval, this is always possible if you take a high enough degree polynomial:\n\nTheorem 2.1: Weierstrass Approximation Theorem (1885)For any \\(f\\in C([0,1])\\) and any \\(\\epsilon &gt; 0\\), there exists a polynomial \\(p(x)\\) such that \\[\n\\max_{0\\leq x\\leq 1}\\big|f(x) - p(x)\\big| \\leq \\epsilon.\n\\]\n\n\n\n\n\n\n\n\nThis may be proved using an explicit sequence of polynomials, called Bernstein polynomials.\n\n\n\nIf \\(f\\) is not continuous, then something other than a polynomial is required, since polynomials can’t handle asymptotic behaviour.\n\n\n\n\n\n\nTo approximate functions like \\(1/x\\), there is a well-developed theory of rational function interpolation, which is beyond the scope of this course.\n\n\n\nIn this chapter, we look for a suitable polynomial \\(p_n\\) by interpolation—that is, requiring \\(p_n(x_i) = f(x_i)\\) at a finite set of points \\(x_i\\), usually called nodes. Sometimes we will also require the derivative(s) of \\(p_n\\) to match those of \\(f\\). This type of function approximation where we want to match values of the function that we know at particular points is very natural in many applications. For example, weather forecasts involve numerically solving huge systems of partial differential equations (PDEs), which means actually solving them on a discrete grid of points. If we want weather predictions between grid points, we must interpolate. Figure Figure 3.1 shows the spatial resolutions of a range of current and past weather models produced by the UK Met Office.\n\n\n\n\n\n\nFigure 3.1: Chart showing a range of weather models produce by the UK Met Office. Even the highest spatial resolution models have more than 1.5km between grid point due to computational constraints.\n\n\n\n\n3.1.1 Taylor series\nA truncated Taylor series is (in some sense) the simplest interpolating polynomial since it uses only a single node \\(x_0\\), although it does require \\(p_n\\) to match both \\(f\\) and some of its derivatives.\n\nWe can approximate this using a Taylor series about the point \\(x_0=0\\), which is \\[\n\\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\ldots.\n\\] This comes from writing \\[\nf(x) = a_0 + a_1(x-x_0) + a_2(x-x_0)^2 + \\ldots,\n\\] then differentiating term-by-term and matching values at \\(x_0\\): \\[\\begin{align*}\nf(x_0) &= a_0,\\\\\nf'(x_0) &= a_1,\\\\\nf''(x_0) &= 2a_2,\\\\\nf'''(x_0) &= 3(2)a_3,\\\\\n&\\vdots\\\\\n\\implies f(x) &= f(x_0) + f'(x_0)(x-x_0) + \\frac{f''(x_0)}{2!}(x-x_0)^2 + \\frac{f'''(x_0)}{3!}(x-x_0)^3 + \\ldots.\n\\end{align*}\\] So \\[\\begin{align*}\n\\textrm{1 term} \\;&\\implies\\; f(0.1) \\approx 0.1,\\\\\n\\textrm{2 terms} \\;&\\implies\\; f(0.1) \\approx 0.1 - \\frac{0.1^3}{6} = 0.099833\\ldots,\\\\\n\\textrm{3 terms} \\;&\\implies\\; f(0.1) \\approx 0.1 - \\frac{0.1^3}{6} + \\frac{0.1^5}{120} = 0.09983341\\ldots.\\\\\n\\end{align*}\\] The next term will be \\(-0.1^7/7! \\approx -10^{-7}/10^3 = -10^{-10}\\), which won’t change the answer to 6 s.f.\n\n\n\n\n\n\n\nThe exact answer is \\(\\sin(0.1)=0.09983341\\).\n\n\n\nMathematically, we can write the remainder as follows.\n\nTheorem 2.2: Taylor’s TheoremLet \\(f\\) be \\(n+1\\) times differentiable on \\((a,b)\\), and let \\(f^{(n)}\\) be continuous on \\([a,b]\\). If \\(x,x_0\\in[a,b]\\) then there exists \\(\\xi \\in (a,b)\\) such that \\[\nf(x) = \\sum_{k=0}^n\\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k \\; + \\; \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)^{n+1}.\n\\]\n\n\nThe sum is called the Taylor polynomial of degree \\(n\\), and the last term is called the Lagrange form of the remainder. Note that the unknown number \\(\\xi\\) depends on \\(x\\).\n\nFor \\(f(x)=\\sin(x)\\), we found the Taylor polynomial \\(p_6(x) = x - x^3/3! + x^5/5!\\), and \\(f^{(7)}(x)=-\\sin(x)\\). So we have \\[\n\\big|f(x) - p_6(x)\\big| = \\left|\\frac{f^{(7)}(\\xi)}{7!}(x-x_0)^7\\right|\n\\] for some \\(\\xi\\) between \\(x_0\\) and \\(x\\). For \\(x=0.1\\), we have \\[\n\\big|f(0.1) - p_6(0.1)\\big| = \\frac{1}{5040}(0.1)^7\\big|f^{(7)}(\\xi)\\big| \\quad \\textrm{for some $\\xi\\in[0,0.1]$}.\n\\] Since \\(\\big|f^{(7)}(\\xi)\\big| = \\big|\\sin(\\xi)\\big| \\leq 1\\), we can say, before calculating, that the error satisfies \\[\n\\big|f(0.1) - p_6(0.1)\\big| \\leq 1.984\\times 10^{-11}.\n\\]\n\n\n\n\n\n\n\nThe actual error is \\(1.983\\times 10^{-11}\\), so this is a tight estimate.\n\n\n\nSince this error arises from approximating \\(f\\) with a truncated series, rather than due to rounding, it is known as truncation error. Note that it tends to be lower if you use more terms (larger \\(n\\)), or if the function oscillates less (smaller \\(f^{(n+1)}\\) on the interval \\((x_0,x)\\)).\nError estimates like the Lagrange remainder play an important role in numerical analysis and computation, so it is important to understand where it comes from. The number \\(\\xi\\) will ultimately come from Rolle’s theorem, which is a special case of the mean value theorem from first-year calculus:\n\nTheorem 2.3: Rolle’s TheoremIf \\(f\\) is continuous on \\([a,b]\\) and differentiable on \\((a,b)\\), with \\(f(a)=f(b)=0\\), then there exists \\(\\xi\\in(a,b)\\) with \\(f'(\\xi)=0\\).\n\n\n\n\n\n\n\n\nNote that Rolle’s Theorem does not tell us what the value of \\(\\xi\\) might actually be, so in practice we must take some kind of worst case estimate to get an error bound, e.g. calculate the max value of \\(f'(\\xi)\\) over the range of possible \\(\\xi\\) values.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#taylor-series",
    "href": "chap-two.html#taylor-series",
    "title": "2  Continuous Functions",
    "section": "Taylor series",
    "text": "Taylor series\nA truncated Taylor series is (in some sense) the simplest interpolating polynomial since it uses only a single node \\(x_0\\), although it does require \\(p_n\\) to match both \\(f\\) and some of its derivatives.\n\nWe can approximate this using a Taylor series about the point \\(x_0=0\\), which is \\[\n\\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\ldots.\n\\] This comes from writing \\[\nf(x) = a_0 + a_1(x-x_0) + a_2(x-x_0)^2 + \\ldots,\n\\] then differentiating term-by-term and matching values at \\(x_0\\): \\[\\begin{align*}\nf(x_0) &= a_0,\\\\\nf'(x_0) &= a_1,\\\\\nf''(x_0) &= 2a_2,\\\\\nf'''(x_0) &= 3(2)a_3,\\\\\n&\\vdots\\\\\n\\implies f(x) &= f(x_0) + f'(x_0)(x-x_0) + \\frac{f''(x_0)}{2!}(x-x_0)^2 + \\frac{f'''(x_0)}{3!}(x-x_0)^3 + \\ldots.\n\\end{align*}\\] So \\[\\begin{align*}\n\\textrm{1 term} \\;&\\implies\\; f(0.1) \\approx 0.1,\\\\\n\\textrm{2 terms} \\;&\\implies\\; f(0.1) \\approx 0.1 - \\frac{0.1^3}{6} = 0.099833\\ldots,\\\\\n\\textrm{3 terms} \\;&\\implies\\; f(0.1) \\approx 0.1 - \\frac{0.1^3}{6} + \\frac{0.1^5}{120} = 0.09983341\\ldots.\\\\\n\\end{align*}\\] The next term will be \\(-0.1^7/7! \\approx -10^{-7}/10^3 = -10^{-10}\\), which won’t change the answer to 6 s.f.\n\n\n\n\n\n\n\nThe exact answer is \\(\\sin(0.1)=0.09983341\\).\n\n\n\nMathematically, we can write the remainder as follows.\n\nTheorem 2.2: Taylor’s TheoremLet \\(f\\) be \\(n+1\\) times differentiable on \\((a,b)\\), and let \\(f^{(n)}\\) be continuous on \\([a,b]\\). If \\(x,x_0\\in[a,b]\\) then there exists \\(\\xi \\in (a,b)\\) such that \\[\nf(x) = \\sum_{k=0}^n\\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k \\; + \\; \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)^{n+1}.\n\\]\n\n\nThe sum is called the Taylor polynomial of degree \\(n\\), and the last term is called the Lagrange form of the remainder. Note that the unknown number \\(\\xi\\) depends on \\(x\\).\n\nFor \\(f(x)=\\sin(x)\\), we found the Taylor polynomial \\(p_6(x) = x - x^3/3! + x^5/5!\\), and \\(f^{(7)}(x)=-\\sin(x)\\). So we have \\[\n\\big|f(x) - p_6(x)\\big| = \\left|\\frac{f^{(7)}(\\xi)}{7!}(x-x_0)^7\\right|\n\\] for some \\(\\xi\\) between \\(x_0\\) and \\(x\\). For \\(x=0.1\\), we have \\[\n\\big|f(0.1) - p_6(0.1)\\big| = \\frac{1}{5040}(0.1)^7\\big|f^{(7)}(\\xi)\\big| \\quad \\textrm{for some $\\xi\\in[0,0.1]$}.\n\\] Since \\(\\big|f^{(7)}(\\xi)\\big| = \\big|\\sin(\\xi)\\big| \\leq 1\\), we can say, before calculating, that the error satisfies \\[\n\\big|f(0.1) - p_6(0.1)\\big| \\leq 1.984\\times 10^{-11}.\n\\]\n\n\n\n\n\n\n\nThe actual error is \\(1.983\\times 10^{-11}\\), so this is a tight estimate.\n\n\n\nSince this error arises from approximating \\(f\\) with a truncated series, rather than due to rounding, it is known as truncation error. Note that it tends to be lower if you use more terms (larger \\(n\\)), or if the function oscillates less (smaller \\(f^{(n+1)}\\) on the interval \\((x_0,x)\\)).\nError estimates like the Lagrange remainder play an important role in numerical analysis and computation, so it is important to understand where it comes from. The number \\(\\xi\\) will ultimately come from Rolle’s theorem, which is a special case of the mean value theorem from first-year calculus:\n\nTheorem 2.3: Rolle’s TheoremIf \\(f\\) is continuous on \\([a,b]\\) and differentiable on \\((a,b)\\), with \\(f(a)=f(b)=0\\), then there exists \\(\\xi\\in(a,b)\\) with \\(f'(\\xi)=0\\).\n\n\n\n\n\n\n\n\nNote that Rolle’s Theorem does not tell us what the value of \\(\\xi\\) might actually be, so in practice we must take some kind of worst case estimate to get an error bound, e.g. calculate the max value of \\(f'(\\xi)\\) over the range of possible \\(\\xi\\) values.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#polynomial-interpolation",
    "href": "chap-two.html#polynomial-interpolation",
    "title": "2  Continuous Functions",
    "section": "3.2 Polynomial interpolation",
    "text": "3.2 Polynomial interpolation\nThe classical problem of polynomial interpolation is to find a polynomial \\[\np_n(x) = a_0 + a_1x + \\ldots + a_n x^n = \\sum_{k=0}^n a_k x^k\n\\] that interpolates our function \\(f\\) at a finite set of nodes \\(\\{x_0, x_1, \\ldots, x_m\\}\\). In other words, \\(p_n(x_i)=f(x_i)\\) at each of the nodes \\(x_i\\). Since the polynomial has \\(n+1\\) unknown coefficients, we expect to need \\(n+1\\) distinct nodes, so let us assume that \\(m=n\\).\n\nHere we have two nodes \\(x_0\\), \\(x_1\\), and seek a polynomial \\(p_1(x) = a_0 + a_1x\\). Then the interpolation conditions require that \\[\n\\begin{cases}\np_1(x_0) = a_0 + a_1x_0 = f(x_0)\\\\\np_1(x_1) = a_0 + a_1x_1 = f(x_1)\n\\end{cases}\n\\implies\\quad\np_1(x) = \\frac{x_1f(x_0) - x_0f(x_1)}{x_1 - x_0} + \\frac{f(x_1) - f(x_0)}{x_1 - x_0}x.\n\\]\n\nFor general \\(n\\), the interpolation conditions require \\[\n\\begin{matrix}\na_0 &+ a_1x_0 &+ a_2x_0^2 &+ \\ldots &+ a_nx_0^n &= f(x_0),\\\\\na_0 &+ a_1x_1 &+ a_2x_1^2 &+ \\ldots &+ a_nx_1^n &= f(x_1),\\\\\n\\vdots  & \\vdots  & \\vdots     &        &\\vdots      & \\vdots\\\\\na_0 &+ a_1x_n &+ a_2x_n^2 &+ \\ldots &+ a_nx_n^n &= f(x_n),\n\\end{matrix}\n\\] so we have to solve \\[\n\\begin{pmatrix}\n1 & x_0 & x_0^2 & \\cdots & x_0^n\\\\\n1 & x_1 & x_1^2 & \\cdots & x_1^n\\\\\n\\vdots & \\vdots &\\vdots& & \\vdots\\\\\n1 & x_n & x_n^2 & \\cdots & x_n^n\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\na_0\\\\ a_1\\\\ \\vdots\\\\ a_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf(x_0)\\\\ f(x_1)\\\\ \\vdots\\\\ f(x_n)\n\\end{pmatrix}.\n\\] This is called a Vandermonde matrix. The determinant of this matrix is \\[\n\\det(A) = \\prod_{0\\leq i &lt; j\\leq n} (x_j - x_i),\n\\] which is non-zero provided the nodes are all distinct. This establishes an important result, where \\(\\mathcal{P}_n\\) denotes the space of all real polynomials of degree \\(\\leq n\\).\n\nTheorem 2.4: Existence/uniquenessGiven \\(n+1\\) distinct nodes \\(x_0, x_1, \\ldots, x_n\\), there is a unique polynomial \\(p_n\\in\\mathcal{P}_n\\) that interpolates \\(f(x)\\) at these nodes.\n\n\nWe may also prove uniqueness by the following elegant argument.\nProof (Uniqueness part of Existence/Uniqueness Theorem):\nSuppose that in addition to \\(p_n\\) there is another interpolating polynomial \\(q_n\\in\\mathcal{P}_n\\). Then the difference \\(r_n := p_n - q_n\\) is also a polynomial with degree \\(\\leq n\\). But we have \\[\nr_n(x_i) = p_n(x_i) - q_n(x_i) = f(x_i)-f(x_i)=0 \\quad \\textrm{for $i=0,\\ldots,n$},\n\\] so \\(r_n(x)\\) has \\(n+1\\) roots. From the Fundamental Theorem of Algebra, this is possible only if \\(r_n(x)\\equiv 0\\), which implies that \\(q_n=p_n\\).\n\n\n\n\n\n\nNote that the unique polynomial through \\(n+1\\) points may have degree \\(&lt; n\\). This happens when \\(a_0=0\\) in the solution to the Vandermonde system above.\n\n\n\n\nWe have \\(x_0=0\\), \\(x_1=\\tfrac{\\pi}{2}\\), \\(x_2=\\pi\\), so \\(f(x_0)=1\\), \\(f(x_1)=0\\), \\(f(x_2)=-1\\). Clearly the unique interpolant is a straight line \\(p_2(x) = 1 - \\tfrac2\\pi x\\).\nIf we took the nodes \\(\\{0,2\\pi,4\\pi\\}\\), we would get a constant function \\(p_2(x)=1\\).\n\n\n\n\n\n\nOne way to compute the interpolating polynomial would be to solve the Vandermonde system above, e.g. by Gaussian elimination. However, this is not recommended. In practice, we choose a different basis for \\(p_n\\); there are two common and effective choices due to Lagrange and Newton.\n\n\n\n\n\n\nThe Vandermonde matrix arises when we write \\(p_n\\) in the natural basis \\(\\{1,x,x^2,\\ldots\\}\\), but we could also choose to work in some other basis…\n\n\n\n\n3.2.1 Lagrange Polynomials\nThis uses a special basis of polynomials \\(\\{\\ell_k\\}\\) in which the interpolation equations reduce to the identity matrix. In other words, the coefficients in this basis are just the function values, \\[\np_n(x) = \\sum_{k=0}^n f(x_k)\\ell_k(x).\n\\]\n\nExample 2.1: Linear interpolation again.We can re-write our linear interpolant to separate out the function values: \\[\np_1(x) = \\underbrace{\\frac{x - x_1}{x_0 - x_1}}_{\\ell_0(x)}f(x_0) + \\underbrace{\\frac{x-x_0}{x_1-x_0}}_{\\ell_1(x)}f(x_1).\n\\] Then \\(\\ell_0\\) and \\(\\ell_1\\) form the necessary basis. In particular, they have the property that \\[\n\\ell_0(x_i) = \\begin{cases}\n1 & \\textrm{if $i=0$},\\\\\n0 & \\textrm{if $i=1$},\n\\end{cases}\n\\qquad\n\\ell_1(x_i) = \\begin{cases}\n0 & \\textrm{if $i=0$},\\\\\n1 & \\textrm{if $i=1$},\n\\end{cases}\n\\]\n\n\nFor general \\(n\\), the \\(n+1\\) Lagrange polynomials are defined as a product \\[\n\\ell_k(x) = \\prod_{\\substack{j=0\\\\j\\neq k}}^n\\frac{x - x_j}{x_k - x_j}.\n\\] By construction, they have the property that \\[\n\\ell_k(x_i) = \\begin{cases}\n1 & \\textrm{if $i=k$},\\\\\n0 & \\textrm{otherwise}.\n\\end{cases}\n\\] From this, it follows that the interpolating polynomial may be written as above.\n\n\n\n\n\n\nBy the Existence/Uniqueness Theorem, the Lagrange polynomials are the unique polynomials with this property.\n\n\n\n\nExample 2.2: Compute the quadratic interpolating polynomial to \\(f(x)=\\cos(x)\\) with nodes \\(\\{-\\tfrac\\pi4, 0, \\tfrac\\pi4\\}\\) using Lagrange polynomials.The Lagrange polynomials of degree 2 for these nodes are \\[\n\\begin{aligned}\n\\ell_0(x) &= \\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)} = \\frac{x(x-\\tfrac\\pi4)}{\\tfrac\\pi4\\cdot\\tfrac\\pi2},\\\\\n\\ell_1(x) &= \\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)} = \\frac{(x+\\tfrac\\pi4)(x-\\tfrac\\pi4)}{-\\tfrac\\pi4\\cdot\\tfrac\\pi4},\\\\\n\\ell_2(x) &= \\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}\\\\\n&= \\frac{x(x+\\tfrac\\pi4)}{\\tfrac\\pi2\\cdot\\tfrac\\pi4}.\n\\end{aligned}\n\\] So the interpolating polynomial is \\[\n\\begin{aligned}\np_2(x) &= f(x_0)\\ell_0(x) + f(x_1)\\ell_1(x) + f(x_2)\\ell_2(x)\\\\\n&= \\tfrac{1}{\\sqrt{2}}\\tfrac8{\\pi^2}x(x-\\tfrac\\pi4) - \\tfrac{16}{\\pi^2}(x+\\tfrac\\pi4)(x-\\tfrac\\pi4) +\\tfrac{1}{\\sqrt{2}}\\tfrac8{\\pi^2}x(x+\\tfrac\\pi4) = \\tfrac{16}{\\pi^2}\\big(\\tfrac{1}{\\sqrt{2}} - 1\\big)x^2 + 1.\n\\end{aligned}\n\\] The Lagrange polynomials and the resulting interpolant are shown below:\n \n\n\n\n\n\n\n\n\nLagrange polynomials were actually discovered by Edward Waring in 1776 and rediscovered by Euler in 1783, before they were published by Lagrange himself in 1795; a classic example of Stigler’s law of eponymy!\n\n\n\nThe Lagrange form of the interpolating polynomial is easy to write down, but expensive to evaluate since all of the \\(\\ell_k\\) must be computed. Moreover, changing any of the nodes means that the \\(\\ell_k\\) must all be recomputed from scratch, and similarly for adding a new node (moving to higher degree).\n\n\n3.2.2 Newton/Divided-Difference Polynomials\nIt would be easy to increase the degree of \\(p_n\\) if \\[\np_{n+1}(x) = p_{n}(x) + g_{n+1}(x), \\quad \\textrm{where $g_{n+1}\\in{\\cal P}_{n+1}$}.\n\\] From the interpolation conditions, we know that \\[\ng_{n+1}(x_i) = p_{n+1}(x_i) - p_{n}(x_i) = f(x_i)-f(x_i)=0 \\quad \\textrm{for $i=0,\\ldots,n$},\n\\] so \\[\ng_{n+1}(x) = a_{n+1}(x-x_0)\\cdots(x-x_{n}).\n\\] The coefficient \\(a_{n+1}\\) is determined by the remaining interpolation condition at \\(x_{n+1}\\), so \\[\np_n(x_{n+1}) + g_{n+1}(x_{n+1}) = f(x_{n+1}) \\quad \\implies \\quad a_{n+1} = \\frac{f(x_{n+1}) - p_{n}(x_{n+1})}{(x_{n+1}-x_0)\\cdots(x_{n+1}-x_{n})}.\n\\]\nThe polynomial \\((x-x_0)(x-x_1)\\cdots(x-x_{n})\\) is called a Newton polynomial. These form a new basis \\[\nn_0(x)=1, \\qquad n_k(x) = \\prod_{j=0}^{k-1}(x-x_j) \\quad \\textrm{for $k&gt;0$}.\n\\]\nThe Newton form of the interpolating polynomial is then \\[\np_n(x) = \\sum_{k=0}^n a_kn_k(x), \\qquad a_0 = f(x_0),\\qquad a_k = \\frac{f(x_k) - p_{k-1}(x_k)}{(x_k-x_0)\\cdots(x_k-x_{k-1})} \\textrm{ for $k&gt;0$}.\n\\] Notice that \\(a_k\\) depends only on \\(x_0,\\ldots x_k\\), so we can construct first \\(a_0\\), then \\(a_1\\), etc.\nIt turns out that the \\(a_k\\) are easy to compute, but it will take a little work to derive the method. We define the divided difference \\(f[x_0,x_1,\\ldots,x_k]\\) to be the coefficient of \\(x^k\\) in the polynomial interpolating \\(f\\) at nodes \\(x_0,\\ldots,x_k\\). It follows that \\[\nf[x_0,x_1,\\ldots,x_k] = a_k,\n\\] where \\(a_k\\) is the coefficient in the Newton form above.\n\nExample 2.3: Compute the Newton interpolating polynomial at two nodes.\\[\n\\begin{aligned}\nf[x_0] &= a_0 = f(x_0),\\\\\nf[x_0,x_1] &= a_1 = \\frac{f(x_1) - p_0(x_1)}{x_1 - x_0} = \\frac{f(x_1)-a_0}{x_1 - x_0} = \\frac{f[x_1]-f[x_0]}{x_1 - x_0}.\n\\end{aligned}\n\\] So the first-order divided difference \\(f[x_0,x_1]\\) is obtained from the zeroth-order differences \\(f[x_0]\\), \\(f[x_1]\\) by subtracting and dividing, hence the name “divided difference”.\n\n\n\nExample 2.4: Compute the Newton interpolating polynomial at three nodes.Continuing from the previous example, we find \\[\n\\begin{aligned}\nf[x_0,x_1,x_2] &= a_2 = \\frac{f(x_2) - p_1(x_2)}{(x_2 - x_0)(x_2-x_1)} = \\frac{f(x_2) - a_0 - a_1(x_2-x_0)}{(x_2 - x_0)(x_2-x_1)}\\\\\n& = \\ldots = \\frac{1}{x_2-x_0}\\left(\\frac{f[x_2]-f[x_1]}{x_2-x_1} - \\frac{f[x_1]-f[x_0]}{x_1-x_0}\\right)\\\\\n&= \\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0}.\n\\end{aligned}\n\\] So again, we subtract and divide.\n\n\nIn general, we have the following.\n\nTheorem 2.5For \\(k&gt;0\\), the divided differences satisfy \\[\nf[x_i,x_{i+1},\\ldots,x_{i+k}] = \\frac{f[x_{i+1},\\ldots,x_{i+k}] - f[x_i,\\ldots,x_{i+k-1}]}{x_{i+k} - x_i}.\n\\]\n\n\nProof:\nWithout loss of generality, we relabel the nodes so that \\(i=0\\). So we want to prove that \\[\nf[x_0,x_1,\\ldots,x_{k}] = \\frac{f[x_1,\\ldots,x_{k}] - f[x_0,\\ldots,x_{k-1}]}{x_{k} - x_0}.\n\\] The trick is to write the interpolant with nodes \\(x_0, \\ldots, x_k\\) in the form \\[\np_k(x) = \\frac{(x_k-x)q_{k-1}(x) + (x-x_0)\\tilde{q}_{k-1}(x)}{x_k-x_0},\n\\] where \\(q_{k-1}\\in{\\cal P}_{k-1}\\) interpolates \\(f\\) at the subset of nodes \\(x_0, x_1, \\ldots, x_{k-1}\\) and \\(\\tilde{q}_{k-1}\\in{\\cal P}_{k-1}\\) interpolates \\(f\\) at the subset \\(x_1, x_2,\\ldots,x_k\\). If this holds, then matching the coefficient of \\(x^k\\) on each side will give the divided difference formula, since, e.g., the leading coefficient of \\(q_{k-1}\\) is \\(f[x_0,\\ldots,x_{k-1}]\\). To see that \\(p_k\\) may really be written this way, note that \\[\n\\begin{aligned}\np_k(x_0) &= q_{k-1}(x_0) = f(x_0),\\\\\np_k(x_k) &= \\tilde{q}_{k-1}(x_k) = f(x_k),\\\\\np_k(x_i) &= \\frac{(x_k-x_i)q_{k-1}(x_i) + (x_i-x_0)\\tilde{q}_{k-1}(x_i)}{x_k - x_0} = f(x_i) \\quad \\textrm{for $i=1,\\ldots,k-1$}.\n\\end{aligned}\n\\] Since \\(p_k\\) agrees with \\(f\\) at the \\(k+1\\) nodes, it is the unique interpolant in \\({\\cal P}_k\\).\nTheorem above gives us our convenient method, which is to construct a divided-difference table.\n\nExample 2.5: Construct the Newton polynomial at the nodes \\(\\{-1,0,1,2\\}\\) and with corresponding function values \\(\\{5,1,1,11\\}\\)We construct a divided-difference table as follows. \\[\n\\begin{matrix}\n&x_0=-1 \\quad &f[x_0]=5 & & &\\\\\n& & &f[x_0,x_1]=-4 & &\\\\\n&x_1=0 \\quad &f[x_1]=1 & &f[x_0,x_1,x_2]=2 &\\\\\n& & &f[x_1,x_2]=0 & &f[x_0,x_1,x_2,x_3]=1\\\\\n&x_2=1 \\quad &f[x_2]=1 & &f[x_1,x_2,x_3]=5 &\\\\\n& & &f[x_2,x_3]=10 & &\\\\\n&x_3=2 \\quad &f[x_3]=11 & & &\\\\\n\\end{matrix}\n\\] The coefficients of the \\(p_3\\) lie at the top of each column, so \\[\n\\begin{aligned}\np_3(x) &= f[x_0] + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1)\\\\\n& + f[x_0,x_1,x_2,x_3](x-x_0)(x-x_1)(x-x_2)\\\\\n&=5 - 4(x+1) + 2x(x+1) + x(x+1)(x-1).\n\\end{aligned}\n\\] Now suppose we add the extra nodes \\(\\{-2,3\\}\\) with data \\(\\{5,35\\}\\). All we need to do to compute \\(p_5\\) is add two rows to the bottom of the table — there is no need to recalculate the rest. This gives \\[\n\\begin{matrix}\n&-1  &5 & & & & &\\\\\n& & &-4 & & & &\\\\\n&0  &1 & &2 & & &\\\\\n& & &0 & &1 & &\\\\\n&1  &1 & &5 & &\\textcolor{red}{-\\tfrac1{12}} &\\\\\n& & &10 & &\\textcolor{red}{\\tfrac{13}{12}} & &\\textcolor{red}{0}\\\\\n&2 &11 & &\\textcolor{red}{\\tfrac{17}{6}} & &\\textcolor{red}{-\\tfrac1{12}}\\\\\n& & &\\textcolor{red}{\\tfrac{3}{2}} & &\\textcolor{red}{\\tfrac{5}{6}} & &\\\\\n&\\textcolor{red}{-2} &\\textcolor{red}{5} & &\\textcolor{red}{\\tfrac92} & &&\\\\\n& & &\\textcolor{red}{6} & & & &\\\\\n&\\textcolor{red}{3}  &\\textcolor{red}{35} & & & & &\\\\\n\\end{matrix}\n\\] The new interpolating polynomial is \\[\np_5(x) = p_3(x) - \\tfrac{1}{12}x(x+1)(x-1)(x-2).\n\\]\n\n\n\n\n\n\n\n\nNotice that the \\(x^5\\) coefficient vanishes for these particular data, meaning that they are consistent with \\(f\\in{\\cal P}_4\\).\n\n\n\n\n\n\n\n\n\nNote that the value of \\(f[x_0,x_1,\\ldots,x_k]\\) is independent of the order of the nodes in the table. This follows from the uniqueness of \\(p_k\\).\n\n\n\nDivided differences are actually approximations for derivatives of \\(f\\). In the limit that the nodes all coincide, the Newton form of \\(p_n(x)\\) becomes the Taylor polynomial.\n\n\n3.2.3 Interpolation Error\nThe goal here is to estimate the error \\(|f(x)-p_n(x)|\\) when we approximate a function \\(f\\) by a polynomial interpolant \\(p_n\\). Clearly this will depend on \\(x\\).\n\nExample 2.6: Quadratic interpolant for \\(f(x)=\\cos(x)\\) with \\(\\{-\\tfrac\\pi4,0,\\tfrac\\pi4\\}\\).From Section 3.2.1, we have \\(p_2(x) = \\tfrac{16}{\\pi^2}\\left(\\tfrac{1}{\\sqrt{2}}-1\\right)x^2 + 1\\), so the error is \\[\n|f(x) - p_2(x)| = \\left|\\cos(x) -  \\tfrac{16}{\\pi^2}\\left(\\tfrac{1}{\\sqrt{2}}-1\\right)x^2 - 1\\right|.\n\\] This is shown below:\n\n\n\n\n\nClearly the error vanishes at the nodes themselves, but note that it generally does better near the middle of the set of nodes — this is quite typical behaviour.\n\n\nWe can adapt the proof of Taylor’s theorem to get a quantitative error estimate.\n\nTheorem 2.6: Cauchy’s Interpolation Error TheoremLet \\(p_n\\in{\\cal P}_n\\) be the unique polynomial interpolating \\(f(x)\\) at the \\(n+1\\) distinct nodes \\(x_0, x_1, \\ldots, x_n \\in [a,b]\\), and let \\(f\\) be continuous on \\([a,b]\\) with \\(n+1\\) continuous derivatives on \\((a,b)\\). Then for each \\(x\\in[a,b]\\) there exists \\(\\xi\\in(a,b)\\) such that \\[\nf(x) - p_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)(x-x_1)\\cdots(x-x_n).\n\\]\n\n\nThis looks similar to the error formula for Taylor polynomials (see Taylor’s Theorem). But now the error vanishes at multiple nodes rather than just at \\(x_0\\).\nFrom the formula, you can see that the error will be larger for a more “wiggly” function, where the derivative \\(f^{(n+1)}\\) is larger. It might also appear that the error will go down as the number of nodes \\(n\\) increases; we will see in Section 3.2.4 that this is not always true.\n\n\n\n\n\n\nAs in Taylor’s theorem, note the appearance of an undetermined point \\(\\xi\\). This will prevent us knowing the error exactly, but we can make an estimate as before.\n\n\n\n\nExample 2.7: Quadratic interpolant for \\(f(x)=\\cos(x)\\) with \\(\\{-\\tfrac\\pi4,0,\\tfrac\\pi4\\}\\).For \\(n=2\\), Cauchy’s Interpolation Error Theorem says that \\[\nf(x) - p_2(x) = \\frac{f^{(3)}(\\xi)}{6}x(x+\\tfrac\\pi4)(x-\\tfrac\\pi4) = \\tfrac16\\sin(\\xi)x(x+\\tfrac\\pi4)(x-\\tfrac\\pi4),\n\\] for some \\(\\xi\\in[-\\tfrac\\pi4,\\tfrac\\pi4]\\).\nFor an upper bound on the error at a particular \\(x\\), we can just use \\(|\\sin(\\xi)|\\leq 1\\) and plug in \\(x\\).\nTo bound the maximum error within the interval \\([-1,1]\\), let us maximise the polynomial \\(w(x)=x(x+\\tfrac\\pi4)(x-\\tfrac\\pi4)\\). We have \\(w'(x) = 3x^2 - \\tfrac{\\pi^2}{16}\\) so turning points are at \\(x=\\pm\\tfrac\\pi{4\\sqrt{3}}\\). We have \\[\nw(-\\tfrac\\pi{4\\sqrt{3}}) = 0.186\\ldots, \\quad\nw(\\tfrac\\pi{4\\sqrt{3}}) = -0.186\\ldots, \\quad\nw(-1) = -0.383\\ldots, \\quad w(1) = 0.383\\ldots.\n\\]\nSo our error estimate for \\(x\\in[-1,1]\\) is \\[\n|f(x) - p_2(x)| \\leq \\tfrac16(0.383) = 0.0638\\ldots\n\\]\nFrom the plot earlier, we see that this bound is satisfied (as it has to be), although not tight.\n\n\n\n\n3.2.4 Node Placement: Chebyshev nodes\nYou might expect polynomial interpolation to converge as \\(n\\to\\infty\\). Surprisingly, this is not the case if you take equally-spaced nodes \\(x_i\\). This was shown by Runge in a famous 1901 paper.\n\nExample 2.8: The Runge function \\(f(x) = 1/(1 + 25x^2)\\) on \\([-1,1]\\).Here are illustrations of \\(p_n\\) for increasing \\(n\\):\n   \nNotice that \\(p_n\\) is converging to \\(f\\) in the middle, but diverging more and more near the ends, even within the interval \\([x_0,x_n]\\). This is called the Runge phenomenon.\n\n\n\n\n\n\n\n\nA full mathematical explanation for this divergence usually uses complex analysis — see Chapter 13 of Approximation Theory and Approximation Practice by L.N. Trefethen (SIAM, 2013). For a more elementary proof, see this StackExchange post.\n\n\n\nThe problem is (largely) coming from the interpolating polynomial \\[\nw(x) = \\prod_{i=0}^n(x-x_i).\n\\] We can avoid the Runge phenomenon by choosing different nodes \\(x_i\\) that are not uniformly spaced.\nSince the problems are occurring near the ends of the interval, it would be logical to put more nodes there. A good choice is given by taking equally-spaced points on the unit circle \\(|z|=1\\), and projecting to the real line:\n\n\n\n\n\nThe points around the circle are \\[\n\\phi_j = \\frac{(2j+1)\\pi}{2(n+1)}, \\quad j=0,\\ldots,n,\n\\] so the corresponding Chebyshev nodes are \\[\nx_j = \\cos\\left[\\frac{(2j + 1)\\pi}{2(n+1)}\\right], \\quad j=0,\\ldots,n.\n\\]\n\nExample 2.9: The Runge function \\(f(x) = 1/(1+25x^2)\\) on \\([-1,1]\\) using the Chebyshev nodes.For \\(n=3\\), the nodes are \\(x_0=\\cos(\\tfrac\\pi8)\\), \\(x_1=\\cos(\\tfrac{3\\pi}{8})\\), \\(x_2=\\cos(\\tfrac{5\\pi}{8})\\), \\(x_3=\\cos(\\tfrac{7\\pi}{8})\\).\nBelow we illustrate the resulting interpolant for \\(n=15\\):\n\n\n\n\n\nCompare this to the example with equally spaced nodes.\n\n\nIn fact, the Chebyshev nodes are, in one sense, an optimal choice. To see this, we first note that they are zeroes of a particular polynomial.\n\nThe Chebyshev points \\(x_j=\\cos\\left[\\frac{(2j+1)\\pi}{2(n+1)}\\right]\\) for \\(j=0,\\ldots,n\\) are zeroes of the Chebyshev polynomial \\[\nT_{n+1}(t) := \\cos\\big[(n+1)\\arccos(t)\\big]\n\\]\n\n\n\n\n\n\n\nThe Chebyshev polynomials are denoted \\(T_n\\) rather than \\(C_n\\) because the name is transliterated from Russian as “Tchebychef” in French, for example.\n\n\n\nIn choosing the Chebyshev nodes, we are choosing the error polynomial \\(w(x):=\\prod_{i=0}^n(x-x_i)\\) to be \\(T_{n+1}(x)/2^n\\). (This normalisation makes the leading coefficient 1) This is a good choice because of the following result.\n\nTheorem 2.7: Chebyshev interpolationLet \\(x_0, x_1, \\ldots, x_n \\in [-1,1]\\) be distinct. Then \\(\\max_{[-1,1]}|w(x)|\\) is minimized if \\[\nw(x) = \\frac{1}{2^n}T_{n+1}(x),\n\\] where \\(T_{n+1}(x)\\) is the Chebyshev polynomial \\(T_{n+1}(x) = \\cos\\Big((n+1)\\arccos(x)\\Big)\\).\n\n\nHaving established that the Chebyshev polynomial minimises the maximum error, we can see convergence as \\(n\\to\\infty\\) from the fact that \\[\n|f(x) - p_n(x)| = \\frac{|f^{(n+1)}(\\xi)|}{(n+1)!}|w(x)| = \\frac{|f^{(n+1)}(\\xi)|}{2^n(n+1)!}|T_{n+1}(x)| \\leq \\frac{|f^{(n+1)}(\\xi)|}{2^n(n+1)!}.\n\\]\nIf the function is well-behaved enough that \\(|f^{(n+1)}(x)| &lt; M\\) for some constant whenever \\(x \\in [-1,1]\\), then the error will tend to zero as \\(n \\to \\infty\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#solving-nonlinear-equations",
    "href": "chap-two.html#solving-nonlinear-equations",
    "title": "2  Continuous Functions",
    "section": "2.2 Solving Nonlinear Equations",
    "text": "2.2 Solving Nonlinear Equations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#interpolation",
    "href": "chap-two.html#interpolation",
    "title": "2  Continuous Functions",
    "section": "",
    "text": "2.1.1 Polynomial Interpolation: Motivation\nThe main idea of this section is to find a polynomial that approximates a general function \\(f\\). But why polynomials? Polynomials have many nice mathematical properties but from the perspective of function approximation, the key one is the following: Any continuous function on a compact interval can be approximated to arbitrary accuracy using a polynomial (provided you are willing to go high enough degree).\n\nTheorem 2.1: Weierstrass Approximation Theorem (1885)For any \\(f\\in C([0,1])\\) and any \\(\\epsilon &gt; 0\\), there exists a polynomial \\(p(x)\\) such that \\[\n\\max_{0\\leq x\\leq 1}\\big|f(x) - p(x)\\big| \\leq \\epsilon.\n\\]\n\n\n\n\n\n\n\n\nThis may be proved using an explicit sequence of polynomials, called Bernstein polynomials.\n\n\n\nIf \\(f\\) is a polynomial of degree \\(n\\), \\[\nf(x) = p_n(x) = a_0 + a_1x + \\ldots + a_nx^n,\n\\] then we only need to store the \\(n+1\\) coefficients \\(a_0,\\ldots,a_n\\). Operations such as taking the derivative or integrating \\(f\\) are also convenient. If \\(f\\) is not continuous, then something other than a polynomial is required, since polynomials can’t handle asymptotic behaviour.\n\n\n\n\n\n\nTo approximate functions like \\(1/x\\), there is a well-developed theory of rational function interpolation, which is beyond the scope of this course.\n\n\n\nIn this chapter, we look for a suitable polynomial \\(p_n\\) by interpolation—that is, requiring \\(p_n(x_i) = f(x_i)\\) at a finite set of points \\(x_i\\), usually called nodes. Sometimes we will also require the derivative(s) of \\(p_n\\) to match those of \\(f\\). This type of function approximation where we want to match values of the function that we know at particular points is very natural in many applications. For example, weather forecasts involve numerically solving huge systems of partial differential equations (PDEs), which means actually solving them on a discrete grid of points. If we want weather predictions between grid points, we must interpolate. Figure Figure 2.1 shows the spatial resolutions of a range of current and past weather models produced by the UK Met Office.\n\n\n\n\n\n\nFigure 2.1: Chart showing a range of weather models produce by the UK Met Office. Even the highest spatial resolution models have more than 1.5km between grid point due to computational constraints.\n\n\n\n\n\n2.1.2 Taylor series\nA truncated Taylor series is (in some sense) the simplest interpolating polynomial since it uses only a single node \\(x_0\\), although it does require \\(p_n\\) to match both \\(f\\) and some of its derivatives.\n\nWe can approximate this using a Taylor series about the point \\(x_0=0\\), which is \\[\n\\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\ldots.\n\\] This comes from writing \\[\nf(x) = a_0 + a_1(x-x_0) + a_2(x-x_0)^2 + \\ldots,\n\\] then differentiating term-by-term and matching values at \\(x_0\\): \\[\\begin{align*}\nf(x_0) &= a_0,\\\\\nf'(x_0) &= a_1,\\\\\nf''(x_0) &= 2a_2,\\\\\nf'''(x_0) &= 3(2)a_3,\\\\\n&\\vdots\\\\\n\\implies f(x) &= f(x_0) + f'(x_0)(x-x_0) + \\frac{f''(x_0)}{2!}(x-x_0)^2 + \\frac{f'''(x_0)}{3!}(x-x_0)^3 + \\ldots.\n\\end{align*}\\] So \\[\\begin{align*}\n\\textrm{1 term} \\;&\\implies\\; f(0.1) \\approx 0.1,\\\\\n\\textrm{2 terms} \\;&\\implies\\; f(0.1) \\approx 0.1 - \\frac{0.1^3}{6} = 0.099833\\ldots,\\\\\n\\textrm{3 terms} \\;&\\implies\\; f(0.1) \\approx 0.1 - \\frac{0.1^3}{6} + \\frac{0.1^5}{120} = 0.09983341\\ldots.\\\\\n\\end{align*}\\] The next term will be \\(-0.1^7/7! \\approx -10^{-7}/10^3 = -10^{-10}\\), which won’t change the answer to 6 s.f.\n\n\n\n\n\n\n\nThe exact answer is \\(\\sin(0.1)=0.09983341\\).\n\n\n\nMathematically, we can write the remainder as follows.\n\nTheorem 2.2: Taylor’s TheoremLet \\(f\\) be \\(n+1\\) times differentiable on \\((a,b)\\), and let \\(f^{(n)}\\) be continuous on \\([a,b]\\). If \\(x,x_0\\in[a,b]\\) then there exists \\(\\xi \\in (a,b)\\) such that \\[\nf(x) = \\sum_{k=0}^n\\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k \\; + \\; \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)^{n+1}.\n\\]\n\n\nThe sum is called the Taylor polynomial of degree \\(n\\), and the last term is called the Lagrange form of the remainder. Note that the unknown number \\(\\xi\\) depends on \\(x\\).\n\nFor \\(f(x)=\\sin(x)\\), we found the Taylor polynomial \\(p_6(x) = x - x^3/3! + x^5/5!\\), and \\(f^{(7)}(x)=-\\sin(x)\\). So we have \\[\n\\big|f(x) - p_6(x)\\big| = \\left|\\frac{f^{(7)}(\\xi)}{7!}(x-x_0)^7\\right|\n\\] for some \\(\\xi\\) between \\(x_0\\) and \\(x\\). For \\(x=0.1\\), we have \\[\n\\big|f(0.1) - p_6(0.1)\\big| = \\frac{1}{5040}(0.1)^7\\big|f^{(7)}(\\xi)\\big| \\quad \\textrm{for some $\\xi\\in[0,0.1]$}.\n\\] Since \\(\\big|f^{(7)}(\\xi)\\big| = \\big|\\sin(\\xi)\\big| \\leq 1\\), we can say, before calculating, that the error satisfies \\[\n\\big|f(0.1) - p_6(0.1)\\big| \\leq 1.984\\times 10^{-11}.\n\\]\n\n\n\n\n\n\n\nThe actual error is \\(1.983\\times 10^{-11}\\), so this is a tight estimate.\n\n\n\nSince this error arises from approximating \\(f\\) with a truncated series, rather than due to rounding, it is known as truncation error. Note that it tends to be lower if you use more terms (larger \\(n\\)), or if the function oscillates less (smaller \\(f^{(n+1)}\\) on the interval \\((x_0,x)\\)).\nError estimates like the Lagrange remainder play an important role in numerical analysis and computation, so it is important to understand where it comes from. The number \\(\\xi\\) will ultimately come from Rolle’s theorem, which is a special case of the mean value theorem from first-year calculus:\n\nTheorem 2.3: Rolle’s TheoremIf \\(f\\) is continuous on \\([a,b]\\) and differentiable on \\((a,b)\\), with \\(f(a)=f(b)=0\\), then there exists \\(\\xi\\in(a,b)\\) with \\(f'(\\xi)=0\\).\n\n\n\n\n\n\n\n\nNote that Rolle’s Theorem does not tell us what the value of \\(\\xi\\) might actually be, so in practice we must take some kind of worst case estimate to get an error bound, e.g. calculate the max value of \\(f'(\\xi)\\) over the range of possible \\(\\xi\\) values.\n\n\n\n\n\n2.1.3 Polynomial Interpolation\nThe classical problem of polynomial interpolation is to find a polynomial \\[\np_n(x) = a_0 + a_1x + \\ldots + a_n x^n = \\sum_{k=0}^n a_k x^k\n\\] that interpolates our function \\(f\\) at a finite set of nodes \\(\\{x_0, x_1, \\ldots, x_m\\}\\). In other words, \\(p_n(x_i)=f(x_i)\\) at each of the nodes \\(x_i\\). Since the polynomial has \\(n+1\\) unknown coefficients, we expect to need \\(n+1\\) distinct nodes, so let us assume that \\(m=n\\).\n\nHere we have two nodes \\(x_0\\), \\(x_1\\), and seek a polynomial \\(p_1(x) = a_0 + a_1x\\). Then the interpolation conditions require that \\[\n\\begin{cases}\np_1(x_0) = a_0 + a_1x_0 = f(x_0)\\\\\np_1(x_1) = a_0 + a_1x_1 = f(x_1)\n\\end{cases}\n\\implies\\quad\np_1(x) = \\frac{x_1f(x_0) - x_0f(x_1)}{x_1 - x_0} + \\frac{f(x_1) - f(x_0)}{x_1 - x_0}x.\n\\]\n\nFor general \\(n\\), the interpolation conditions require \\[\n\\begin{matrix}\na_0 &+ a_1x_0 &+ a_2x_0^2 &+ \\ldots &+ a_nx_0^n &= f(x_0),\\\\\na_0 &+ a_1x_1 &+ a_2x_1^2 &+ \\ldots &+ a_nx_1^n &= f(x_1),\\\\\n\\vdots  & \\vdots  & \\vdots     &        &\\vdots      & \\vdots\\\\\na_0 &+ a_1x_n &+ a_2x_n^2 &+ \\ldots &+ a_nx_n^n &= f(x_n),\n\\end{matrix}\n\\] so we have to solve \\[\n\\begin{pmatrix}\n1 & x_0 & x_0^2 & \\cdots & x_0^n\\\\\n1 & x_1 & x_1^2 & \\cdots & x_1^n\\\\\n\\vdots & \\vdots &\\vdots& & \\vdots\\\\\n1 & x_n & x_n^2 & \\cdots & x_n^n\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\na_0\\\\ a_1\\\\ \\vdots\\\\ a_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf(x_0)\\\\ f(x_1)\\\\ \\vdots\\\\ f(x_n)\n\\end{pmatrix}.\n\\] This is called a Vandermonde matrix. The determinant of this matrix is \\[\n\\det(A) = \\prod_{0\\leq i &lt; j\\leq n} (x_j - x_i),\n\\] which is non-zero provided the nodes are all distinct. This establishes an important result, where \\(\\mathcal{P}_n\\) denotes the space of all real polynomials of degree \\(\\leq n\\).\n\nTheorem 2.4: Existence/uniquenessGiven \\(n+1\\) distinct nodes \\(x_0, x_1, \\ldots, x_n\\), there is a unique polynomial \\(p_n\\in\\mathcal{P}_n\\) that interpolates \\(f(x)\\) at these nodes.\n\n\nWe may also prove uniqueness by the following elegant argument.\nProof (Uniqueness part of Existence/Uniqueness Theorem):\nSuppose that in addition to \\(p_n\\) there is another interpolating polynomial \\(q_n\\in\\mathcal{P}_n\\). Then the difference \\(r_n := p_n - q_n\\) is also a polynomial with degree \\(\\leq n\\). But we have \\[\nr_n(x_i) = p_n(x_i) - q_n(x_i) = f(x_i)-f(x_i)=0 \\quad \\textrm{for $i=0,\\ldots,n$},\n\\] so \\(r_n(x)\\) has \\(n+1\\) roots. From the Fundamental Theorem of Algebra, this is possible only if \\(r_n(x)\\equiv 0\\), which implies that \\(q_n=p_n\\).\n\n\n\n\n\n\nNote that the unique polynomial through \\(n+1\\) points may have degree \\(&lt; n\\). This happens when \\(a_0=0\\) in the solution to the Vandermonde system above.\n\n\n\n\nWe have \\(x_0=0\\), \\(x_1=\\tfrac{\\pi}{2}\\), \\(x_2=\\pi\\), so \\(f(x_0)=1\\), \\(f(x_1)=0\\), \\(f(x_2)=-1\\). Clearly the unique interpolant is a straight line \\(p_2(x) = 1 - \\tfrac2\\pi x\\).\nIf we took the nodes \\(\\{0,2\\pi,4\\pi\\}\\), we would get a constant function \\(p_2(x)=1\\).\n\n\n\n\n\n\nOne way to compute the interpolating polynomial would be to solve the Vandermonde system above, e.g. by Gaussian elimination. However, this is not recommended. In practice, we choose a different basis for \\(p_n\\); there are two common and effective choices due to Lagrange and Newton.\n\n\n\n\n\n\nThe Vandermonde matrix arises when we write \\(p_n\\) in the natural basis \\(\\{1,x,x^2,\\ldots\\}\\), but we could also choose to work in some other basis…\n\n\n\n\n\n2.1.4 Lagrange Polynomials\nThis uses a special basis of polynomials \\(\\{\\ell_k\\}\\) in which the interpolation equations reduce to the identity matrix. In other words, the coefficients in this basis are just the function values, \\[\np_n(x) = \\sum_{k=0}^n f(x_k)\\ell_k(x).\n\\]\n\nExample 2.1: Linear interpolation again.We can re-write our linear interpolant to separate out the function values: \\[\np_1(x) = \\underbrace{\\frac{x - x_1}{x_0 - x_1}}_{\\ell_0(x)}f(x_0) + \\underbrace{\\frac{x-x_0}{x_1-x_0}}_{\\ell_1(x)}f(x_1).\n\\] Then \\(\\ell_0\\) and \\(\\ell_1\\) form the necessary basis. In particular, they have the property that \\[\n\\ell_0(x_i) = \\begin{cases}\n1 & \\textrm{if $i=0$},\\\\\n0 & \\textrm{if $i=1$},\n\\end{cases}\n\\qquad\n\\ell_1(x_i) = \\begin{cases}\n0 & \\textrm{if $i=0$},\\\\\n1 & \\textrm{if $i=1$},\n\\end{cases}\n\\]\n\n\nFor general \\(n\\), the \\(n+1\\) Lagrange polynomials are defined as a product \\[\n\\ell_k(x) = \\prod_{\\substack{j=0\\\\j\\neq k}}^n\\frac{x - x_j}{x_k - x_j}.\n\\] By construction, they have the property that \\[\n\\ell_k(x_i) = \\begin{cases}\n1 & \\textrm{if $i=k$},\\\\\n0 & \\textrm{otherwise}.\n\\end{cases}\n\\] From this, it follows that the interpolating polynomial may be written as above.\n\n\n\n\n\n\nBy the Existence/Uniqueness Theorem, the Lagrange polynomials are the unique polynomials with this property.\n\n\n\n\nExample 2.2: Compute the quadratic interpolating polynomial to \\(f(x)=\\cos(x)\\) with nodes \\(\\{-\\tfrac\\pi4, 0, \\tfrac\\pi4\\}\\) using Lagrange polynomials.The Lagrange polynomials of degree 2 for these nodes are \\[\n\\begin{aligned}\n\\ell_0(x) &= \\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)} = \\frac{x(x-\\tfrac\\pi4)}{\\tfrac\\pi4\\cdot\\tfrac\\pi2},\\\\\n\\ell_1(x) &= \\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)} = \\frac{(x+\\tfrac\\pi4)(x-\\tfrac\\pi4)}{-\\tfrac\\pi4\\cdot\\tfrac\\pi4},\\\\\n\\ell_2(x) &= \\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}\\\\\n&= \\frac{x(x+\\tfrac\\pi4)}{\\tfrac\\pi2\\cdot\\tfrac\\pi4}.\n\\end{aligned}\n\\] So the interpolating polynomial is \\[\n\\begin{aligned}\np_2(x) &= f(x_0)\\ell_0(x) + f(x_1)\\ell_1(x) + f(x_2)\\ell_2(x)\\\\\n&= \\tfrac{1}{\\sqrt{2}}\\tfrac8{\\pi^2}x(x-\\tfrac\\pi4) - \\tfrac{16}{\\pi^2}(x+\\tfrac\\pi4)(x-\\tfrac\\pi4) +\\tfrac{1}{\\sqrt{2}}\\tfrac8{\\pi^2}x(x+\\tfrac\\pi4) = \\tfrac{16}{\\pi^2}\\big(\\tfrac{1}{\\sqrt{2}} - 1\\big)x^2 + 1.\n\\end{aligned}\n\\] The Lagrange polynomials and the resulting interpolant are shown below:\n \n\n\n\n\n\n\n\n\nLagrange polynomials were actually discovered by Edward Waring in 1776 and rediscovered by Euler in 1783, before they were published by Lagrange himself in 1795; a classic example of Stigler’s law of eponymy!\n\n\n\nThe Lagrange form of the interpolating polynomial is easy to write down, but expensive to evaluate since all of the \\(\\ell_k\\) must be computed. Moreover, changing any of the nodes means that the \\(\\ell_k\\) must all be recomputed from scratch, and similarly for adding a new node (moving to higher degree).\n\n\n2.1.5 Newton/Divided-Difference Polynomials\nIt would be easy to increase the degree of \\(p_n\\) if \\[\np_{n+1}(x) = p_{n}(x) + g_{n+1}(x), \\quad \\textrm{where $g_{n+1}\\in{\\cal P}_{n+1}$}.\n\\] From the interpolation conditions, we know that \\[\ng_{n+1}(x_i) = p_{n+1}(x_i) - p_{n}(x_i) = f(x_i)-f(x_i)=0 \\quad \\textrm{for $i=0,\\ldots,n$},\n\\] so \\[\ng_{n+1}(x) = a_{n+1}(x-x_0)\\cdots(x-x_{n}).\n\\] The coefficient \\(a_{n+1}\\) is determined by the remaining interpolation condition at \\(x_{n+1}\\), so \\[\np_n(x_{n+1}) + g_{n+1}(x_{n+1}) = f(x_{n+1}) \\quad \\implies \\quad a_{n+1} = \\frac{f(x_{n+1}) - p_{n}(x_{n+1})}{(x_{n+1}-x_0)\\cdots(x_{n+1}-x_{n})}.\n\\]\nThe polynomial \\((x-x_0)(x-x_1)\\cdots(x-x_{n})\\) is called a Newton polynomial. These form a new basis \\[\nn_0(x)=1, \\qquad n_k(x) = \\prod_{j=0}^{k-1}(x-x_j) \\quad \\textrm{for $k&gt;0$}.\n\\]\nThe Newton form of the interpolating polynomial is then \\[\np_n(x) = \\sum_{k=0}^n a_kn_k(x), \\qquad a_0 = f(x_0),\\qquad a_k = \\frac{f(x_k) - p_{k-1}(x_k)}{(x_k-x_0)\\cdots(x_k-x_{k-1})} \\textrm{ for $k&gt;0$}.\n\\] Notice that \\(a_k\\) depends only on \\(x_0,\\ldots x_k\\), so we can construct first \\(a_0\\), then \\(a_1\\), etc.\nIt turns out that the \\(a_k\\) are easy to compute, but it will take a little work to derive the method. We define the divided difference \\(f[x_0,x_1,\\ldots,x_k]\\) to be the coefficient of \\(x^k\\) in the polynomial interpolating \\(f\\) at nodes \\(x_0,\\ldots,x_k\\). It follows that \\[\nf[x_0,x_1,\\ldots,x_k] = a_k,\n\\] where \\(a_k\\) is the coefficient in the Newton form above.\n\nExample 2.3: Compute the Newton interpolating polynomial at two nodes.\\[\n\\begin{aligned}\nf[x_0] &= a_0 = f(x_0),\\\\\nf[x_0,x_1] &= a_1 = \\frac{f(x_1) - p_0(x_1)}{x_1 - x_0} = \\frac{f(x_1)-a_0}{x_1 - x_0} = \\frac{f[x_1]-f[x_0]}{x_1 - x_0}.\n\\end{aligned}\n\\] So the first-order divided difference \\(f[x_0,x_1]\\) is obtained from the zeroth-order differences \\(f[x_0]\\), \\(f[x_1]\\) by subtracting and dividing, hence the name “divided difference”.\n\n\n\nExample 2.4: Compute the Newton interpolating polynomial at three nodes.Continuing from the previous example, we find \\[\n\\begin{aligned}\nf[x_0,x_1,x_2] &= a_2 = \\frac{f(x_2) - p_1(x_2)}{(x_2 - x_0)(x_2-x_1)} = \\frac{f(x_2) - a_0 - a_1(x_2-x_0)}{(x_2 - x_0)(x_2-x_1)}\\\\\n& = \\ldots = \\frac{1}{x_2-x_0}\\left(\\frac{f[x_2]-f[x_1]}{x_2-x_1} - \\frac{f[x_1]-f[x_0]}{x_1-x_0}\\right)\\\\\n&= \\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0}.\n\\end{aligned}\n\\] So again, we subtract and divide.\n\n\nIn general, we have the following.\n\nTheorem 2.5For \\(k&gt;0\\), the divided differences satisfy \\[\nf[x_i,x_{i+1},\\ldots,x_{i+k}] = \\frac{f[x_{i+1},\\ldots,x_{i+k}] - f[x_i,\\ldots,x_{i+k-1}]}{x_{i+k} - x_i}.\n\\]\n\n\nProof:\nWithout loss of generality, we relabel the nodes so that \\(i=0\\). So we want to prove that \\[\nf[x_0,x_1,\\ldots,x_{k}] = \\frac{f[x_1,\\ldots,x_{k}] - f[x_0,\\ldots,x_{k-1}]}{x_{k} - x_0}.\n\\] The trick is to write the interpolant with nodes \\(x_0, \\ldots, x_k\\) in the form \\[\np_k(x) = \\frac{(x_k-x)q_{k-1}(x) + (x-x_0)\\tilde{q}_{k-1}(x)}{x_k-x_0},\n\\] where \\(q_{k-1}\\in{\\cal P}_{k-1}\\) interpolates \\(f\\) at the subset of nodes \\(x_0, x_1, \\ldots, x_{k-1}\\) and \\(\\tilde{q}_{k-1}\\in{\\cal P}_{k-1}\\) interpolates \\(f\\) at the subset \\(x_1, x_2,\\ldots,x_k\\). If this holds, then matching the coefficient of \\(x^k\\) on each side will give the divided difference formula, since, e.g., the leading coefficient of \\(q_{k-1}\\) is \\(f[x_0,\\ldots,x_{k-1}]\\). To see that \\(p_k\\) may really be written this way, note that \\[\n\\begin{aligned}\np_k(x_0) &= q_{k-1}(x_0) = f(x_0),\\\\\np_k(x_k) &= \\tilde{q}_{k-1}(x_k) = f(x_k),\\\\\np_k(x_i) &= \\frac{(x_k-x_i)q_{k-1}(x_i) + (x_i-x_0)\\tilde{q}_{k-1}(x_i)}{x_k - x_0} = f(x_i) \\quad \\textrm{for $i=1,\\ldots,k-1$}.\n\\end{aligned}\n\\] Since \\(p_k\\) agrees with \\(f\\) at the \\(k+1\\) nodes, it is the unique interpolant in \\({\\cal P}_k\\).\nTheorem above gives us our convenient method, which is to construct a divided-difference table.\n\nExample 2.5: Construct the Newton polynomial at the nodes \\(\\{-1,0,1,2\\}\\) and with corresponding function values \\(\\{5,1,1,11\\}\\)We construct a divided-difference table as follows. \\[\n\\begin{matrix}\n&x_0=-1 \\quad &f[x_0]=5 & & &\\\\\n& & &f[x_0,x_1]=-4 & &\\\\\n&x_1=0 \\quad &f[x_1]=1 & &f[x_0,x_1,x_2]=2 &\\\\\n& & &f[x_1,x_2]=0 & &f[x_0,x_1,x_2,x_3]=1\\\\\n&x_2=1 \\quad &f[x_2]=1 & &f[x_1,x_2,x_3]=5 &\\\\\n& & &f[x_2,x_3]=10 & &\\\\\n&x_3=2 \\quad &f[x_3]=11 & & &\\\\\n\\end{matrix}\n\\] The coefficients of the \\(p_3\\) lie at the top of each column, so \\[\n\\begin{aligned}\np_3(x) &= f[x_0] + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1)\\\\\n& + f[x_0,x_1,x_2,x_3](x-x_0)(x-x_1)(x-x_2)\\\\\n&=5 - 4(x+1) + 2x(x+1) + x(x+1)(x-1).\n\\end{aligned}\n\\] Now suppose we add the extra nodes \\(\\{-2,3\\}\\) with data \\(\\{5,35\\}\\). All we need to do to compute \\(p_5\\) is add two rows to the bottom of the table — there is no need to recalculate the rest. This gives \\[\n\\begin{matrix}\n&-1  &5 & & & & &\\\\\n& & &-4 & & & &\\\\\n&0  &1 & &2 & & &\\\\\n& & &0 & &1 & &\\\\\n&1  &1 & &5 & &\\textcolor{red}{-\\tfrac1{12}} &\\\\\n& & &10 & &\\textcolor{red}{\\tfrac{13}{12}} & &\\textcolor{red}{0}\\\\\n&2 &11 & &\\textcolor{red}{\\tfrac{17}{6}} & &\\textcolor{red}{-\\tfrac1{12}}\\\\\n& & &\\textcolor{red}{\\tfrac{3}{2}} & &\\textcolor{red}{\\tfrac{5}{6}} & &\\\\\n&\\textcolor{red}{-2} &\\textcolor{red}{5} & &\\textcolor{red}{\\tfrac92} & &&\\\\\n& & &\\textcolor{red}{6} & & & &\\\\\n&\\textcolor{red}{3}  &\\textcolor{red}{35} & & & & &\\\\\n\\end{matrix}\n\\] The new interpolating polynomial is \\[\np_5(x) = p_3(x) - \\tfrac{1}{12}x(x+1)(x-1)(x-2).\n\\]\n\n\n\n\n\n\n\n\nNotice that the \\(x^5\\) coefficient vanishes for these particular data, meaning that they are consistent with \\(f\\in{\\cal P}_4\\).\n\n\n\n\n\n\n\n\n\nNote that the value of \\(f[x_0,x_1,\\ldots,x_k]\\) is independent of the order of the nodes in the table. This follows from the uniqueness of \\(p_k\\).\n\n\n\nDivided differences are actually approximations for derivatives of \\(f\\). In the limit that the nodes all coincide, the Newton form of \\(p_n(x)\\) becomes the Taylor polynomial.\n\n\n2.1.6 Interpolation Error\nThe goal here is to estimate the error \\(|f(x)-p_n(x)|\\) when we approximate a function \\(f\\) by a polynomial interpolant \\(p_n\\). Clearly this will depend on \\(x\\).\n\nExample 2.6: Quadratic interpolant for \\(f(x)=\\cos(x)\\) with \\(\\{-\\tfrac\\pi4,0,\\tfrac\\pi4\\}\\).From Section 2.1.4, we have \\(p_2(x) = \\tfrac{16}{\\pi^2}\\left(\\tfrac{1}{\\sqrt{2}}-1\\right)x^2 + 1\\), so the error is \\[\n|f(x) - p_2(x)| = \\left|\\cos(x) -  \\tfrac{16}{\\pi^2}\\left(\\tfrac{1}{\\sqrt{2}}-1\\right)x^2 - 1\\right|.\n\\] This is shown below:\n\n\n\n\n\nClearly the error vanishes at the nodes themselves, but note that it generally does better near the middle of the set of nodes — this is quite typical behaviour.\n\n\nWe can adapt the proof of Taylor’s theorem to get a quantitative error estimate.\n\nTheorem 2.6: Cauchy’s Interpolation Error TheoremLet \\(p_n\\in{\\cal P}_n\\) be the unique polynomial interpolating \\(f(x)\\) at the \\(n+1\\) distinct nodes \\(x_0, x_1, \\ldots, x_n \\in [a,b]\\), and let \\(f\\) be continuous on \\([a,b]\\) with \\(n+1\\) continuous derivatives on \\((a,b)\\). Then for each \\(x\\in[a,b]\\) there exists \\(\\xi\\in(a,b)\\) such that \\[\nf(x) - p_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)(x-x_1)\\cdots(x-x_n).\n\\]\n\n\nThis looks similar to the error formula for Taylor polynomials (see Taylor’s Theorem). But now the error vanishes at multiple nodes rather than just at \\(x_0\\).\nFrom the formula, you can see that the error will be larger for a more “wiggly” function, where the derivative \\(f^{(n+1)}\\) is larger. It might also appear that the error will go down as the number of nodes \\(n\\) increases; we will see in Section 2.1.7 that this is not always true.\n\n\n\n\n\n\nAs in Taylor’s theorem, note the appearance of an undetermined point \\(\\xi\\). This will prevent us knowing the error exactly, but we can make an estimate as before.\n\n\n\n\nExample 2.7: Quadratic interpolant for \\(f(x)=\\cos(x)\\) with \\(\\{-\\tfrac\\pi4,0,\\tfrac\\pi4\\}\\).For \\(n=2\\), Cauchy’s Interpolation Error Theorem says that \\[\nf(x) - p_2(x) = \\frac{f^{(3)}(\\xi)}{6}x(x+\\tfrac\\pi4)(x-\\tfrac\\pi4) = \\tfrac16\\sin(\\xi)x(x+\\tfrac\\pi4)(x-\\tfrac\\pi4),\n\\] for some \\(\\xi\\in[-\\tfrac\\pi4,\\tfrac\\pi4]\\).\nFor an upper bound on the error at a particular \\(x\\), we can just use \\(|\\sin(\\xi)|\\leq 1\\) and plug in \\(x\\).\nTo bound the maximum error within the interval \\([-1,1]\\), let us maximise the polynomial \\(w(x)=x(x+\\tfrac\\pi4)(x-\\tfrac\\pi4)\\). We have \\(w'(x) = 3x^2 - \\tfrac{\\pi^2}{16}\\) so turning points are at \\(x=\\pm\\tfrac\\pi{4\\sqrt{3}}\\). We have \\[\nw(-\\tfrac\\pi{4\\sqrt{3}}) = 0.186\\ldots, \\quad\nw(\\tfrac\\pi{4\\sqrt{3}}) = -0.186\\ldots, \\quad\nw(-1) = -0.383\\ldots, \\quad w(1) = 0.383\\ldots.\n\\]\nSo our error estimate for \\(x\\in[-1,1]\\) is \\[\n|f(x) - p_2(x)| \\leq \\tfrac16(0.383) = 0.0638\\ldots\n\\]\nFrom the plot earlier, we see that this bound is satisfied (as it has to be), although not tight.\n\n\n\n\n2.1.7 Node Placement: Chebyshev nodes\nYou might expect polynomial interpolation to converge as \\(n\\to\\infty\\). Surprisingly, this is not the case if you take equally-spaced nodes \\(x_i\\). This was shown by Runge in a famous 1901 paper.\n\nExample 2.8: The Runge function \\(f(x) = 1/(1 + 25x^2)\\) on \\([-1,1]\\).Here are illustrations of \\(p_n\\) for increasing \\(n\\):\n   \nNotice that \\(p_n\\) is converging to \\(f\\) in the middle, but diverging more and more near the ends, even within the interval \\([x_0,x_n]\\). This is called the Runge phenomenon.\n\n\n\n\n\n\n\n\nA full mathematical explanation for this divergence usually uses complex analysis — see Chapter 13 of Approximation Theory and Approximation Practice by L.N. Trefethen (SIAM, 2013). For a more elementary proof, see this StackExchange post.\n\n\n\nThe problem is (largely) coming from the interpolating polynomial \\[\nw(x) = \\prod_{i=0}^n(x-x_i).\n\\] We can avoid the Runge phenomenon by choosing different nodes \\(x_i\\) that are not uniformly spaced.\nSince the problems are occurring near the ends of the interval, it would be logical to put more nodes there. A good choice is given by taking equally-spaced points on the unit circle \\(|z|=1\\), and projecting to the real line:\n\n\n\n\n\nThe points around the circle are \\[\n\\phi_j = \\frac{(2j+1)\\pi}{2(n+1)}, \\quad j=0,\\ldots,n,\n\\] so the corresponding Chebyshev nodes are \\[\nx_j = \\cos\\left[\\frac{(2j + 1)\\pi}{2(n+1)}\\right], \\quad j=0,\\ldots,n.\n\\]\n\nExample 2.9: The Runge function \\(f(x) = 1/(1+25x^2)\\) on \\([-1,1]\\) using the Chebyshev nodes.For \\(n=3\\), the nodes are \\(x_0=\\cos(\\tfrac\\pi8)\\), \\(x_1=\\cos(\\tfrac{3\\pi}{8})\\), \\(x_2=\\cos(\\tfrac{5\\pi}{8})\\), \\(x_3=\\cos(\\tfrac{7\\pi}{8})\\).\nBelow we illustrate the resulting interpolant for \\(n=15\\):\n\n\n\n\n\nCompare this to the example with equally spaced nodes.\n\n\nIn fact, the Chebyshev nodes are, in one sense, an optimal choice. To see this, we first note that they are zeroes of a particular polynomial.\n\nThe Chebyshev points \\(x_j=\\cos\\left[\\frac{(2j+1)\\pi}{2(n+1)}\\right]\\) for \\(j=0,\\ldots,n\\) are zeroes of the Chebyshev polynomial \\[\nT_{n+1}(t) := \\cos\\big[(n+1)\\arccos(t)\\big]\n\\]\n\n\n\n\n\n\n\nThe Chebyshev polynomials are denoted \\(T_n\\) rather than \\(C_n\\) because the name is transliterated from Russian as “Tchebychef” in French, for example.\n\n\n\nIn choosing the Chebyshev nodes, we are choosing the error polynomial \\(w(x):=\\prod_{i=0}^n(x-x_i)\\) to be \\(T_{n+1}(x)/2^n\\). (This normalisation makes the leading coefficient 1) This is a good choice because of the following result.\n\nTheorem 2.7: Chebyshev interpolationLet \\(x_0, x_1, \\ldots, x_n \\in [-1,1]\\) be distinct. Then \\(\\max_{[-1,1]}|w(x)|\\) is minimized if \\[\nw(x) = \\frac{1}{2^n}T_{n+1}(x),\n\\] where \\(T_{n+1}(x)\\) is the Chebyshev polynomial \\(T_{n+1}(x) = \\cos\\Big((n+1)\\arccos(x)\\Big)\\).\n\n\nHaving established that the Chebyshev polynomial minimises the maximum error, we can see convergence as \\(n\\to\\infty\\) from the fact that \\[\n|f(x) - p_n(x)| = \\frac{|f^{(n+1)}(\\xi)|}{(n+1)!}|w(x)| = \\frac{|f^{(n+1)}(\\xi)|}{2^n(n+1)!}|T_{n+1}(x)| \\leq \\frac{|f^{(n+1)}(\\xi)|}{2^n(n+1)!}.\n\\]\nIf the function is well-behaved enough that \\(|f^{(n+1)}(x)| &lt; M\\) for some constant whenever \\(x \\in [-1,1]\\), then the error will tend to zero as \\(n \\to \\infty\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#nonlinear-equations",
    "href": "chap-two.html#nonlinear-equations",
    "title": "2  Continuous Functions",
    "section": "2.2 Nonlinear Equations",
    "text": "2.2 Nonlinear Equations\n\n\nHow do we find roots of nonlinear equations?\n\nGiven a general equation \\[\nf(x) = 0,\n\\] there will usually be no explicit formula for the root(s) \\(x_*\\), so we must use an iterative method.\nRootfinding is a delicate business, and it is essential to begin by plotting a graph of \\(f(x)\\), so that you can tell whether the answer you get from your numerical method is correct.\n\nExample 2.10: \\(f(x) = \\frac{1}{x} - a\\), for \\(a &gt; 0\\).\nClearly we know the root is exactly \\(x_* = \\frac{1}{a}\\), but this will serve as a running example to test some of our methods\n\n\n\n2.2.1 Interval Bisection\nIf \\(f\\) is continuous and we can find an interval where it changes sign, then it must have a root in this interval. Formally, this is based on:\n\nTheorem 2.8: Intermediate Value TheoremIf \\(f\\) is continuous on \\([a,b]\\) and \\(c\\) lies between \\(f(a)\\) and \\(f(b)\\), then there is at least one point \\(x\\in[a,b]\\) such that \\(f(x)=c\\).\n\n\nIf \\(f(a)f(b)&lt;0\\), then \\(f\\) changes sign at least once in \\([a,b]\\), so by the Intermediate Value Theorem there must be a point \\(x_*\\in[a,b]\\) where \\(f(x_*)=0\\).\nWe can turn this into the following iterative algorithm:\n\nAlgorithm 2.1: Interval bisection\nLet \\(f\\) be continuous on \\([a_0,b_0]\\), with \\(f(a_0)f(b_0)&lt;0\\).\n\nAt each step, set \\(m_k = (a_k + b_k)/2\\).\nIf \\(f(a_k)f(m_k)\\geq 0\\) then set \\(a_{k+1}=m_k\\), \\(b_{k+1}=b_k\\), otherwise set \\(a_{k+1}=a_k\\), \\(b_{k+1}=m_k\\).\n\n\n\n\nExample 2.11: \\(f(x)=\\tfrac1x - 0.5\\).\n\nTry \\(a_0=1\\), \\(b_0=3\\) so that \\(f(a_0)f(b_0)=0.5(-0.1666) &lt; 0\\).\nNow the midpoint is \\(m_0=(1 + 3)/2 = 2\\), with \\(f(m_0)=0\\).\nWe are lucky and have already stumbled on the root \\(x_*=m_0=2\\)!\nSuppose we had tried \\(a_0=1.5\\), \\(b_0=3\\), so \\(f(a_0)=0.1666\\) and \\(f(b_0)=-0.1666\\), and again \\(f(a_0)f(b_0)&lt;0\\).\nNow \\(m_0 = 2.25\\), \\(f(m_0)=-0.0555\\). We have \\(f(a_0)f(m_0) &lt; 0\\), so we set \\(a_1 = a_0=1.5\\) and \\(b_1 = m_0=2.25\\). The root must lie in \\([1.5,2.25]\\).\nNow \\(m_1 = 1.875\\), \\(f(m_1)=0.0333\\), and \\(f(a_1)f(m_1)&gt;0\\), so we take \\(a_2 = m_1=1.875\\), \\(b_2 = b_1=2.25\\). The root must lie in \\([1.875,2.25]\\).\nWe can continue this algorithm, halving the length of the interval each time.\n\n\n\nSince the interval halves in size at each iteration, and always contains a root, we are guaranteed to converge to a root provided that \\(f\\) is continuous. Stopping at step \\(k\\), we get the minimum possible error by choosing \\(m_k\\) as our approximation.\n\nExample 2.12: Same example with initial interval \\([-0.5,0.5]\\).\nIn this case \\(f(a_0)f(b_0)&lt;0\\), but there is no root in the interval.\n\n\nThe rate of convergence is steady, so we can pre-determine how many iterations will be needed to converge to a given accuracy. After \\(k\\) iterations, the interval has length \\[\n|b_k - a_k| = \\frac{|b_0 - a_0|}{2^k},\n\\] so the error in the mid-point satisfies \\[\n|m_k - x_*| \\leq \\frac{|b_0 - a_0|}{2^{k+1}}.\n\\] In order for \\(|m_k - x_*| \\leq \\delta\\), we need \\(n\\) iterations, where \\[\n\\frac{|b_0 - a_0|}{2^{n+1}} \\leq \\delta \\quad \\implies \\log|b_0-a_0| - (n+1)\\log(2) \\leq \\log(\\delta) \\quad \\implies n \\geq \\frac{\\log|b_0-a_0| - \\log(\\delta)}{\\log(2)} - 1.\n\\]\n\nExample 2.13: Previous example continuedWith \\(a_0=1.5\\), \\(b_0=3\\), as in the above example, then for \\(\\delta = \\epsilon_{\\rm M}=1.1\\times 10^{-16}\\) we would need \\[\nn \\geq \\frac{\\log(1.5) - \\log(1.1\\times 10^{-16})}{\\log(2)}-1 \\quad \\implies n \\geq 53 \\textrm{ iterations}.\n\\]\n\n\n\n\n\n\n\n\nThis convergence is pretty slow, but the method has the advantage of being very robust (i.e., use it if all else fails…). It has the more serious disadvantage of only working in one dimension.\n\n\n\n\n\n2.2.2 Fixed point iteration\nThis is a very common type of rootfinding method. The idea is to transform \\(f(x)=0\\) into the form \\(g(x)=x\\), so that a root \\(x_*\\) of \\(f\\) is a fixed point of \\(g\\), meaning \\(g(x_*)=x_*\\). To find \\(x_*\\), we start from some initial guess \\(x_0\\) and iterate \\[\nx_{k+1} = g(x_k)\n\\] until \\(|x_{k+1}-x_k|\\) is sufficiently small. For a given equation \\(f(x)=0\\), there are many ways to transform it into the form \\(x=g(x)\\). Only some will result in a convergent iteration.\n\nExample 2.14: \\(f(x)=x^2-2x-3\\).Note that the roots are \\(-1\\) and \\(3\\). Consider some different rearrangements, with \\(x_0=0\\).\n\n\\(g(x) = \\sqrt{2x+3}\\), gives \\(x_k\\to 3\\) [to machine accuracy after 33 iterations].\n\\(g(x) = 3/(x-2)\\), gives \\(x_k\\to -1\\) [to machine accuracy after 33 iterations].\n\\(g(x) = (x^2 - 3)/2\\), gives \\(x_k\\to -1\\) [but very slowly!].\n\\(g(x) = x^2 - x - 3\\), gives \\(x_k\\to\\infty\\).\n\\(g(x) = (x^2+3)/(2x-2)\\), gives \\(x_k\\to -1\\) [to machine accuracy after 5 iterations].\n\nIf instead we take \\(x_0=42\\), then (1) and (2) still converge to the same roots, (3) now diverges, (4) still diverges, and (5) now converges to the other root \\(x_k\\to 3\\).\n\n\nIn this section, we will consider which iterations will converge, before addressing the rate of convergence in Section 2.2.3.\nOne way to ensure that the iteration will work is to find a contraction mapping \\(g\\), which is a map \\(L\\to L\\) (for some closed interval \\(L\\)) satisfying \\[\n|g(x)-g(y)| \\leq \\lambda|x-y|\n\\] for some \\(\\lambda &lt; 1\\) and for all \\(x\\), \\(y \\in L\\). The sketch below shows the idea:\n\n\n\n\n\n\nTheorem 2.9: Contraction Mapping TheoremIf \\(g\\) is a contraction mapping on \\(L=[a,b]\\), then 1. There exists a unique fixed point \\(x_*\\in L\\) with \\(g(x_*)=x_*\\). 2. For any \\(x_0\\in L\\), the iteration \\(x_{k+1}=g(x_k)\\) will converge to \\(x_*\\) as \\(k\\to\\infty\\).\n\n\nProof:\nTo prove existence, consider \\(h(x)=g(x)-x\\). Since \\(g:L\\to L\\) we have \\(h(a)=g(a)-a\\geq 0\\) and \\(h(b)=g(b)-b\\leq 0\\). Moreover, it follows from the contraction property above that \\(g\\) is continuous (think of “\\(\\epsilon\\delta\\)”), therefore so is \\(h\\). So the Intermediate Value Theorem guarantees the existence of at least one point \\(x_*\\in L\\) such that \\(h(x_*)=0\\), i.e. \\(g(x_*)=x_*\\).\nFor uniqueness, suppose \\(x_*\\) and \\(y_*\\) are both fixed points of \\(g\\) in \\(L\\). Then \\[\n|x_*-y_*| = |g(x_*)-g(y_*)| \\leq \\lambda |x_*-y_*| &lt; |x_*-y_*|,\n\\] which is a contradiction.\nFinally, to show convergence, consider \\[\n|x_*- x_{k+1} | = |g(x_*) - g(x_k)| \\leq \\lambda |x_* - x_k| \\leq \\ldots \\leq \\lambda^{k+1}|x_*-x_0|.\n\\] Since \\(\\lambda&lt;1\\), we see that \\(x_k\\to x_*\\) as \\(k\\to\\infty\\).\n\n\n\n\n\n\nThe Contraction Mapping Theorem is also known as the Banach fixed point theorem, and was proved by Stefan Banach in his 1920 PhD thesis.\n\n\n\nTo apply this result in practice, we need to know whether a given function \\(g\\) is a contraction mapping on some interval.\nIf \\(g\\) is differentiable, then Taylor’s theorem says that there exists \\(\\xi\\in(x,y)\\) with \\[\ng(x) = g(y) + g'(\\xi)(x-y) \\,\\, \\implies \\,\\, |g(x)-g(y)| \\leq \\Big(\\max_{\\xi\\in L}|g'(\\xi)|\\Big)\\,|x-y|.\n\\] So if (a) \\(g:L\\to L\\) and (b) \\(|g'(x)|\\leq M\\) for all \\(x\\in L\\) with \\(M&lt;1\\), then \\(g\\) is a contraction mapping on \\(L\\).\n\nExample 2.15: Iteration (a) from previous example, \\(g(x) = \\sqrt{2x + 3}\\).Here \\(g'=(2x + 3)^{-1/2}\\), so we see that \\(|g'(x)|&lt;1\\) for all \\(x&gt;-1\\).\nFor \\(g\\) to be a contraction mapping on an interval \\(L\\), we also need that \\(g\\) maps \\(L\\) into itself. Since our particular \\(g\\) is continuous and monotonic increasing (for \\(x&gt;-\\tfrac32\\)), it will map an interval \\([a,b]\\) to another interval whose end-points are \\(g(a)\\) and \\(g(b)\\).\nFor example, \\(g(-\\tfrac12)=\\sqrt{2}\\) and \\(g(4)=\\sqrt{11}\\), so the interval \\(L=[-\\tfrac12,4]\\) is mapped into itself. It follows by the Contraction Mapping Theorem that (1) there is a unique fixed point \\(x_*\\in[-\\tfrac12,4]\\) (which we know is \\(x_*=3\\)), and (2) the iteration will converge to \\(x_*\\) for any \\(x_0\\) in this interval (as we saw for \\(x_0=0\\)).\n\n\nIn practice, it is not always easy to find a suitable interval \\(L\\). But knowing that \\(|g'(x_*)|&lt;1\\) is enough to guarantee that the iteration will converge if \\(x_0\\) is close enough to \\(x_*\\).\n\nTheorem 2.10: Local Convergence TheoremLet \\(g\\) and \\(g'\\) be continuous in the neighbourhood of an isolated fixed point \\(x_*=g(x_*)\\). If \\(|g'(x_*)|&lt;1\\) then there is an interval \\(L=[x_*-\\delta,x_*+\\delta]\\) such that \\(x_{k+1}=g(x_k)\\) converges to \\(x_*\\) whenever \\(x_0\\in L\\).\n\n\nProof:\nBy continuity of \\(g'\\), there exists some interval \\(L=[x_*-\\delta,x_*+\\delta]\\) with \\(\\delta&gt;0\\) such that \\(|g'(x)|\\leq\nM\\) for some \\(M&lt;1\\), for all \\(x\\in L\\). Now let \\(x\\in L\\). It follows that \\[\n|x_* - g(x)| = |g(x_*)-g(x)| \\leq M|x_*-x| &lt; |x_*-x| \\leq \\delta,\n\\] so \\(g(x)\\in L\\). Hence \\(g\\) is a contraction mapping on \\(L\\) and the Contraction Mapping Theorem shows that \\(x_k\\to x_*\\).\n\nExample 2.16: Iteration (a) again, \\(g(x) = \\sqrt{2x + 3}\\).Here we know that \\(x_*=3\\), and \\(|g'(3)|=\\tfrac13 &lt; 1\\), so the Local Convergence Theorem tells us that the iteration will converge to \\(3\\) if \\(x_0\\) is close enough to \\(3\\).\n\n\n\nExample 2.17: Iteration (e) again, \\(g(x) = (x^2+3)/(2x-2)\\).Here we have \\[\ng'(x) = \\frac{x^2-2x-3}{2(x-1)^2},\n\\] so we see that \\(g'(-1)=g'(3)=0 &lt; 1\\). So the Local Convergence Theorem tells us that the iteration will converge to either root if we start close enough.\n\n\n\n\n\n\n\n\nAs we will see, the fact that \\(g'(x_*)=0\\) is related to the fast convergence of iteration (e).\n\n\n\n\n\n2.2.3 Orders of convergence\nTo measure the speed of convergence, we compare the error \\(|x_*-x_{k+1}|\\) to the error at the previous step, \\(|x_*-x_k|\\).\n\nExample 2.18: Interval bisection.Here we had \\(|x_*-m_{k+1}| \\leq \\tfrac12|x_*-m_k|\\). This is called linear convergence, meaning that we have \\(|x_*-x_{k+1}|\\leq \\lambda |x_* - x_{k}|\\) for some constant \\(\\lambda &lt; 1\\).\n\n\nWe can compare a few different iteration schemes that should converge to the same answer to get a sense for how our choice of scheme can impact the convergence order.\n\nExample 2.19: Iteration (a) again, \\(g(x)=\\sqrt{2x+3}\\).Look at the sequence of errors in this case:\n\n\n\n\\(x_k\\)\n\\(|3-x_k|\\)\n\\(|3-x_k|/|3-x_{k-1}|\\)\n\n\n\n\n0.0000000000\n3.0000000000\n-\n\n\n1.7320508076\n1.2679491924\n0.4226497308\n\n\n2.5424597568\n0.4575402432\n0.3608506129\n\n\n2.8433992885\n0.1566007115\n0.3422665304\n\n\n2.9473375404\n0.0526624596\n0.3362849319\n\n\n2.9823941860\n0.0176058140\n0.3343143126\n\n\n2.9941256440\n0.0058743560\n0.3336600063\n\n\n\nWe see that the ratio \\(|x_*-x_k|/|x_*-x_{k-1}|\\) is indeed less than \\(1\\), and seems to be converging to \\(\\lambda\\approx\\tfrac13\\). So this is a linearly convergent iteration.\n\n\n\nExample 2.20: Iteration (e) again, \\(g(x) = (x^2+3)/(2x-2)\\).Now the sequence is:\n\n\n\n\\(x_k\\)\n\\(|(-1)-x_k|\\)\n\\(|(-1)-x_k|/|(-1)-x_{k-1}|\\)\n\n\n\n\n0.0000000000\n1.0000000000\n-\n\n\n-1.5000000000\n0.5000000000\n0.5000000000\n\n\n-1.0500000000\n0.0500000000\n0.1000000000\n\n\n-1.0006097561\n0.0006097561\n0.0121951220\n\n\n-1.0000000929\n0.0000000929\n0.0001523926\n\n\n\nAgain the ratio \\(|x_*-x_{k}|/|x_*-x_{k-1}|\\) is certainly less than \\(1\\), but this time we seem to have \\(\\lambda\\to 0\\) as \\(k\\to\\infty\\). This is called superlinear convergence, meaning that the convergence is in some sense “accelerating”.\n\n\nIn general, if \\(x_k\\to x_*\\) then we say that the sequence \\(\\{x_k\\}\\) converges linearly if \\[\n\\lim_{k\\to\\infty}\\frac{|x_*-x_{k+1}|}{|x_*-x_k|} = \\lambda \\quad \\textrm{with} \\quad 0&lt;\\lambda &lt;1.\n\\] If \\(\\lambda=0\\) then the convergence is superlinear.\n\n\n\n\n\n\nThe constant \\(\\lambda\\) is called the rate or ratio.\n\n\n\nThe following result establishes conditions for linear and superlinear convergence.\n\nTheorem 2.11\nLet \\(g'\\) be continuous in the neighbourhood of a fixed point \\(x_*=g(x_*)\\), and suppose that \\(x_{k+1}=g(x_k)\\) converges to \\(x_*\\) as \\(k\\to\\infty\\).\n\nIf \\(|g'(x_*)|\\neq 0\\) then the convergence will be linear with rate \\(\\lambda=|g'(x_*)|\\).\nIf \\(|g'(x_*)|=0\\) then the convergence will be superlinear.\n\n\n\nProof:\nBy Taylor’s theorem, note that \\[\nx_* - x_{k+1} = g(x_*) - g(x_k) = g(x_*) - \\Big[g(x_*) + g'(\\xi_k)(x_k-x_*)\\Big] = g'(\\xi_k)(x_* - x_k)\n\\] for some \\(\\xi_k\\) between \\(x_*\\) and \\(x_k\\). Since \\(x_k\\to x_*\\), we have \\(\\xi_k\\to x_*\\) as \\(k\\to\\infty\\), so \\[\n\\lim_{k\\to\\infty}\\frac{|x_*-x_{k+1}|}{|x_*-x_k|} = \\lim_{k\\to\\infty}|g'(\\xi_k)| = |g'(x_*)|.\n\\] This proves the result.\n\nExample 2.21: Iteration (a) again, \\(g(x)=\\sqrt{2x+3}\\).We saw before that \\(g'(3)=\\tfrac13\\), so the theorem above shows that convergence will be linear with \\(\\lambda = |g'(3)| = \\tfrac13\\) as we found numerically.\n\n\n\nExample 2.22: Iteration (e) again, \\(g(x) = (x^2+3)/(2x-2)\\).We saw that \\(g'(-1)=0\\), so the theorem above shows that convergence will be superlinear, again consistent with our numerical findings.\n\n\nWe can further classify superlinear convergence by the order of convergence, defined as \\[\n\\alpha = \\sup\\left\\{\\beta \\, : \\, \\lim_{k\\to\\infty}\\frac{|x_*-x_{k+1}|}{|x_*-x_k|^\\beta} &lt; \\infty \\right\\}.\n\\]\nFor example, \\(\\alpha=2\\) is called quadratic convergence and \\(\\alpha=3\\) is called cubic convergence, although for a general sequence \\(\\alpha\\) need not be an integer (e.g. the secant method below).\n\n\n2.2.4 Newton’s method\nThis is a particular fixed point iteration that is very widely used because (as we will see) it usually converges superlinearly.\n\n\n\n\n\nGraphically, the idea of Newton’s method is simple: given \\(x_k\\), draw the tangent line to \\(f\\) at \\(x=x_k\\), and let \\(x_{k+1}\\) be the \\(x\\)-intercept of this tangent. So \\[\n\\frac{0 - f(x_k)}{x_{k+1} - x_k} = f'(x_k) \\quad \\implies\\quad  x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}.\n\\]\n\n\n\n\n\n\nIn fact, Newton only applied the method to polynomial equations, and without using calculus. The general form using derivatives (“fluxions”) was first published by Thomas Simpson in 1740. [See “Historical Development of the Newton-Raphson Method” by T.J. Ypma, SIAM Review 37, 531 (1995).]\n\n\n\nAnother way to derive this iteration is to approximate \\(f(x)\\) by the linear part of its Taylor series centred at \\(x_k\\): \\[\n0 \\approx f(x_{k+1}) \\approx f(x_k) + f'(x_k)(x_{k+1} - x_{k}).\n\\]\nThe iteration function for Newton’s method is \\[\ng(x) = x - \\frac{f(x)}{f'(x)},\n\\] so using \\(f(x_*)=0\\) we see that \\(g(x_*)=x_*\\). To assess the convergence, note that \\[\ng'(x) = 1 - \\frac{f'(x)f'(x) - f(x)f''(x)}{[f'(x)]^2} = \\frac{f(x)f''(x)}{[f'(x)]^2} \\quad \\implies g'(x_*)=0 \\quad \\textrm{if $f'(x_*)\\neq 0$}.\n\\] So if \\(f'(x_*)\\neq 0\\), the Local Convergence Theorem shows that the iteration will converge for \\(x_0\\) close enough to \\(x_*\\). Moreover, since \\(g'(x_*)=0\\), the order theorem shows that this convergence will be superlinear.\n\nExample 2.23: Calculate \\(a^{-1}\\) using \\(f(x)=\\frac1x - a\\) for \\(a&gt;0\\).Newton’s method gives the iterative formula \\[\nx_{k+1} = x_k - \\frac{\\frac{1}{x_k}-a}{-\\frac{1}{x_k^2}} = 2x_k - a x_k^2.\n\\] From the graph of \\(f\\), it is clear that the iteration will converge for any \\(x_0\\in(0,a^{-1})\\), but will diverge if \\(x_0\\) is too large. With \\(a=0.5\\) and \\(x_0=1\\), Python gives\n\n\n\n\n\n\n\n\n\n\\(x_k\\)\n\\(|2 - x_k|\\)\n\\(|2-x_k|/|2-x_{k-1}|\\)\n\\(|2-x_k|/|2-x_{k-1}|^2\\)\n\n\n\n\n1.0\n1.0\n-\n-\n\n\n1.5\n0.5\n0.5\n0.5\n\n\n1.875\n0.125\n0.25\n0.5\n\n\n1.9921875\n0.0078125\n0.0625\n0.5\n\n\n1.999969482\n\\(3.05\\times 10^{-5}\\)\n0.00390625\n0.5\n\n\n2.0\n\\(4.65\\times 10^{-10}\\)\n\\(1.53\\times 10^{-5}\\)\n0.5\n\n\n2.0\n\\(1.08\\times 10^{-19}\\)\n\\(2.33\\times 10^{-10}\\)\n0.5\n\n\n\nIn 6 steps, the error is below \\(\\epsilon_{\\rm M}\\): pretty rapid convergence! The third column shows that the convergence is superlinear. The fourth column shows that \\(|x_*-x_{k+1}|/|x_*-x_k|^2\\) is constant, indicating that the convergence is quadratic (order \\(\\alpha=2\\)).\n\n\n\n\n\n\n\n\nAlthough the solution \\(\\tfrac1a\\) is known exactly, this method is so efficient that it is sometimes used in computer hardware to do division!\n\n\n\nIn practice, it is not usually possible to determine ahead of time whether a given starting value \\(x_0\\) will converge.\nA robust computer implementation should catch any attempt to take too large a step, and switch to a less sensitive (but slower) algorithm (e.g. bisection).\nHowever, it always makes sense to avoid any points where \\(f'(x)=0\\).\n\nExample 2.24: \\(f(x) = x^3-2x+2\\).Here \\(f'(x) = 3x^2-2\\) so there are turning points at \\(x=\\pm\\sqrt{\\tfrac23}\\) where \\(f'(x)=0\\), as well as a single real root at \\(x_*\\approx -1.769\\). The presence of points where \\(f'(x)=0\\) means that care is needed in choosing a starting value \\(x_0\\).\nIf we take \\(x_0=0\\), then \\(x_1 = 0 - f(0)/f'(0) = 1\\), but then \\(x_2 = 1 - f(1)/f'(1)=0\\), so the iteration gets stuck in an infinite loop:\n\n\n\n\n\nOther starting values, e.g. \\(x_0=-0.5\\) can also be sucked into this infinite loop! The correct answer is obtained for \\(x_0=-1.0\\).\n\n\n\n\n\n\n\n\nThe sensitivity of Newton’s method to the choice of \\(x_0\\) is beautifully illustrated by applying it to a complex function such as \\(f(z) = z^3 - 1\\). The following plot colours points \\(z_0\\) in the complex plane according to which root they converge to (\\(1\\), \\(e^{2\\pi i/3}\\), or \\(e^{-2\\pi i/3}\\)):\n\n\n\n\n\nThe boundaries of these basins of attraction are fractal.\n\n\n\n\n\n2.2.5 Newton’s method for systems\nNewton’s method generalizes to higher-dimensional problems where we want to find \\(\\mathbf{x}\\in\\mathbb{R}^m\\) that satisfies \\(\\mathbf{f}(\\mathbf{x})=0\\) for some function \\(\\mathbf{f}:\\mathbb{R}^m\\to\\mathbb{R}^m\\).\nTo see how it works, take \\(m=2\\) so that \\(\\mathbf{x}=(x_1,x_2)^\\top\\) and \\(\\mathbf{f}=[f_1(\\mathbf{x}),f_2(\\mathbf{x})]^\\top\\). Taking the linear terms in Taylor’s theorem for two variables gives \\[\n\\begin{aligned}\n0 &\\approx f_1(\\mathbf{x}_{k+1}) \\approx f_1(\\mathbf{x}_k) + \\left.\\frac{\\partial f_1}{\\partial x_1}\\right|_{\\mathbf{x}_k}(x_{1,k+1} - x_{1,k}) + \\left.\\frac{\\partial f_1}{\\partial x_2}\\right|_{\\mathbf{x}_k}(x_{2,k+1} - x_{2,k}),\\\\\n0 &\\approx f_2(\\mathbf{x}_{k+1}) \\approx f_2(\\mathbf{x}_k) + \\left.\\frac{\\partial f_2}{\\partial x_1}\\right|_{\\mathbf{x}_k}(x_{1,k+1} - x_{1,k}) + \\left.\\frac{\\partial f_2}{\\partial x_2}\\right|_{\\mathbf{x}_k}(x_{2,k+1} - x_{2,k}).\n\\end{aligned}\n\\]\nIn matrix form, we can write \\[\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf_1(\\mathbf{x}_k)\\\\\nf_2(\\mathbf{x}_k)\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x_1}(\\mathbf{x}_k) & \\frac{\\partial f_1}{\\partial x_2}(\\mathbf{x}_k)\\\\\n\\frac{\\partial f_2}{\\partial x_1}(\\mathbf{x}_k) & \\frac{\\partial f_2}{\\partial x_2}(\\mathbf{x}_k)\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{1,k+1} - x_{1,k}\\\\\nx_{2,k+1} - x_{2,k}\n\\end{pmatrix}.\n\\]\nThe matrix of partial derivatives is called the Jacobian matrix \\(J(\\mathbf{x}_k)\\), so (for any \\(m\\)) we have \\[\n\\mathbf{0} = \\mathbf{f}(\\mathbf{x}_k) + J(\\mathbf{x}_k)(\\mathbf{x}_{k+1} - \\mathbf{x}_{k}).\n\\]\nTo derive Newton’s method, we rearrange this equation for \\(\\mathbf{x}_{k+1}\\), \\[\nJ(\\mathbf{x}_k)(\\mathbf{x}_{k+1}-\\mathbf{x}_k) = -\\mathbf{f}(\\mathbf{x}_k) \\quad \\implies \\quad \\mathbf{x}_{k+1} = \\mathbf{x}_k - J^{-1}(\\mathbf{x}_k)\\mathbf{f}(\\mathbf{x}_k).\n\\] So to apply the method, we need the inverse of \\(J\\).\n\n\n\n\n\n\nIf \\(m=1\\), then \\(J(x_k) = \\frac{\\partial f}{\\partial x}(x_k)\\), and \\(J^{-1}=1/J\\), so this reduces to the scalar Newton’s method.\n\n\n\n\nExample 2.25: Apply Newton’s method to the simultaneous equations \\(xy - y^3 - 1 = 0\\) and \\(x^2y + y -5=0\\), with starting values \\(x_0=2\\), \\(y_0=3\\).The Jacobian matrix is \\[\nJ(x,y) = \\begin{pmatrix}\ny & x-3y^2\\\\\n2xy & x^2 + 1\n\\end{pmatrix},\n\\] and hence its inverse is given by \\[\nJ^{-1}(x,y) = \\frac{1}{y(x^2+1)-2xy(x-3y^2)}\\begin{pmatrix}\nx^2 + 1 & 3y^2-x\\\\\n-2xy & y\n\\end{pmatrix}.\n\\]\nThe first iteration of Newton’s method gives \\[\n\\begin{pmatrix}\nx_1\\\\\ny_1\n\\end{pmatrix}\n= \\begin{pmatrix}\n2\\\\\n3\n\\end{pmatrix}\n- \\frac{1}{3(5)-12(2-27)}\n\\begin{pmatrix}\n5 & 25\\\\\n-12 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\n-22\\\\\n10\n\\end{pmatrix}\n= \\begin{pmatrix}\n1.55555556\\\\\n2.06666667\n\\end{pmatrix}.\n\\]\nSubsequent iterations give \\[\n\\begin{pmatrix}\nx_2\\\\\ny_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.54720541\\\\\n1.47779333\n\\end{pmatrix}, \\,\n\\begin{pmatrix}\nx_3\\\\\ny_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.78053503\\\\\n1.15886481\n\\end{pmatrix},\n\\] and \\[\n\\begin{pmatrix}\nx_4\\\\\ny_4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.952843\\\\\n1.02844269\n\\end{pmatrix}, \\\\\n\\begin{pmatrix}\nx_5\\\\\ny_5\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.99776297\\\\\n1.00124041\n\\end{pmatrix},\n\\]\nso the method is converging accurately to the root \\(x_*=2\\), \\(y_*=1\\), shown in the following plot:\n\n\n\n\n\n\n\nBy generalising the scalar analysis (beyond the scope of this course), it can be shown that the convergence is quadratic for \\(\\mathbf{x}_0\\) sufficiently close to \\(\\mathbf{x}_*\\), provided that \\(J(\\mathbf{x}_*)\\) is non-singular (i.e., \\(\\det[J(\\mathbf{x}_*)]\\neq 0\\)).\n\n\n\n\n\n\nIn general, finding a good starting point in more than one dimension is difficult, particularly because interval bisection is not available. Algorithms that try to mimic bisection in higher dimensions are available, proceeding by a ‘grid search’ approach.\n\n\n\n\n\n2.2.6 Quasi-Newton methods\nA drawback of Newton’s method is that the derivative \\(f'(x_k)\\) must be computed at each iteration. This may be expensive to compute, or may not be available as a formula. For example, the function \\(f\\) might be the right-hand side of some complex partial differential equation, and hence both difficult to differentiate and very high dimensional!\nInstead we can use a quasi-Newton method \\[\nx_{k+1} = x_k - \\frac{f(x_k)}{g_k},\n\\] where \\(g_k\\) is some easily-computed approximation to \\(f'(x_k)\\).\n\nExample 2.26: Steffensen's method\\[\ng_k = \\frac{f\\big(f(x_k) + x_k\\big) - f(x_k)}{f(x_k)}.\n\\] This has the form \\(\\frac{1}{h}\\big(f(x_k+h) - f(x_k)\\big)\\) with \\(h=f(x_k)\\).\n\n\nSteffensen’s method requires two function evaluations per iteration. But once the iteration has started, we already have two nearby points \\(x_{k-1}\\), \\(x_k\\), so we could approximate \\(f'(x_k)\\) by a backward difference \\[\ng_k = \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}} \\quad \\implies \\quad x_{k+1} = x_k - \\frac{f(x_k)(x_k - x_{k-1})}{f(x_k) - f(x_{k-1})}.\n\\] This is called the secant method, and requires only one function evaluation per iteration (once underway). The name comes from its graphical interpretation:\n\n\n\n\n\n\n\n\n\n\n\nThe secant method was introduced by Newton.\n\n\n\n\nExample 2.27: \\(f(x)=\\tfrac1x - 0.5\\).Now we need two starting values, so take \\(x_0=0.25\\), \\(x_1=0.5\\). The secant method gives:\n\n\n\n\\(k\\)\n\\(x_k\\)\n\\(|x_* - x_k|/|x_* - x_{k-1}|\\)\n\n\n\n\n2\n0.6875\n0.75\n\n\n3\n1.01562\n0.75\n\n\n4\n1.354\n0.65625\n\n\n5\n1.68205\n0.492188\n\n\n6\n1.8973\n0.322998\n\n\n7\n1.98367\n0.158976\n\n\n8\n1.99916\n0.0513488\n\n\n\nConvergence to \\(\\epsilon_{\\rm M}\\) is achieved in 12 iterations. Notice that the error ratio is decreasing, so the convergence is superlinear.\n\n\nThe secant method is a two-point method since \\(x_{k+1} = g(x_{k-1},x_k)\\). So theorems about single-point fixed-point iterations do not apply.\nIn general, one can have multipoint methods based on higher-order interpolation.\n\nTheorem 2.12If \\(f'(x_*)\\neq 0\\) then the secant method converges for \\(x_0\\), \\(x_1\\) sufficiently close to \\(x_*\\), and the order of convergence is \\((1+\\sqrt{5})/2 = 1.618\\ldots\\).\n\n\n\n\n\n\n\n\nThis illustrates that orders of convergence need not be integers, and is also an appearance of the golden ratio.\n\n\n\nProof:\nTo simplify the notation, denote the truncation error by \\[\n\\varepsilon_k := x_* - x_k.\n\\] Expanding in Taylor series around \\(x_*\\), and using \\(f(x_*)=0\\), gives \\[\n\\begin{aligned}\nf(x_{k-1}) &= -f'(x_*)\\varepsilon_{k-1} + \\frac{f''(x_*)}{2}\\varepsilon_{k-1}^2 + {\\cal O}(\\varepsilon_{k-1}^3),\\\\\nf(x_{k}) &= -f'(x_*)\\varepsilon_{k} + \\frac{f''(x_*)}{2}\\varepsilon_{k}^2 + {\\cal O}(\\varepsilon_{k}^3).\n\\end{aligned}\n\\] So using the secant formula above we get \\[\n\\begin{aligned}\n\\varepsilon_{k+1} &= \\varepsilon_k - (\\varepsilon_{k}-\\varepsilon_{k-1})\\frac{-f'(x_*)\\varepsilon_{k} + \\frac{f''(x_*)}{2}\\varepsilon_{k}^2 + {\\cal O}(\\varepsilon_{k}^3)}{-f'(x_*)(\\varepsilon_{k}-\\varepsilon_{k-1})+ \\frac{f''(x_*)}{2}(\\varepsilon_{k}^2 - \\varepsilon_{k-1}^2) + {\\cal O}(\\varepsilon_{k-1}^3)}\\\\\n&= \\varepsilon_k - \\frac{ -f'(x_*)\\varepsilon_{k} + \\frac{f''(x_*)}{2}\\varepsilon_{k}^2 + {\\cal O}(\\varepsilon_{k}^3)}{-f'(x_*) + \\frac{f''(x_*)}{2}(\\varepsilon_k + \\varepsilon_{k-1}) + {\\cal O}(\\varepsilon_{k-1}^2)}\\\\\n&= \\varepsilon_k + \\frac{-\\varepsilon_k + \\tfrac12\\varepsilon_k^2f''(x_*)/f'(x_*) + {\\cal O}(\\varepsilon_k^3)}{1 - \\tfrac12(\\varepsilon_k + \\varepsilon_{k-1})f''(x_*)/f'(x_*) + {\\cal O}(\\varepsilon_{k-1}^2)}\\\\\n&= \\varepsilon_k + \\left(-\\varepsilon_k + \\frac{f''(x_*)}{2f'(x_*)}\\varepsilon_k^2 + {\\cal O}(\\varepsilon_k^3) \\right)\\left(1 + (\\varepsilon_k + \\varepsilon_{k-1})\\frac{f''(x_*)}{2f'(x_*)} + {\\cal O}(\\varepsilon_{k-1}^2) \\right)\\\\\n&= \\varepsilon_k - \\varepsilon_k + \\frac{f''(x_*)}{2f'(x_*)}\\varepsilon_k^2 - \\frac{f''(x_*)}{2f'(x_*)}\\varepsilon_k(\\varepsilon_k + \\varepsilon_{k-1}) + {\\cal O}(\\varepsilon_{k-1}^3)\\\\\n&= -\\frac{f''(x_*)}{2f'(x_*)}\\varepsilon_k\\varepsilon_{k-1} + {\\cal O}(\\varepsilon_{k-1}^3).\n\\end{aligned}\n\\] This is similar to the corresponding formula for Newton’s method, where we have \\[\n\\varepsilon_{k+1} = -\\frac{f''(x_*)}{2f'(x_*)}\\varepsilon_k^2 + {\\cal O}(\\varepsilon_k^3).\n\\] The above tells us that the error for the secant method tends to zero faster than linearly, but not quadratically (because \\(\\varepsilon_{k-1} &gt; \\varepsilon_k\\)).\nTo find the order of convergence, note that \\(\\varepsilon_{k+1}\\sim \\varepsilon_k\\varepsilon_{k-1}\\) suggests a power-law relation of the form \\[\n|\\varepsilon_k| = |\\varepsilon_{k-1}|^\\alpha\\left|\\frac{f''(x_*)}{2f'(x_*)}\\right|^\\beta \\quad \\implies \\quad |\\varepsilon_{k-1}| = |\\varepsilon_k|^{1/\\alpha}\\left|\\frac{f''(x_*)}{2f'(x_*)}\\right|^{-\\beta/\\alpha}.\n\\] Putting this in both sides of the previous equation gives \\[\n|\\varepsilon_{k}|^\\alpha\\left|\\frac{f''(x_*)}{2f'(x_*)}\\right|^\\beta = |\\varepsilon_k|^{(1+\\alpha)/\\alpha}\\left|\\frac{f''(x_*)}{2f'(x_*)}\\right|^{(\\alpha-\\beta)/\\alpha}.\n\\] Equating powers gives \\[\n\\alpha = \\frac{1+\\alpha}{\\alpha} \\quad \\implies \\alpha = \\frac{1 + \\sqrt{5}}{2}, \\quad \\beta = \\frac{\\alpha - \\beta}{\\alpha} \\quad \\implies \\beta = \\frac{\\alpha}{\\alpha + 1} = \\frac{1}{\\alpha}.\n\\] It follows that \\[\n\\lim_{k\\to\\infty}\\frac{|x_* - x_{k+1}|}{|x_* - x_k|^\\alpha} = \\lim_{k\\to\\infty}\\frac{|\\varepsilon_{k+1}|}{|\\varepsilon_k|^\\alpha} = \\left|\\frac{f''(x_*)}{2f'(x_*)}\\right|^{1/\\alpha},\n\\] so the secant method has order of convergence \\(\\alpha\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  }
]