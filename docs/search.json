[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "",
    "text": "Introduction\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nWelcome to Computational Mathematics II!\nThis course aims to help you build skills and knowledge in using modern computational methods to do and apply mathematics. It will involve a blend of hands-on computing work and mathematical theory—this theory will include aspects of numerical analysis, computational algebra, and other topics within scientific computing. These areas consist of studying the mathematical properties of the computational representations of mathematical objects (numerical values as well as symbolic manipulations). The computing skills developed in this module will be valuable in all subsequent courses in your degree at Durham and well beyond. We will also introduce you to the use (and abuse) of various computational tools invaluable for doing mathematics, such as AI and searchable websites. While we will encourage you throughout to use all the tools at your disposal, it is imperative that you understand the details and scope of what you are doing! You will also develop your communication, presentation, and group-work skills through the various assessments involved in the course – more on that below!\nThis module has no final exam. In fact, there are no exams of any kind. Instead, the summative assessment and associated final grade are entirely based on coursework undertaken during the term. This means that you should expect to spend more time on this course during the term relative to your other modules. We believe this workload distribution is a better way to train the skills we are trying to develop, and as a bonus, you will not need to worry about this course any further once the term ends!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Content",
    "text": "Content\nThe module’s content is divided into six chapters of roughly equal length; some will focus slightly more on theory, while others have a more practical and hands-on nature.\n\nChapter 1: Introduction to Computational Mathematics\n\nProgramming basics (including GitHub, and numerical versus symbolic computation)\nLaTeX, Overleaf, and presenting lab reports\nFinite-precision arithmetic, rounding error, symbolic representations\n\nChapter 2: Continuous Functions\n\nInterpolation using polynomials – fitting curves to data (Lagrange polynomials, error estimates, convergence, and Chebyshev nodes)\nSolving nonlinear equations (bisection, fixed-point iteration, Newton’s method)\n\nChapter 3: Linear Algebra\n\nSolving linear systems numerically (LU decomposition, Gaussian elimination, conditioning) and symbolically\nApplications: PageRank, computer graphics\n\nChapter 4: Calculus\n\nNumerical differentiation (finite differences)\nNumerical integration (quadrature rules, Newton-Cotes formulae)\n\nChapter 5: Ordinary Differential Equations (ODEs)\n\nNumerically approximating solutions of ODEs\nTimestepping: explicit and implicit methods\nStability and convergence order\n\nChapter 6: Selected Further Topics\n\nIntro. to random numbers and stochastic processes\nIntro. to partial differential equations",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#weekly-workflow-and-summative-assessment",
    "href": "index.html#weekly-workflow-and-summative-assessment",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Weekly workflow and summative assessment",
    "text": "Weekly workflow and summative assessment\nThe final grade for this module is determined as follows:\n\nWeekly lab reports (weeks 1-6) – 20%\nWeekly e-assessments (weeks 1-6) – 30%\nProject (weeks 7-10) – 50%\n\n\nLab reports\nEach week for the first six weeks of the course, we will release a short set of exercises based on the lectures from the previous week. Students will be expected to submit a brief report (1-2 pages A4, including figures) with their solutions to the set of exercises – the report will consist of written answers and figures/plots. The reports will be evaluated for correctness and quality of the presentation and communication (quality of figures, clarity of argumentation, etc.).\nThe lab report for a given week will be due at noon on Monday of the following week (e.g., week one’s lab report is due on Monday of week two and so on). Solutions and generalised feedback will be provided to the class on common mistakes and issues arising in each report. Students can also seek detailed feedback on their submission from the lecturers during drop-in sessions and office hours. There will be six lab reports in total, and your mark is based on your four highest-scoring submissions.\n\n\nE-assessments\nEach week for the first six weeks of the course, we will release an e-assessment based on the lectures from the previous week. These exercises are designed to complement the lab reports by focusing exclusively on coding skills. The e-assessments will involve submitting code auto-marked by an online grading tool, and hence give immediate feedback. As with the lab reports, the e-assessment for a given week will be due at noon on Monday of the following week. There will be six e-assessments in total, and your mark is based on your four highest-scoring submissions.\n\n\nProject\nThe single largest component of the assessment for this module is the project. Weeks 7-10 of this course focus exclusively on project work with lectures ending in Week 6. We will be releasing more detailed instructions on the project submission format and assessment criteria separately, but briefly, the main aspects of the project are as follows:\n\nThere will be approximately eight different project options to choose from across different areas of mathematics (e.g., pure, applied, probability, mathematical physics, etc.); each project has a distinct member of the Maths Department as supervisor.\nStudents will submit their preferred project options (ranked choice preferences) in Week 4 of the term and be allocated to projects by the end of Week 6 (there are maximum subscription numbers for each option to ensure equity of supervision).\nEach project consists of two parts: a guided component that is completed as part of a small group and an extension component that is open-ended and completed as an individual. Group allocations will be done by the lecturers.\nEach group will jointly submit a five-page report for the guided component of the project, and this is worth 60% of the project grade.\nEach student will also submit a three-page report and a six-minute video presentation on their extension component. This submission is worth 40% of the project grade.\n\nIn Weeks 7-10 of the term, lectures will be replaced by project workshop sessions during which students can discuss their project with the designated supervisor. This will be an opportunity to discuss progress, ask questions, and seek clarification. Each student only needs to attend the one project drop-in weekly session relevant to their project. Computing drop-in sessions will continue as scheduled in the first six weeks to provide additional support for coding pertinent tasks for the projects – there will be two timetabled computing drop-ins per week and students are encouraged to attend at least one of them.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#lectures-computing-drop-ins-project-workshops",
    "href": "index.html#lectures-computing-drop-ins-project-workshops",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Lectures, computing drop-ins & project workshops",
    "text": "Lectures, computing drop-ins & project workshops\nLectures will primarily present, explain, and discuss new material (especially theory), but will also feature computer demonstrations of the algorithms and numerical methods. As such, students are encouraged to bring their laptops to lectures to run the examples themselves. Students must bring a laptop or device capable of running code to the computer drop-ins to work on the e-assessments and lab reports.\n\n\n\n\n\nActivities\nContent\n\n\n\n\nWeek 1\nIntroductory lecture, 2 lectures\nChapter 1\n\n\nWeek 2\n3 lectures, 1 computing drop-in\nChapter 2\n\n\nWeek 3\n3 lectures, 1 computing drop-in\nChapter 3\n\n\nWeek 4\n3 lectures, 1 computing drop-in\nChapter 4\n\n\nWeek 5\n3 lectures, 1 computing drop-in\nChapter 5\n\n\nWeek 6\n3 lectures, 1 computing drop-in\nChapter 5/6\n\n\nWeek 7\n0 lectures, 1 project workshop\nProject\n\n\nWeek 8\n0 lectures, 1 project workshop\nProject\n\n\nWeek 9\n0 lectures, 1 project workshop\nProject\n\n\nWeek 10\n0 lectures, 1 project workshop\nProject",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#contact-details-and-reading-materials",
    "href": "index.html#contact-details-and-reading-materials",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Contact details and Reading Materials",
    "text": "Contact details and Reading Materials\nIf you have questions or need clarification on any of the above, please speak to us during lectures, drop-in sessions, or office hours. Alternatively, email one or both of us at denis.d.patterson@durham.ac.uk or andrew.krause@durham.ac.uk.\nThe lecture notes are designed to be sufficient and self-contained. Hence, students do not need to purchase a textbook to complete the course successfully. References for additional reading will also be given at the end of each chapter.\nThe following texts may be useful supplementary references for students wishing to read further into topics from the course:\n\nBurden, R. L., & Faires, J. D. (1997). Numerical Analysis (6th ed.). Pacific Grove, CA: Brooks/Cole Publishing Company.\nSüli, E., & Mayers, D. F. (2003). An Introduction to Numerical Analysis. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Computational Mathematics II (MATH2731)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe are indebted to Prof. Anthony Yeates (Durham) whose numerical analysis notes formed the basis of several chapters of the coures notes.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chap-one.html",
    "href": "chap-one.html",
    "title": "1  Numerical and Symbolic Computing",
    "section": "",
    "text": "1.1 Fixed-point numbers\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nThe goal of this chapter is to explore and begin to answer the following question:\nIntegers and arithmetic operations on computers can be represented exactly, up to some maximum size.\nIn contrast to the integers, only a subset of real numbers within any given interval can be represented exactly.\nSymbolic expressions representing a range of mathematical objects and operations can also be manipulated exactly using Computer Algebra Systems (CAS), although such operations are almost always much slower than numerical computations using integer or real-valued numbers. These dualities between numerical and symbolic computation will be a key theme throughout the course.\nIn everyday life, we tend to use a fixed point representation of real numbers: \\[\nx = \\pm (d_1d_2\\cdots d_{k-1}.d_k\\cdots d_n)_\\beta, \\quad \\textrm{where} \\quad d_1,\\ldots,d_n\\in\\{0,1,\\ldots,\\beta - 1\\}.\n\\] Here \\(\\beta\\) is the base (e.g. 10 for decimal arithmetic or 2 for binary).\nIf we require that \\(d_1\\neq 0\\) unless \\(k=2\\), then every number has a unique representation of this form, except for infinite trailing sequences of digits \\(\\beta - 1\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numerical and Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "chap-one.html#floating-point-numbers",
    "href": "chap-one.html#floating-point-numbers",
    "title": "1  Numerical and Symbolic Computing",
    "section": "1.2 Floating-point numbers",
    "text": "1.2 Floating-point numbers\nComputers use a floating-point representation. Only numbers in a floating-point number system \\(F\\subset\\mathbb{R}\\) can be represented exactly, where \\[\nF = \\big\\{ \\pm (0.d_1d_2\\cdots d_{m})_\\beta\\beta^e \\;| \\;  \\beta, d_i, e \\in \\mathbb{Z}, \\;0 \\leq d_i \\leq \\beta-1, \\;e_{\\rm min} \\leq e \\leq e_{\\rm max}\\big\\}.\n\\] Here \\((0.d_1d_2\\cdots d_{m})_\\beta\\) is called the fraction (or significand or mantissa), \\(\\beta\\) is the base, and \\(e\\) is the exponent. This can represent a much larger range of numbers than a fixed-point system of the same size, although at the cost that the numbers are not equally spaced. If \\(d_1\\neq 0\\) then each number in \\(F\\) has a unique representation and \\(F\\) is called normalised.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that the spacing between numbers jumps by a factor \\(\\beta\\) at each power of \\(\\beta\\). The largest possible number is \\((0.111)_22^2 = (\\tfrac12 + \\tfrac14 + \\tfrac18)(4) = \\tfrac72\\). The smallest non-zero number is \\((0.100)_22^{-1}=\\tfrac12(\\tfrac12) = \\tfrac14\\).\n\n\n\n\nHere \\(\\beta=2\\), and there are 52 bits for the fraction, 11 for the exponent, and 1 for the sign. The actual format used is \\[\n\\pm (1.d_1\\cdots d_{52})_22^{e-1023} = \\pm (0.1d_1\\cdots d_{52})_22^{e-1022}, \\quad e = (e_1e_2\\cdots e_{11})_2.\n\\] When \\(\\beta=2\\), the first digit of a normalized number is always \\(1\\), so doesn’t need to be stored in memory. The exponent bias of 1022 means that the actual exponents are in the range \\(-1022\\) to \\(1025\\), since \\(e\\in[0,2047]\\). Actually the exponents \\(-1022\\) and \\(1025\\) are used to store \\(\\pm 0\\) and \\(\\pm\\infty\\) respectively.\nThe smallest non-zero number in this system is \\((0.1)_22^{-1021} \\approx 2.225\\times 10^{-308}\\), and the largest number is \\((0.1\\cdots 1)_22^{1024} \\approx 1.798\\times 10^{308}\\).\n\n\n\n\n\n\n\nIEEE stands for Institute of Electrical and Electronics Engineers. Matlab uses the IEEE 754 standard for floating point arithmetic. The automatic 1 is sometimes called the “hidden bit”. The exponent bias avoids the need to store the sign of the exponent.\n\n\n\nNumbers outside the finite set \\(F\\) cannot be represented exactly. If a calculation falls below the lower non-zero limit (in absolute value), it is called underflow, and usually set to 0. If it falls above the upper limit, it is called overflow, and usually results in a floating-point exception.\n\n\n\n\n\n\nAriane 5 rocket failure (1996): The maiden flight ended in failure. Only 40 seconds after initiation, at altitude 3700m, the launcher veered off course and exploded. The cause was a software exception during data conversion from a 64-bit float to a 16-bit integer. The converted number was too large to be represented, causing an exception.\n\n\n\n\n\n\n\n\n\nIn IEEE arithmetic, some numbers in the “zero gap” can be represented using \\(e=0\\), since only two possible fraction values are needed for \\(\\pm 0\\). The other fraction values may be used with first (hidden) bit 0 to store a set of so-called subnormal numbers.\n\n\n\nThe mapping from \\(\\mathbb{R}\\) to \\(F\\) is called rounding and denoted \\(\\mathrm{fl}(x)\\). Usually it is simply the nearest number in \\(F\\) to \\(x\\). If \\(x\\) lies exactly midway between two numbers in \\(F\\), a method of breaking ties is required. The IEEE standard specifies round to nearest even—i.e., take the neighbour with last digit 0 in the fraction.\n\n\n\n\n\n\nThis avoids statistical bias or prolonged drift.\n\n\n\n\n\n\n\n\n\n\\(\\tfrac98 = (1.001)_2\\) has neighbours \\(1 = (0.100)_22^1\\) and \\(\\tfrac54 = (0.101)_22^1\\), so is rounded down to \\(1\\).\n\\(\\tfrac{11}{8} = (1.011)_2\\) has neighbours \\(\\tfrac54 = (0.101)_22^1\\) and \\(\\tfrac32=(0.110)_22^1\\), so is rounded up to \\(\\tfrac32\\).\n\n\n\n\n\n\n\nVancouver stock exchange index: In 1982, the index was established at 1000. By November 1983, it had fallen to 520, even though the exchange seemed to be doing well. Explanation: the index was rounded down to 3 digits at every recomputation. Since the errors were always in the same direction, they added up to a large error over time. Upon recalculation, the index doubled!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numerical and Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "chap-one.html#significant-figures",
    "href": "chap-one.html#significant-figures",
    "title": "1  Numerical and Symbolic Computing",
    "section": "1.3 Significant figures",
    "text": "1.3 Significant figures\nWhen doing calculations without a computer, we often use the terminology of significant figures. To count the number of significant figures in a number \\(x\\), start with the first non-zero digit from the left, and count all the digits thereafter, including final zeros if they are after the decimal point.\n\n\n\nTo round \\(x\\) to \\(n\\) s.f., replace \\(x\\) by the nearest number with \\(n\\) s.f. An approximation \\(\\hat{x}\\) of \\(x\\) is “correct to \\(n\\) s.f.” if both \\(\\hat{x}\\) and \\(x\\) round to the same number to \\(n\\) s.f.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numerical and Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "chap-one.html#rounding-error",
    "href": "chap-one.html#rounding-error",
    "title": "1  Numerical and Symbolic Computing",
    "section": "1.4 Rounding error",
    "text": "1.4 Rounding error\nIf \\(|x|\\) lies between the smallest non-zero number in \\(F\\) and the largest number in \\(F\\), then \\[\n\\mathrm{fl}(x) = x(1+\\delta),\n\\] where the relative error incurred by rounding is \\[\n|\\delta| = \\frac{|\\mathrm{fl}(x) - x|}{|x|}.\n\\]\n\n\n\n\n\n\nRelative errors are often more useful because they are scale invariant. E.g., an error of 1 hour is irrelevant in estimating the age of a lecture theatre, but catastrophic in timing your arrival at the lecture.\n\n\n\nNow \\(x\\) may be written as \\(x=(0.d_1d_2\\cdots)_\\beta\\beta^e\\) for some \\(e\\in[e_{\\rm min},e_{\\rm max}]\\), but the fraction will not terminate after \\(m\\) digits if \\(x\\notin F\\). However, this fraction will differ from that of \\(\\mathrm{fl}(x)\\) by at most \\(\\tfrac12\\beta^{-m}\\), so \\[\n|\\mathrm{fl}(x) - x| \\leq \\tfrac12\\beta^{-m}\\beta^e \\quad \\implies \\quad |\\delta| \\leq \\tfrac12\\beta^{1-m}.\n\\] Here we used that the fractional part of \\(|x|\\) is at least \\((0.1)_\\beta \\equiv \\beta^{-1}\\). The number \\(\\epsilon_{\\rm M} = \\tfrac12\\beta^{1-m}\\) is called the machine epsilon (or unit roundoff), and is independent of \\(x\\). So the relative rounding error satisfies \\[\n|\\delta| \\leq \\epsilon_{\\rm M}.\n\\]\n\n\n\n\n\n\nTo check the machine epsilon value in Matlab you can just type ‘eps’ in the command line, which will return the value 2.2204e-16.\n\n\n\n\n\n\n\n\n\nThe name “unit roundoff” arises because \\(\\beta^{1-m}\\) is the distance between 1 and the next number in the system.\n\n\n\n\n\n\nWhen adding/subtracting/multiplying/dividing two numbers in \\(F\\), the result will not be in \\(F\\) in general, so must be rounded.\n\nLet us multiply \\(x=\\tfrac58\\) and \\(y=\\tfrac78\\). We have \\[\nxy = \\tfrac{35}{64} = \\tfrac12 + \\tfrac1{32} + \\tfrac1{64} = (0.100011)_2.\n\\] This has too many significant digits to represent in our system, so the best we can do is round the result to \\(\\mathrm{fl}(xy) = (0.100)_2 = \\tfrac12\\).\n\n\n\n\n\n\n\nTypically additional digits are used during the computation itself, as in our example.\n\n\n\nFor \\({\\circ} = +,-,\\times, \\div\\), IEEE standard arithmetic requires rounded exact operations, so that \\[\n\\mathrm{fl}(x {\\,\\circ\\,} y) = (x {\\,\\circ\\,} y)(1+\\delta), \\quad |\\delta|\\leq\\epsilon_{\\rm M}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numerical and Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "chap-one.html#loss-of-significance",
    "href": "chap-one.html#loss-of-significance",
    "title": "1  Numerical and Symbolic Computing",
    "section": "1.5 Loss of significance",
    "text": "1.5 Loss of significance\nYou might think that the above guarantees the accuracy of calculations to within \\(\\epsilon_{\\rm M}\\), but this is true only if \\(x\\) and \\(y\\) are themselves exact. In reality, we are probably starting from \\(\\bar{x}=x(1+\\delta_1)\\) and \\(\\bar{y}=y(1 + \\delta_2)\\), with \\(|\\delta_1|, |\\delta_2| \\leq \\epsilon_{\\rm M}\\). In that case, there is an error even before we round the result, since \\[\n\\begin{aligned}\n\\bar{x} \\pm \\bar{y} &= x(1+ \\delta_1) \\pm y(1 + \\delta_2)\\\\\n&= (x\\pm y)\\left(1 + \\frac{x\\delta_1 \\pm y\\delta_2}{x\\pm y}\\right).\n\\end{aligned}\n\\] If the correct answer \\(x\\pm y\\) is very small, then there can be an arbitrarily large relative error in the result, compared to the errors in the initial \\(\\bar{x}\\) and \\(\\bar{y}\\). In particular, this relative error can be much larger than \\(\\epsilon_{\\rm M}\\). This is called loss of significance, and is a major cause of errors in floating-point calculations.\n\nTo 4 s.f., the roots are \\[\nx_1 = 28 + \\sqrt{783} = 55.98, \\quad x_2 = 28-\\sqrt{783} = 0.01786.\n\\] However, working to 4 s.f. we would compute \\(\\sqrt{783} = 27.98\\), which would lead to the results \\[\n\\bar{x}_1 = 55.98, \\quad \\bar{x}_2 = 0.02000.\n\\] The smaller root is not correct to 4 s.f., because of cancellation error. One way around this is to note that \\(x^2 - 56x + 1 = (x-x_1)(x-x_2)\\), and compute \\(x_2\\) from \\(x_2 = 1/x_1\\), which gives the correct answer.\n\n\n\n\n\n\n\nNote that the error crept in when we rounded \\(\\sqrt{783}\\) to \\(27.98\\), because this removed digits that would otherwise have been significant after the subtraction.\n\n\n\n\nLet us plot this function in the range \\(-5\\times 10^{-8}\\leq x \\leq 5\\times 10^{-8}\\) – even in IEEE double precision arithmetic we find significant errors, as shown by the blue curve:\n\n\n\n\n\nThe red curve shows the correct result approximated using the Taylor series \\[\n\\begin{aligned}\nf(x) &= \\left(1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\ldots\\right) - \\left( 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\ldots\\right) - x\\\\\n&\\approx x^2 + \\frac{x^3}{6}.\n\\end{aligned}\n\\] This avoids subtraction of nearly equal numbers.\n\n\n\n\n\n\n\nWe will look in more detail at polynomial approximations in the next section.\n\n\n\nNote that floating-point arithmetic violates many of the usual rules of real arithmetic, such as \\((a+b)+c = a + (b+c)\\).\n\n\\[\n\\begin{aligned}\n\\mathrm{fl}\\big[(5.9 + 5.5) + 0.4\\big] &= \\mathrm{fl}\\big[\\mathrm{fl}(11.4) + 0.4\\big] = \\mathrm{fl}(11.0 + 0.4) = 11.0,\\\\\n\\mathrm{fl}\\big[5.9 + (5.5 + 0.4)\\big] &= \\mathrm{fl}\\big[5.9 + 5.9 \\big] = \\mathrm{fl}(11.8) = 12.0.\n\\end{aligned}\n\\]\n\n\nIn \\(\\mathbb{R}\\), the average of two numbers always lies between the numbers. But if we work to 3 decimal digits, \\[\n\\mathrm{fl}\\left(\\frac{5.01 + 5.02}{2}\\right) = \\frac{\\mathrm{fl}(10.03)}{2} = \\frac{10.0}{2} = 5.0.\n\\]\n\nThe moral of the story is that sometimes care is needed to ensure that we carry out a calculation accurately and as intended!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numerical and Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "chap-one.html#example-weather-modelling",
    "href": "chap-one.html#example-weather-modelling",
    "title": "1  Numerical and Symbolic Computing",
    "section": "1.6 Example: Weather Modelling",
    "text": "1.6 Example: Weather Modelling\n\nA very simplistic description of an atmospheric fluid is given by the Lorenz system of differential equations: \\[\n\\begin{aligned}\n\\frac{dx}{dt} &= \\sigma(y-x),\\\\\n\\frac{dy}{dt} &= x(\\rho-z)-y,\\\\\n\\frac{dz}{dt} &= xy-\\beta z,\n\\end{aligned}\n\\] where \\(\\sigma\\), \\(\\rho\\), and \\(\\beta\\) are positive constants\n\nWe will describe how to solve such equations numerically in Chapter 5. For now, let’s look at a simulation of these equations computed by just iterating a bunch of addition and multiplication steps:\n\n\n\n\n\nHere we plot all three variables over a particular window of time (loosely corresponding to something between minutes to hours, depending on several details of the physics we are neglecting). For each variable, we have ran two simulations with two different values of \\(\\beta\\). One simulation has \\(\\beta = 8/3 = 2.66666666666\\dots\\) and the other has \\(\\beta = 8/3 + 10^{-10} = 2.66666666676\\dots\\). Nevertheless, you can clearly see that this tiny difference in parameters (well below what is experimentally measurable) has a huge impact on the solution. The reason for this, and a key reason why weather is fundamentally hard to predict, is the existence of chaos in many models of physical phenomena. However truncation errors and floating point errors also contribute to the discrepancies in this simulation.\nThe overall lesson of this Chapter so far is that small errors can exist due to analytically-understood reasons such as the truncation error in a Taylor series approximation, or the floating point error in the numerical methods used. Such errors can not only impact individual calculations, but they can accumulate and entirely change outcomes of simulations, as we will see throughout the course. Nevertheless, computational methods still underly an enormous range of scientific fields, so understanding these sources of error (and how they can become amplified) is a central theme of this course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numerical and Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "chap-one.html#exact-and-symbolic-computing",
    "href": "chap-one.html#exact-and-symbolic-computing",
    "title": "1  Numerical and Symbolic Computing",
    "section": "1.7 Exact and symbolic computing",
    "text": "1.7 Exact and symbolic computing\nSymbolic computations require different data types from numerical ones; in MATLAB we can use the Symbolic Math Toolbox. In the following chapters, we will compare symbolic and numerical approaches to solving mathematical problems. One key difference is that symbolic computations are exact, but much more expensive to scale up to solve larger problems; for example, we will tackle numerical problems involving matrices of size \\(10^4 \\times 10^4\\) or larger, which would not be possible to successfully manipulate symbolically on a modern computer.\nSymbolic computations are used to check or do laborious analytical work, but also to rigorously prove mathematical Theorems which would be too onerous to carry out by hand. FOr instance, while Lorenz equations above were first noted to have strange dependencies on initial conditions, it was only in 2002 that this was mathematically proved to be a chaotic system. Among other tools, the proof relied on interval arithmetic, which is a method to exactly operate on intervals defined in terms of two rational numbers.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numerical and Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "chap-one.html#knowledge-checklist",
    "href": "chap-one.html#knowledge-checklist",
    "title": "1  Numerical and Symbolic Computing",
    "section": "Knowledge checklist",
    "text": "Knowledge checklist\nKey topics:\n\nInteger and floating point representations of real numbers on computers.\nOverflow, underflow and loss of significance.\nSymbolic and numerical representations.\n\nKey skills:\n\nUnderstanding and distinguishing integer, fixed-point, and floating-point representations.\nAnalyzing the effects of rounding and machine epsilon in calculations.\nDiagnosing and managing rounding errors, overflow, and underflow.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numerical and Symbolic Computing</span>"
    ]
  },
  {
    "objectID": "chap-two.html",
    "href": "chap-two.html",
    "title": "2  Continuous Functions",
    "section": "",
    "text": "2.1 Interpolation\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nThe goal of this chapter is to explore and begin to answer the following question:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#interpolation",
    "href": "chap-two.html#interpolation",
    "title": "2  Continuous Functions",
    "section": "",
    "text": "2.1.1 Polynomial Interpolation: Motivation\nThe main idea of this section is to find a polynomial that approximates a general function \\(f\\). But why polynomials? Polynomials have many nice mathematical properties but from the perspective of function approximation, the key one is the following: Any continuous function on a compact interval can be approximated to arbitrary accuracy using a polynomial (provided you are willing to go high enough degree).\n\nTheorem 2.1: Weierstrass Approximation Theorem (1885)For any \\(f\\in C([0,1])\\) and any \\(\\epsilon &gt; 0\\), there exists a polynomial \\(p(x)\\) such that \\[\n\\max_{0\\leq x\\leq 1}\\big|f(x) - p(x)\\big| \\leq \\epsilon.\n\\]\n\n\n\n\n\n\n\n\nThis may be proved using an explicit sequence of polynomials, called Bernstein polynomials.\n\n\n\nIf \\(f\\) is a polynomial of degree \\(n\\), \\[\nf(x) = p_n(x) = a_0 + a_1x + \\ldots + a_nx^n,\n\\] then we only need to store the \\(n+1\\) coefficients \\(a_0,\\ldots,a_n\\). Operations such as taking the derivative or integrating \\(f\\) are also convenient. If \\(f\\) is not continuous, then something other than a polynomial is required, since polynomials can’t handle asymptotic behaviour.\n\n\n\n\n\n\nTo approximate functions like \\(1/x\\), there is a well-developed theory of rational function interpolation, which is beyond the scope of this course.\n\n\n\nIn this chapter, we look for a suitable polynomial \\(p_n\\) by interpolation—that is, requiring \\(p_n(x_i) = f(x_i)\\) at a finite set of points \\(x_i\\), usually called nodes. Sometimes we will also require the derivative(s) of \\(p_n\\) to match those of \\(f\\). This type of function approximation where we want to match values of the function that we know at particular points is very natural in many applications. For example, weather forecasts involve numerically solving huge systems of partial differential equations (PDEs), which means actually solving them on a discrete grid of points. If we want weather predictions between grid points, we must interpolate. Figure Figure 2.1 shows the spatial resolutions of a range of current and past weather models produced by the UK Met Office.\n\n\n\n\n\n\nFigure 2.1: Chart showing a range of weather models produce by the UK Met Office. Even the highest spatial resolution models have more than 1.5km between grid point due to computational constraints.\n\n\n\n\n\n2.1.2 Taylor series\nA truncated Taylor series is (in some sense) the simplest interpolating polynomial since it uses only a single node \\(x_0\\), although it does require \\(p_n\\) to match both \\(f\\) and some of its derivatives.\n\nWe can approximate this using a Taylor series about the point \\(x_0=0\\), which is \\[\n\\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\ldots.\n\\] This comes from writing \\[\nf(x) = a_0 + a_1(x-x_0) + a_2(x-x_0)^2 + \\ldots,\n\\] then differentiating term-by-term and matching values at \\(x_0\\): \\[\\begin{align*}\nf(x_0) &= a_0,\\\\\nf'(x_0) &= a_1,\\\\\nf''(x_0) &= 2a_2,\\\\\nf'''(x_0) &= 3(2)a_3,\\\\\n&\\vdots\\\\\n\\implies f(x) &= f(x_0) + f'(x_0)(x-x_0) + \\frac{f''(x_0)}{2!}(x-x_0)^2 + \\frac{f'''(x_0)}{3!}(x-x_0)^3 + \\ldots.\n\\end{align*}\\] So \\[\\begin{align*}\n\\textrm{1 term} \\;&\\implies\\; f(0.1) \\approx 0.1,\\\\\n\\textrm{2 terms} \\;&\\implies\\; f(0.1) \\approx 0.1 - \\frac{0.1^3}{6} = 0.099833\\ldots,\\\\\n\\textrm{3 terms} \\;&\\implies\\; f(0.1) \\approx 0.1 - \\frac{0.1^3}{6} + \\frac{0.1^5}{120} = 0.09983341\\ldots.\\\\\n\\end{align*}\\] The next term will be \\(-0.1^7/7! \\approx -10^{-7}/10^3 = -10^{-10}\\), which won’t change the answer to 6 s.f.\n\n\n\n\n\n\n\nThe exact answer is \\(\\sin(0.1)=0.09983341\\).\n\n\n\nMathematically, we can write the remainder as follows.\n\nTheorem 2.2: Taylor’s TheoremLet \\(f\\) be \\(n+1\\) times differentiable on \\((a,b)\\), and let \\(f^{(n)}\\) be continuous on \\([a,b]\\). If \\(x,x_0\\in[a,b]\\) then there exists \\(\\xi \\in (a,b)\\) such that \\[\nf(x) = \\sum_{k=0}^n\\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k \\; + \\; \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)^{n+1}.\n\\]\n\n\nThe sum is called the Taylor polynomial of degree \\(n\\), and the last term is called the Lagrange form of the remainder. Note that the unknown number \\(\\xi\\) depends on \\(x\\).\n\nFor \\(f(x)=\\sin(x)\\), we found the Taylor polynomial \\(p_6(x) = x - x^3/3! + x^5/5!\\), and \\(f^{(7)}(x)=-\\sin(x)\\). So we have \\[\n\\big|f(x) - p_6(x)\\big| = \\left|\\frac{f^{(7)}(\\xi)}{7!}(x-x_0)^7\\right|\n\\] for some \\(\\xi\\) between \\(x_0\\) and \\(x\\). For \\(x=0.1\\), we have \\[\n\\big|f(0.1) - p_6(0.1)\\big| = \\frac{1}{5040}(0.1)^7\\big|f^{(7)}(\\xi)\\big| \\quad \\textrm{for some $\\xi\\in[0,0.1]$}.\n\\] Since \\(\\big|f^{(7)}(\\xi)\\big| = \\big|\\sin(\\xi)\\big| \\leq 1\\), we can say, before calculating, that the error satisfies \\[\n\\big|f(0.1) - p_6(0.1)\\big| \\leq 1.984\\times 10^{-11}.\n\\]\n\n\n\n\n\n\n\nThe actual error is \\(1.983\\times 10^{-11}\\), so this is a tight estimate.\n\n\n\nSince this error arises from approximating \\(f\\) with a truncated series, rather than due to rounding, it is known as truncation error. Note that it tends to be lower if you use more terms (larger \\(n\\)), or if the function oscillates less (smaller \\(f^{(n+1)}\\) on the interval \\((x_0,x)\\)).\nError estimates like the Lagrange remainder play an important role in numerical analysis and computation, so it is important to understand where it comes from. The number \\(\\xi\\) will ultimately come from Rolle’s theorem, which is a special case of the mean value theorem from first-year calculus:\n\nTheorem 2.3: Rolle’s TheoremIf \\(f\\) is continuous on \\([a,b]\\) and differentiable on \\((a,b)\\), with \\(f(a)=f(b)=0\\), then there exists \\(\\xi\\in(a,b)\\) with \\(f'(\\xi)=0\\).\n\n\n\n\n\n\n\n\nNote that Rolle’s Theorem does not tell us what the value of \\(\\xi\\) might actually be, so in practice we must take some kind of worst case estimate to get an error bound, e.g. calculate the max value of \\(f'(\\xi)\\) over the range of possible \\(\\xi\\) values.\n\n\n\n\n\n2.1.3 Polynomial Interpolation\nThe classical problem of polynomial interpolation is to find a polynomial \\[\np_n(x) = a_0 + a_1x + \\ldots + a_n x^n = \\sum_{k=0}^n a_k x^k\n\\] that interpolates our function \\(f\\) at a finite set of nodes \\(\\{x_0, x_1, \\ldots, x_m\\}\\). In other words, \\(p_n(x_i)=f(x_i)\\) at each of the nodes \\(x_i\\). Since the polynomial has \\(n+1\\) unknown coefficients, we expect to need \\(n+1\\) distinct nodes, so let us assume that \\(m=n\\).\n\nHere we have two nodes \\(x_0\\), \\(x_1\\), and seek a polynomial \\(p_1(x) = a_0 + a_1x\\). Then the interpolation conditions require that \\[\n\\begin{cases}\np_1(x_0) = a_0 + a_1x_0 = f(x_0)\\\\\np_1(x_1) = a_0 + a_1x_1 = f(x_1)\n\\end{cases}\n\\implies\\quad\np_1(x) = \\frac{x_1f(x_0) - x_0f(x_1)}{x_1 - x_0} + \\frac{f(x_1) - f(x_0)}{x_1 - x_0}x.\n\\]\n\nFor general \\(n\\), the interpolation conditions require \\[\n\\begin{matrix}\na_0 &+ a_1x_0 &+ a_2x_0^2 &+ \\ldots &+ a_nx_0^n &= f(x_0),\\\\\na_0 &+ a_1x_1 &+ a_2x_1^2 &+ \\ldots &+ a_nx_1^n &= f(x_1),\\\\\n\\vdots  & \\vdots  & \\vdots     &        &\\vdots      & \\vdots\\\\\na_0 &+ a_1x_n &+ a_2x_n^2 &+ \\ldots &+ a_nx_n^n &= f(x_n),\n\\end{matrix}\n\\] so we have to solve \\[\n\\begin{pmatrix}\n1 & x_0 & x_0^2 & \\cdots & x_0^n\\\\\n1 & x_1 & x_1^2 & \\cdots & x_1^n\\\\\n\\vdots & \\vdots &\\vdots& & \\vdots\\\\\n1 & x_n & x_n^2 & \\cdots & x_n^n\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\na_0\\\\ a_1\\\\ \\vdots\\\\ a_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf(x_0)\\\\ f(x_1)\\\\ \\vdots\\\\ f(x_n)\n\\end{pmatrix}.\n\\] This is called a Vandermonde matrix. The determinant of this matrix is \\[\n\\det(A) = \\prod_{0\\leq i &lt; j\\leq n} (x_j - x_i),\n\\] which is non-zero provided the nodes are all distinct. This establishes an important result, where \\(\\mathcal{P}_n\\) denotes the space of all real polynomials of degree \\(\\leq n\\).\n\nTheorem 2.4: Existence/uniquenessGiven \\(n+1\\) distinct nodes \\(x_0, x_1, \\ldots, x_n\\), there is a unique polynomial \\(p_n\\in\\mathcal{P}_n\\) that interpolates \\(f(x)\\) at these nodes.\n\n\nWe may also prove uniqueness by the following elegant argument.\nProof (Uniqueness part of Existence/Uniqueness Theorem):\nSuppose that in addition to \\(p_n\\) there is another interpolating polynomial \\(q_n\\in\\mathcal{P}_n\\). Then the difference \\(r_n := p_n - q_n\\) is also a polynomial with degree \\(\\leq n\\). But we have \\[\nr_n(x_i) = p_n(x_i) - q_n(x_i) = f(x_i)-f(x_i)=0 \\quad \\textrm{for $i=0,\\ldots,n$},\n\\] so \\(r_n(x)\\) has \\(n+1\\) roots. From the Fundamental Theorem of Algebra, this is possible only if \\(r_n(x)\\equiv 0\\), which implies that \\(q_n=p_n\\).\n\n\n\n\n\n\nNote that the unique polynomial through \\(n+1\\) points may have degree \\(&lt; n\\). This happens when \\(a_0=0\\) in the solution to the Vandermonde system above.\n\n\n\n\nWe have \\(x_0=0\\), \\(x_1=\\tfrac{\\pi}{2}\\), \\(x_2=\\pi\\), so \\(f(x_0)=1\\), \\(f(x_1)=0\\), \\(f(x_2)=-1\\). Clearly the unique interpolant is a straight line \\(p_2(x) = 1 - \\tfrac2\\pi x\\).\nIf we took the nodes \\(\\{0,2\\pi,4\\pi\\}\\), we would get a constant function \\(p_2(x)=1\\).\n\n\n\n\n\n\nOne way to compute the interpolating polynomial would be to solve the Vandermonde system above, e.g. by Gaussian elimination. However, this is not recommended. In practice, we choose a different basis for \\(p_n\\); there are two common and effective choices due to Lagrange and Newton.\n\n\n\n\n\n\nThe Vandermonde matrix arises when we write \\(p_n\\) in the natural basis \\(\\{1,x,x^2,\\ldots\\}\\), but we could also choose to work in some other basis…\n\n\n\n\n\n2.1.4 Lagrange Polynomials\nThis uses a special basis of polynomials \\(\\{\\ell_k\\}\\) in which the interpolation equations reduce to the identity matrix. In other words, the coefficients in this basis are just the function values, \\[\np_n(x) = \\sum_{k=0}^n f(x_k)\\ell_k(x).\n\\]\n\nExample 2.1: Linear interpolation again.We can re-write our linear interpolant to separate out the function values: \\[\np_1(x) = \\underbrace{\\frac{x - x_1}{x_0 - x_1}}_{\\ell_0(x)}f(x_0) + \\underbrace{\\frac{x-x_0}{x_1-x_0}}_{\\ell_1(x)}f(x_1).\n\\] Then \\(\\ell_0\\) and \\(\\ell_1\\) form the necessary basis. In particular, they have the property that \\[\n\\ell_0(x_i) = \\begin{cases}\n1 & \\textrm{if $i=0$},\\\\\n0 & \\textrm{if $i=1$},\n\\end{cases}\n\\qquad\n\\ell_1(x_i) = \\begin{cases}\n0 & \\textrm{if $i=0$},\\\\\n1 & \\textrm{if $i=1$},\n\\end{cases}\n\\]\n\n\nFor general \\(n\\), the \\(n+1\\) Lagrange polynomials are defined as a product \\[\n\\ell_k(x) = \\prod_{\\substack{j=0\\\\j\\neq k}}^n\\frac{x - x_j}{x_k - x_j}.\n\\] By construction, they have the property that \\[\n\\ell_k(x_i) = \\begin{cases}\n1 & \\textrm{if $i=k$},\\\\\n0 & \\textrm{otherwise}.\n\\end{cases}\n\\] From this, it follows that the interpolating polynomial may be written as above.\n\n\n\n\n\n\nBy the Existence/Uniqueness Theorem, the Lagrange polynomials are the unique polynomials with this property.\n\n\n\n\nExample 2.2: Compute the quadratic interpolating polynomial to \\(f(x)=\\cos(x)\\) with nodes \\(\\{-\\tfrac\\pi4, 0, \\tfrac\\pi4\\}\\) using Lagrange polynomials.The Lagrange polynomials of degree 2 for these nodes are \\[\n\\begin{aligned}\n\\ell_0(x) &= \\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)} = \\frac{x(x-\\tfrac\\pi4)}{\\tfrac\\pi4\\cdot\\tfrac\\pi2},\\\\\n\\ell_1(x) &= \\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)} = \\frac{(x+\\tfrac\\pi4)(x-\\tfrac\\pi4)}{-\\tfrac\\pi4\\cdot\\tfrac\\pi4},\\\\\n\\ell_2(x) &= \\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}\\\\\n&= \\frac{x(x+\\tfrac\\pi4)}{\\tfrac\\pi2\\cdot\\tfrac\\pi4}.\n\\end{aligned}\n\\] So the interpolating polynomial is \\[\n\\begin{aligned}\np_2(x) &= f(x_0)\\ell_0(x) + f(x_1)\\ell_1(x) + f(x_2)\\ell_2(x)\\\\\n&= \\tfrac{1}{\\sqrt{2}}\\tfrac8{\\pi^2}x(x-\\tfrac\\pi4) - \\tfrac{16}{\\pi^2}(x+\\tfrac\\pi4)(x-\\tfrac\\pi4) +\\tfrac{1}{\\sqrt{2}}\\tfrac8{\\pi^2}x(x+\\tfrac\\pi4) = \\tfrac{16}{\\pi^2}\\big(\\tfrac{1}{\\sqrt{2}} - 1\\big)x^2 + 1.\n\\end{aligned}\n\\] The Lagrange polynomials and the resulting interpolant are shown below:\n \n\n\n\n\n\n\n\n\nLagrange polynomials were actually discovered by Edward Waring in 1776 and rediscovered by Euler in 1783, before they were published by Lagrange himself in 1795; a classic example of Stigler’s law of eponymy!\n\n\n\nThe Lagrange form of the interpolating polynomial is easy to write down, but expensive to evaluate since all of the \\(\\ell_k\\) must be computed. Moreover, changing any of the nodes means that the \\(\\ell_k\\) must all be recomputed from scratch, and similarly for adding a new node (moving to higher degree).\n\n\n2.1.5 Newton/Divided-Difference Polynomials\nIt would be easy to increase the degree of \\(p_n\\) if \\[\np_{n+1}(x) = p_{n}(x) + g_{n+1}(x), \\quad \\textrm{where $g_{n+1}\\in{\\cal P}_{n+1}$}.\n\\] From the interpolation conditions, we know that \\[\ng_{n+1}(x_i) = p_{n+1}(x_i) - p_{n}(x_i) = f(x_i)-f(x_i)=0 \\quad \\textrm{for $i=0,\\ldots,n$},\n\\] so \\[\ng_{n+1}(x) = a_{n+1}(x-x_0)\\cdots(x-x_{n}).\n\\] The coefficient \\(a_{n+1}\\) is determined by the remaining interpolation condition at \\(x_{n+1}\\), so \\[\np_n(x_{n+1}) + g_{n+1}(x_{n+1}) = f(x_{n+1}) \\quad \\implies \\quad a_{n+1} = \\frac{f(x_{n+1}) - p_{n}(x_{n+1})}{(x_{n+1}-x_0)\\cdots(x_{n+1}-x_{n})}.\n\\]\nThe polynomial \\((x-x_0)(x-x_1)\\cdots(x-x_{n})\\) is called a Newton polynomial. These form a new basis \\[\nn_0(x)=1, \\qquad n_k(x) = \\prod_{j=0}^{k-1}(x-x_j) \\quad \\textrm{for $k&gt;0$}.\n\\]\nThe Newton form of the interpolating polynomial is then \\[\np_n(x) = \\sum_{k=0}^n a_kn_k(x), \\qquad a_0 = f(x_0),\\qquad a_k = \\frac{f(x_k) - p_{k-1}(x_k)}{(x_k-x_0)\\cdots(x_k-x_{k-1})} \\textrm{ for $k&gt;0$}.\n\\] Notice that \\(a_k\\) depends only on \\(x_0,\\ldots x_k\\), so we can construct first \\(a_0\\), then \\(a_1\\), etc.\nIt turns out that the \\(a_k\\) are easy to compute, but it will take a little work to derive the method. We define the divided difference \\(f[x_0,x_1,\\ldots,x_k]\\) to be the coefficient of \\(x^k\\) in the polynomial interpolating \\(f\\) at nodes \\(x_0,\\ldots,x_k\\). It follows that \\[\nf[x_0,x_1,\\ldots,x_k] = a_k,\n\\] where \\(a_k\\) is the coefficient in the Newton form above.\n\nExample 2.3: Compute the Newton interpolating polynomial at two nodes.\\[\n\\begin{aligned}\nf[x_0] &= a_0 = f(x_0),\\\\\nf[x_0,x_1] &= a_1 = \\frac{f(x_1) - p_0(x_1)}{x_1 - x_0} = \\frac{f(x_1)-a_0}{x_1 - x_0} = \\frac{f[x_1]-f[x_0]}{x_1 - x_0}.\n\\end{aligned}\n\\] So the first-order divided difference \\(f[x_0,x_1]\\) is obtained from the zeroth-order differences \\(f[x_0]\\), \\(f[x_1]\\) by subtracting and dividing, hence the name “divided difference”.\n\n\n\nExample 2.4: Compute the Newton interpolating polynomial at three nodes.Continuing from the previous example, we find \\[\n\\begin{aligned}\nf[x_0,x_1,x_2] &= a_2 = \\frac{f(x_2) - p_1(x_2)}{(x_2 - x_0)(x_2-x_1)} = \\frac{f(x_2) - a_0 - a_1(x_2-x_0)}{(x_2 - x_0)(x_2-x_1)}\\\\\n& = \\ldots = \\frac{1}{x_2-x_0}\\left(\\frac{f[x_2]-f[x_1]}{x_2-x_1} - \\frac{f[x_1]-f[x_0]}{x_1-x_0}\\right)\\\\\n&= \\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0}.\n\\end{aligned}\n\\] So again, we subtract and divide.\n\n\nIn general, we have the following.\n\nTheorem 2.5For \\(k&gt;0\\), the divided differences satisfy \\[\nf[x_i,x_{i+1},\\ldots,x_{i+k}] = \\frac{f[x_{i+1},\\ldots,x_{i+k}] - f[x_i,\\ldots,x_{i+k-1}]}{x_{i+k} - x_i}.\n\\]\n\n\nProof:\nWithout loss of generality, we relabel the nodes so that \\(i=0\\). So we want to prove that \\[\nf[x_0,x_1,\\ldots,x_{k}] = \\frac{f[x_1,\\ldots,x_{k}] - f[x_0,\\ldots,x_{k-1}]}{x_{k} - x_0}.\n\\] The trick is to write the interpolant with nodes \\(x_0, \\ldots, x_k\\) in the form \\[\np_k(x) = \\frac{(x_k-x)q_{k-1}(x) + (x-x_0)\\tilde{q}_{k-1}(x)}{x_k-x_0},\n\\] where \\(q_{k-1}\\in{\\cal P}_{k-1}\\) interpolates \\(f\\) at the subset of nodes \\(x_0, x_1, \\ldots, x_{k-1}\\) and \\(\\tilde{q}_{k-1}\\in{\\cal P}_{k-1}\\) interpolates \\(f\\) at the subset \\(x_1, x_2,\\ldots,x_k\\). If this holds, then matching the coefficient of \\(x^k\\) on each side will give the divided difference formula, since, e.g., the leading coefficient of \\(q_{k-1}\\) is \\(f[x_0,\\ldots,x_{k-1}]\\). To see that \\(p_k\\) may really be written this way, note that \\[\n\\begin{aligned}\np_k(x_0) &= q_{k-1}(x_0) = f(x_0),\\\\\np_k(x_k) &= \\tilde{q}_{k-1}(x_k) = f(x_k),\\\\\np_k(x_i) &= \\frac{(x_k-x_i)q_{k-1}(x_i) + (x_i-x_0)\\tilde{q}_{k-1}(x_i)}{x_k - x_0} = f(x_i) \\quad \\textrm{for $i=1,\\ldots,k-1$}.\n\\end{aligned}\n\\] Since \\(p_k\\) agrees with \\(f\\) at the \\(k+1\\) nodes, it is the unique interpolant in \\({\\cal P}_k\\).\nTheorem above gives us our convenient method, which is to construct a divided-difference table.\n\nExample 2.5: Construct the Newton polynomial at the nodes \\(\\{-1,0,1,2\\}\\) and with corresponding function values \\(\\{5,1,1,11\\}\\)We construct a divided-difference table as follows. \\[\n\\begin{matrix}\n&x_0=-1 \\quad &f[x_0]=5 & & &\\\\\n& & &f[x_0,x_1]=-4 & &\\\\\n&x_1=0 \\quad &f[x_1]=1 & &f[x_0,x_1,x_2]=2 &\\\\\n& & &f[x_1,x_2]=0 & &f[x_0,x_1,x_2,x_3]=1\\\\\n&x_2=1 \\quad &f[x_2]=1 & &f[x_1,x_2,x_3]=5 &\\\\\n& & &f[x_2,x_3]=10 & &\\\\\n&x_3=2 \\quad &f[x_3]=11 & & &\\\\\n\\end{matrix}\n\\] The coefficients of the \\(p_3\\) lie at the top of each column, so \\[\n\\begin{aligned}\np_3(x) &= f[x_0] + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1)\\\\\n& + f[x_0,x_1,x_2,x_3](x-x_0)(x-x_1)(x-x_2)\\\\\n&=5 - 4(x+1) + 2x(x+1) + x(x+1)(x-1).\n\\end{aligned}\n\\] Now suppose we add the extra nodes \\(\\{-2,3\\}\\) with data \\(\\{5,35\\}\\). All we need to do to compute \\(p_5\\) is add two rows to the bottom of the table — there is no need to recalculate the rest. This gives \\[\n\\begin{matrix}\n&-1  &5 & & & & &\\\\\n& & &-4 & & & &\\\\\n&0  &1 & &2 & & &\\\\\n& & &0 & &1 & &\\\\\n&1  &1 & &5 & &\\textcolor{red}{-\\tfrac1{12}} &\\\\\n& & &10 & &\\textcolor{red}{\\tfrac{13}{12}} & &\\textcolor{red}{0}\\\\\n&2 &11 & &\\textcolor{red}{\\tfrac{17}{6}} & &\\textcolor{red}{-\\tfrac1{12}}\\\\\n& & &\\textcolor{red}{\\tfrac{3}{2}} & &\\textcolor{red}{\\tfrac{5}{6}} & &\\\\\n&\\textcolor{red}{-2} &\\textcolor{red}{5} & &\\textcolor{red}{\\tfrac92} & &&\\\\\n& & &\\textcolor{red}{6} & & & &\\\\\n&\\textcolor{red}{3}  &\\textcolor{red}{35} & & & & &\\\\\n\\end{matrix}\n\\] The new interpolating polynomial is \\[\np_5(x) = p_3(x) - \\tfrac{1}{12}x(x+1)(x-1)(x-2).\n\\]\n\n\n\n\n\n\n\n\nNotice that the \\(x^5\\) coefficient vanishes for these particular data, meaning that they are consistent with \\(f\\in{\\cal P}_4\\).\n\n\n\n\n\n\n\n\n\nNote that the value of \\(f[x_0,x_1,\\ldots,x_k]\\) is independent of the order of the nodes in the table. This follows from the uniqueness of \\(p_k\\).\n\n\n\nDivided differences are actually approximations for derivatives of \\(f\\). In the limit that the nodes all coincide, the Newton form of \\(p_n(x)\\) becomes the Taylor polynomial.\n\n\n2.1.6 Interpolation Error\nThe goal here is to estimate the error \\(|f(x)-p_n(x)|\\) when we approximate a function \\(f\\) by a polynomial interpolant \\(p_n\\). Clearly this will depend on \\(x\\).\n\nExample 2.6: Quadratic interpolant for \\(f(x)=\\cos(x)\\) with \\(\\{-\\tfrac\\pi4,0,\\tfrac\\pi4\\}\\).From Section 2.1.4, we have \\(p_2(x) = \\tfrac{16}{\\pi^2}\\left(\\tfrac{1}{\\sqrt{2}}-1\\right)x^2 + 1\\), so the error is \\[\n|f(x) - p_2(x)| = \\left|\\cos(x) -  \\tfrac{16}{\\pi^2}\\left(\\tfrac{1}{\\sqrt{2}}-1\\right)x^2 - 1\\right|.\n\\] This is shown below:\n\n\n\n\n\nClearly the error vanishes at the nodes themselves, but note that it generally does better near the middle of the set of nodes — this is quite typical behaviour.\n\n\nWe can adapt the proof of Taylor’s theorem to get a quantitative error estimate.\n\nTheorem 2.6: Cauchy’s Interpolation Error TheoremLet \\(p_n\\in{\\cal P}_n\\) be the unique polynomial interpolating \\(f(x)\\) at the \\(n+1\\) distinct nodes \\(x_0, x_1, \\ldots, x_n \\in [a,b]\\), and let \\(f\\) be continuous on \\([a,b]\\) with \\(n+1\\) continuous derivatives on \\((a,b)\\). Then for each \\(x\\in[a,b]\\) there exists \\(\\xi\\in(a,b)\\) such that \\[\nf(x) - p_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)(x-x_1)\\cdots(x-x_n).\n\\]\n\n\nThis looks similar to the error formula for Taylor polynomials (see Taylor’s Theorem). But now the error vanishes at multiple nodes rather than just at \\(x_0\\).\nFrom the formula, you can see that the error will be larger for a more “wiggly” function, where the derivative \\(f^{(n+1)}\\) is larger. It might also appear that the error will go down as the number of nodes \\(n\\) increases; we will see in Section 2.1.7 that this is not always true.\n\n\n\n\n\n\nAs in Taylor’s theorem, note the appearance of an undetermined point \\(\\xi\\). This will prevent us knowing the error exactly, but we can make an estimate as before.\n\n\n\n\nExample 2.7: Quadratic interpolant for \\(f(x)=\\cos(x)\\) with \\(\\{-\\tfrac\\pi4,0,\\tfrac\\pi4\\}\\).For \\(n=2\\), Cauchy’s Interpolation Error Theorem says that \\[\nf(x) - p_2(x) = \\frac{f^{(3)}(\\xi)}{6}x(x+\\tfrac\\pi4)(x-\\tfrac\\pi4) = \\tfrac16\\sin(\\xi)x(x+\\tfrac\\pi4)(x-\\tfrac\\pi4),\n\\] for some \\(\\xi\\in[-\\tfrac\\pi4,\\tfrac\\pi4]\\).\nFor an upper bound on the error at a particular \\(x\\), we can just use \\(|\\sin(\\xi)|\\leq 1\\) and plug in \\(x\\).\nTo bound the maximum error within the interval \\([-1,1]\\), let us maximise the polynomial \\(w(x)=x(x+\\tfrac\\pi4)(x-\\tfrac\\pi4)\\). We have \\(w'(x) = 3x^2 - \\tfrac{\\pi^2}{16}\\) so turning points are at \\(x=\\pm\\tfrac\\pi{4\\sqrt{3}}\\). We have \\[\nw(-\\tfrac\\pi{4\\sqrt{3}}) = 0.186\\ldots, \\quad\nw(\\tfrac\\pi{4\\sqrt{3}}) = -0.186\\ldots, \\quad\nw(-1) = -0.383\\ldots, \\quad w(1) = 0.383\\ldots.\n\\]\nSo our error estimate for \\(x\\in[-1,1]\\) is \\[\n|f(x) - p_2(x)| \\leq \\tfrac16(0.383) = 0.0638\\ldots\n\\]\nFrom the plot earlier, we see that this bound is satisfied (as it has to be), although not tight.\n\n\n\n\n2.1.7 Node Placement: Chebyshev nodes\nYou might expect polynomial interpolation to converge as \\(n\\to\\infty\\). Surprisingly, this is not the case if you take equally-spaced nodes \\(x_i\\). This was shown by Runge in a famous 1901 paper.\n\nExample 2.8: The Runge function \\(f(x) = 1/(1 + 25x^2)\\) on \\([-1,1]\\).Here are illustrations of \\(p_n\\) for increasing \\(n\\):\n   \nNotice that \\(p_n\\) is converging to \\(f\\) in the middle, but diverging more and more near the ends, even within the interval \\([x_0,x_n]\\). This is called the Runge phenomenon.\n\n\n\n\n\n\n\n\nA full mathematical explanation for this divergence usually uses complex analysis — see Chapter 13 of Approximation Theory and Approximation Practice by L.N. Trefethen (SIAM, 2013). For a more elementary proof, see this StackExchange post.\n\n\n\nThe problem is (largely) coming from the interpolating polynomial \\[\nw(x) = \\prod_{i=0}^n(x-x_i).\n\\] We can avoid the Runge phenomenon by choosing different nodes \\(x_i\\) that are not uniformly spaced.\nSince the problems are occurring near the ends of the interval, it would be logical to put more nodes there. A good choice is given by taking equally-spaced points on the unit circle \\(|z|=1\\), and projecting to the real line:\n\n\n\n\n\nThe points around the circle are \\[\n\\phi_j = \\frac{(2j+1)\\pi}{2(n+1)}, \\quad j=0,\\ldots,n,\n\\] so the corresponding Chebyshev nodes are \\[\nx_j = \\cos\\left[\\frac{(2j + 1)\\pi}{2(n+1)}\\right], \\quad j=0,\\ldots,n.\n\\]\n\nExample 2.9: The Runge function \\(f(x) = 1/(1+25x^2)\\) on \\([-1,1]\\) using the Chebyshev nodes.For \\(n=3\\), the nodes are \\(x_0=\\cos(\\tfrac\\pi8)\\), \\(x_1=\\cos(\\tfrac{3\\pi}{8})\\), \\(x_2=\\cos(\\tfrac{5\\pi}{8})\\), \\(x_3=\\cos(\\tfrac{7\\pi}{8})\\).\nBelow we illustrate the resulting interpolant for \\(n=15\\):\n\n\n\n\n\nCompare this to the example with equally spaced nodes.\n\n\nIn fact, the Chebyshev nodes are, in one sense, an optimal choice. To see this, we first note that they are zeroes of a particular polynomial.\n\nThe Chebyshev points \\(x_j=\\cos\\left[\\frac{(2j+1)\\pi}{2(n+1)}\\right]\\) for \\(j=0,\\ldots,n\\) are zeroes of the Chebyshev polynomial \\[\nT_{n+1}(t) := \\cos\\big[(n+1)\\arccos(t)\\big]\n\\]\n\n\n\n\n\n\n\nThe Chebyshev polynomials are denoted \\(T_n\\) rather than \\(C_n\\) because the name is transliterated from Russian as “Tchebychef” in French, for example.\n\n\n\nIn choosing the Chebyshev nodes, we are choosing the error polynomial \\(w(x):=\\prod_{i=0}^n(x-x_i)\\) to be \\(T_{n+1}(x)/2^n\\). (This normalisation makes the leading coefficient 1) This is a good choice because of the following result.\n\nTheorem 2.7: Chebyshev interpolationLet \\(x_0, x_1, \\ldots, x_n \\in [-1,1]\\) be distinct. Then \\(\\max_{[-1,1]}|w(x)|\\) is minimized if \\[\nw(x) = \\frac{1}{2^n}T_{n+1}(x),\n\\] where \\(T_{n+1}(x)\\) is the Chebyshev polynomial \\(T_{n+1}(x) = \\cos\\Big((n+1)\\arccos(x)\\Big)\\).\n\n\nHaving established that the Chebyshev polynomial minimises the maximum error, we can see convergence as \\(n\\to\\infty\\) from the fact that \\[\n|f(x) - p_n(x)| = \\frac{|f^{(n+1)}(\\xi)|}{(n+1)!}|w(x)| = \\frac{|f^{(n+1)}(\\xi)|}{2^n(n+1)!}|T_{n+1}(x)| \\leq \\frac{|f^{(n+1)}(\\xi)|}{2^n(n+1)!}.\n\\]\nIf the function is well-behaved enough that \\(|f^{(n+1)}(x)| &lt; M\\) for some constant whenever \\(x \\in [-1,1]\\), then the error will tend to zero as \\(n \\to \\infty\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#nonlinear-equations",
    "href": "chap-two.html#nonlinear-equations",
    "title": "2  Continuous Functions",
    "section": "2.2 Nonlinear Equations",
    "text": "2.2 Nonlinear Equations\n\n\nHow do we find roots of nonlinear equations?\n\nGiven a general equation \\[\nf(x) = 0,\n\\] there will usually be no explicit formula for the root(s) \\(x_*\\), so we must use an iterative method.\nRootfinding is a delicate business, and it is essential to begin by plotting a graph of \\(f(x)\\), so that you can tell whether the answer you get from your numerical method is correct.\n\nExample 2.10: \\(f(x) = \\frac{1}{x} - a\\), for \\(a &gt; 0\\).\nClearly we know the root is exactly \\(x_* = \\frac{1}{a}\\), but this will serve as a running example to test some of our methods\n\n\n\n2.2.1 Interval Bisection\nIf \\(f\\) is continuous and we can find an interval where it changes sign, then it must have a root in this interval. Formally, this is based on:\n\nTheorem 2.8: Intermediate Value TheoremIf \\(f\\) is continuous on \\([a,b]\\) and \\(c\\) lies between \\(f(a)\\) and \\(f(b)\\), then there is at least one point \\(x\\in[a,b]\\) such that \\(f(x)=c\\).\n\n\nIf \\(f(a)f(b)&lt;0\\), then \\(f\\) changes sign at least once in \\([a,b]\\), so by the Intermediate Value Theorem there must be a point \\(x_*\\in[a,b]\\) where \\(f(x_*)=0\\).\nWe can turn this into the following iterative algorithm:\n\nAlgorithm 2.1: Interval bisection\nLet \\(f\\) be continuous on \\([a_0,b_0]\\), with \\(f(a_0)f(b_0)&lt;0\\).\n\nAt each step, set \\(m_k = (a_k + b_k)/2\\).\nIf \\(f(a_k)f(m_k)\\geq 0\\) then set \\(a_{k+1}=m_k\\), \\(b_{k+1}=b_k\\), otherwise set \\(a_{k+1}=a_k\\), \\(b_{k+1}=m_k\\).\n\n\n\n\nExample 2.11: \\(f(x)=\\tfrac1x - 0.5\\).\n\nTry \\(a_0=1\\), \\(b_0=3\\) so that \\(f(a_0)f(b_0)=0.5(-0.1666) &lt; 0\\).\nNow the midpoint is \\(m_0=(1 + 3)/2 = 2\\), with \\(f(m_0)=0\\).\nWe are lucky and have already stumbled on the root \\(x_*=m_0=2\\)!\nSuppose we had tried \\(a_0=1.5\\), \\(b_0=3\\), so \\(f(a_0)=0.1666\\) and \\(f(b_0)=-0.1666\\), and again \\(f(a_0)f(b_0)&lt;0\\).\nNow \\(m_0 = 2.25\\), \\(f(m_0)=-0.0555\\). We have \\(f(a_0)f(m_0) &lt; 0\\), so we set \\(a_1 = a_0=1.5\\) and \\(b_1 = m_0=2.25\\). The root must lie in \\([1.5,2.25]\\).\nNow \\(m_1 = 1.875\\), \\(f(m_1)=0.0333\\), and \\(f(a_1)f(m_1)&gt;0\\), so we take \\(a_2 = m_1=1.875\\), \\(b_2 = b_1=2.25\\). The root must lie in \\([1.875,2.25]\\).\nWe can continue this algorithm, halving the length of the interval each time.\n\n\n\nSince the interval halves in size at each iteration, and always contains a root, we are guaranteed to converge to a root provided that \\(f\\) is continuous. Stopping at step \\(k\\), we get the minimum possible error by choosing \\(m_k\\) as our approximation.\n\nExample 2.12: Same example with initial interval \\([-0.5,0.5]\\).\nIn this case \\(f(a_0)f(b_0)&lt;0\\), but there is no root in the interval.\n\n\nThe rate of convergence is steady, so we can pre-determine how many iterations will be needed to converge to a given accuracy. After \\(k\\) iterations, the interval has length \\[\n|b_k - a_k| = \\frac{|b_0 - a_0|}{2^k},\n\\] so the error in the mid-point satisfies \\[\n|m_k - x_*| \\leq \\frac{|b_0 - a_0|}{2^{k+1}}.\n\\] In order for \\(|m_k - x_*| \\leq \\delta\\), we need \\(n\\) iterations, where \\[\n\\frac{|b_0 - a_0|}{2^{n+1}} \\leq \\delta \\quad \\implies \\log|b_0-a_0| - (n+1)\\log(2) \\leq \\log(\\delta) \\quad \\implies n \\geq \\frac{\\log|b_0-a_0| - \\log(\\delta)}{\\log(2)} - 1.\n\\]\n\nExample 2.13: Previous example continuedWith \\(a_0=1.5\\), \\(b_0=3\\), as in the above example, then for \\(\\delta = \\epsilon_{\\rm M}=1.1\\times 10^{-16}\\) we would need \\[\nn \\geq \\frac{\\log(1.5) - \\log(1.1\\times 10^{-16})}{\\log(2)}-1 \\quad \\implies n \\geq 53 \\textrm{ iterations}.\n\\]\n\n\n\n\n\n\n\n\nThis convergence is pretty slow, but the method has the advantage of being very robust (i.e., use it if all else fails…). It has the more serious disadvantage of only working in one dimension.\n\n\n\n\n\n2.2.2 Fixed point iteration\nThis is a very common type of rootfinding method. The idea is to transform \\(f(x)=0\\) into the form \\(g(x)=x\\), so that a root \\(x_*\\) of \\(f\\) is a fixed point of \\(g\\), meaning \\(g(x_*)=x_*\\). To find \\(x_*\\), we start from some initial guess \\(x_0\\) and iterate \\[\nx_{k+1} = g(x_k)\n\\] until \\(|x_{k+1}-x_k|\\) is sufficiently small. For a given equation \\(f(x)=0\\), there are many ways to transform it into the form \\(x=g(x)\\). Only some will result in a convergent iteration.\n\nExample 2.14: \\(f(x)=x^2-2x-3\\).Note that the roots are \\(-1\\) and \\(3\\). Consider some different rearrangements, with \\(x_0=0\\).\n\n\\(g(x) = \\sqrt{2x+3}\\), gives \\(x_k\\to 3\\) [to machine accuracy after 33 iterations].\n\\(g(x) = 3/(x-2)\\), gives \\(x_k\\to -1\\) [to machine accuracy after 33 iterations].\n\\(g(x) = (x^2 - 3)/2\\), gives \\(x_k\\to -1\\) [but very slowly!].\n\\(g(x) = x^2 - x - 3\\), gives \\(x_k\\to\\infty\\).\n\\(g(x) = (x^2+3)/(2x-2)\\), gives \\(x_k\\to -1\\) [to machine accuracy after 5 iterations].\n\nIf instead we take \\(x_0=42\\), then (1) and (2) still converge to the same roots, (3) now diverges, (4) still diverges, and (5) now converges to the other root \\(x_k\\to 3\\).\n\n\nIn this section, we will consider which iterations will converge, before addressing the rate of convergence in Section 2.2.3.\nOne way to ensure that the iteration will work is to find a contraction mapping \\(g\\), which is a map \\(L\\to L\\) (for some closed interval \\(L\\)) satisfying \\[\n|g(x)-g(y)| \\leq \\lambda|x-y|\n\\] for some \\(\\lambda &lt; 1\\) and for all \\(x\\), \\(y \\in L\\). The sketch below shows the idea:\n\n\n\n\n\n\nTheorem 2.9: Contraction Mapping TheoremIf \\(g\\) is a contraction mapping on \\(L=[a,b]\\), then 1. There exists a unique fixed point \\(x_*\\in L\\) with \\(g(x_*)=x_*\\). 2. For any \\(x_0\\in L\\), the iteration \\(x_{k+1}=g(x_k)\\) will converge to \\(x_*\\) as \\(k\\to\\infty\\).\n\n\nProof:\nTo prove existence, consider \\(h(x)=g(x)-x\\). Since \\(g:L\\to L\\) we have \\(h(a)=g(a)-a\\geq 0\\) and \\(h(b)=g(b)-b\\leq 0\\). Moreover, it follows from the contraction property above that \\(g\\) is continuous (think of “\\(\\epsilon\\delta\\)”), therefore so is \\(h\\). So the Intermediate Value Theorem guarantees the existence of at least one point \\(x_*\\in L\\) such that \\(h(x_*)=0\\), i.e. \\(g(x_*)=x_*\\).\nFor uniqueness, suppose \\(x_*\\) and \\(y_*\\) are both fixed points of \\(g\\) in \\(L\\). Then \\[\n|x_*-y_*| = |g(x_*)-g(y_*)| \\leq \\lambda |x_*-y_*| &lt; |x_*-y_*|,\n\\] which is a contradiction.\nFinally, to show convergence, consider \\[\n|x_*- x_{k+1} | = |g(x_*) - g(x_k)| \\leq \\lambda |x_* - x_k| \\leq \\ldots \\leq \\lambda^{k+1}|x_*-x_0|.\n\\] Since \\(\\lambda&lt;1\\), we see that \\(x_k\\to x_*\\) as \\(k\\to\\infty\\).\n\n\n\n\n\n\nThe Contraction Mapping Theorem is also known as the Banach fixed point theorem, and was proved by Stefan Banach in his 1920 PhD thesis.\n\n\n\nTo apply this result in practice, we need to know whether a given function \\(g\\) is a contraction mapping on some interval.\nIf \\(g\\) is differentiable, then Taylor’s theorem says that there exists \\(\\xi\\in(x,y)\\) with \\[\ng(x) = g(y) + g'(\\xi)(x-y) \\,\\, \\implies \\,\\, |g(x)-g(y)| \\leq \\Big(\\max_{\\xi\\in L}|g'(\\xi)|\\Big)\\,|x-y|.\n\\] So if (a) \\(g:L\\to L\\) and (b) \\(|g'(x)|\\leq M\\) for all \\(x\\in L\\) with \\(M&lt;1\\), then \\(g\\) is a contraction mapping on \\(L\\).\n\nExample 2.15: Iteration (a) from previous example, \\(g(x) = \\sqrt{2x + 3}\\).Here \\(g'=(2x + 3)^{-1/2}\\), so we see that \\(|g'(x)|&lt;1\\) for all \\(x&gt;-1\\).\nFor \\(g\\) to be a contraction mapping on an interval \\(L\\), we also need that \\(g\\) maps \\(L\\) into itself. Since our particular \\(g\\) is continuous and monotonic increasing (for \\(x&gt;-\\tfrac32\\)), it will map an interval \\([a,b]\\) to another interval whose end-points are \\(g(a)\\) and \\(g(b)\\).\nFor example, \\(g(-\\tfrac12)=\\sqrt{2}\\) and \\(g(4)=\\sqrt{11}\\), so the interval \\(L=[-\\tfrac12,4]\\) is mapped into itself. It follows by the Contraction Mapping Theorem that (1) there is a unique fixed point \\(x_*\\in[-\\tfrac12,4]\\) (which we know is \\(x_*=3\\)), and (2) the iteration will converge to \\(x_*\\) for any \\(x_0\\) in this interval (as we saw for \\(x_0=0\\)).\n\n\nIn practice, it is not always easy to find a suitable interval \\(L\\). But knowing that \\(|g'(x_*)|&lt;1\\) is enough to guarantee that the iteration will converge if \\(x_0\\) is close enough to \\(x_*\\).\n\nTheorem 2.10: Local Convergence TheoremLet \\(g\\) and \\(g'\\) be continuous in the neighbourhood of an isolated fixed point \\(x_*=g(x_*)\\). If \\(|g'(x_*)|&lt;1\\) then there is an interval \\(L=[x_*-\\delta,x_*+\\delta]\\) such that \\(x_{k+1}=g(x_k)\\) converges to \\(x_*\\) whenever \\(x_0\\in L\\).\n\n\nProof:\nBy continuity of \\(g'\\), there exists some interval \\(L=[x_*-\\delta,x_*+\\delta]\\) with \\(\\delta&gt;0\\) such that \\(|g'(x)|\\leq\nM\\) for some \\(M&lt;1\\), for all \\(x\\in L\\). Now let \\(x\\in L\\). It follows that \\[\n|x_* - g(x)| = |g(x_*)-g(x)| \\leq M|x_*-x| &lt; |x_*-x| \\leq \\delta,\n\\] so \\(g(x)\\in L\\). Hence \\(g\\) is a contraction mapping on \\(L\\) and the Contraction Mapping Theorem shows that \\(x_k\\to x_*\\).\n\nExample 2.16: Iteration (a) again, \\(g(x) = \\sqrt{2x + 3}\\).Here we know that \\(x_*=3\\), and \\(|g'(3)|=\\tfrac13 &lt; 1\\), so the Local Convergence Theorem tells us that the iteration will converge to \\(3\\) if \\(x_0\\) is close enough to \\(3\\).\n\n\n\nExample 2.17: Iteration (e) again, \\(g(x) = (x^2+3)/(2x-2)\\).Here we have \\[\ng'(x) = \\frac{x^2-2x-3}{2(x-1)^2},\n\\] so we see that \\(g'(-1)=g'(3)=0 &lt; 1\\). So the Local Convergence Theorem tells us that the iteration will converge to either root if we start close enough.\n\n\n\n\n\n\n\n\nAs we will see, the fact that \\(g'(x_*)=0\\) is related to the fast convergence of iteration (e).\n\n\n\n\n\n2.2.3 Orders of convergence\nTo measure the speed of convergence, we compare the error \\(|x_*-x_{k+1}|\\) to the error at the previous step, \\(|x_*-x_k|\\).\n\nExample 2.18: Interval bisection.Here we had \\(|x_*-m_{k+1}| \\leq \\tfrac12|x_*-m_k|\\). This is called linear convergence, meaning that we have \\(|x_*-x_{k+1}|\\leq \\lambda |x_* - x_{k}|\\) for some constant \\(\\lambda &lt; 1\\).\n\n\nWe can compare a few different iteration schemes that should converge to the same answer to get a sense for how our choice of scheme can impact the convergence order.\n\nExample 2.19: Iteration (a) again, \\(g(x)=\\sqrt{2x+3}\\).Look at the sequence of errors in this case:\n\n\n\n\\(x_k\\)\n\\(|3-x_k|\\)\n\\(|3-x_k|/|3-x_{k-1}|\\)\n\n\n\n\n0.0000000000\n3.0000000000\n-\n\n\n1.7320508076\n1.2679491924\n0.4226497308\n\n\n2.5424597568\n0.4575402432\n0.3608506129\n\n\n2.8433992885\n0.1566007115\n0.3422665304\n\n\n2.9473375404\n0.0526624596\n0.3362849319\n\n\n2.9823941860\n0.0176058140\n0.3343143126\n\n\n2.9941256440\n0.0058743560\n0.3336600063\n\n\n\nWe see that the ratio \\(|x_*-x_k|/|x_*-x_{k-1}|\\) is indeed less than \\(1\\), and seems to be converging to \\(\\lambda\\approx\\tfrac13\\). So this is a linearly convergent iteration.\n\n\n\nExample 2.20: Iteration (e) again, \\(g(x) = (x^2+3)/(2x-2)\\).Now the sequence is:\n\n\n\n\\(x_k\\)\n\\(|(-1)-x_k|\\)\n\\(|(-1)-x_k|/|(-1)-x_{k-1}|\\)\n\n\n\n\n0.0000000000\n1.0000000000\n-\n\n\n-1.5000000000\n0.5000000000\n0.5000000000\n\n\n-1.0500000000\n0.0500000000\n0.1000000000\n\n\n-1.0006097561\n0.0006097561\n0.0121951220\n\n\n-1.0000000929\n0.0000000929\n0.0001523926\n\n\n\nAgain the ratio \\(|x_*-x_{k}|/|x_*-x_{k-1}|\\) is certainly less than \\(1\\), but this time we seem to have \\(\\lambda\\to 0\\) as \\(k\\to\\infty\\). This is called superlinear convergence, meaning that the convergence is in some sense “accelerating”.\n\n\nIn general, if \\(x_k\\to x_*\\) then we say that the sequence \\(\\{x_k\\}\\) converges linearly if \\[\n\\lim_{k\\to\\infty}\\frac{|x_*-x_{k+1}|}{|x_*-x_k|} = \\lambda \\quad \\textrm{with} \\quad 0&lt;\\lambda &lt;1.\n\\] If \\(\\lambda=0\\) then the convergence is superlinear.\n\n\n\n\n\n\nThe constant \\(\\lambda\\) is called the rate or ratio.\n\n\n\nThe following result establishes conditions for linear and superlinear convergence.\n\nTheorem 2.11\nLet \\(g'\\) be continuous in the neighbourhood of a fixed point \\(x_*=g(x_*)\\), and suppose that \\(x_{k+1}=g(x_k)\\) converges to \\(x_*\\) as \\(k\\to\\infty\\).\n\nIf \\(|g'(x_*)|\\neq 0\\) then the convergence will be linear with rate \\(\\lambda=|g'(x_*)|\\).\nIf \\(|g'(x_*)|=0\\) then the convergence will be superlinear.\n\n\n\nProof:\nBy Taylor’s theorem, note that \\[\nx_* - x_{k+1} = g(x_*) - g(x_k) = g(x_*) - \\Big[g(x_*) + g'(\\xi_k)(x_k-x_*)\\Big] = g'(\\xi_k)(x_* - x_k)\n\\] for some \\(\\xi_k\\) between \\(x_*\\) and \\(x_k\\). Since \\(x_k\\to x_*\\), we have \\(\\xi_k\\to x_*\\) as \\(k\\to\\infty\\), so \\[\n\\lim_{k\\to\\infty}\\frac{|x_*-x_{k+1}|}{|x_*-x_k|} = \\lim_{k\\to\\infty}|g'(\\xi_k)| = |g'(x_*)|.\n\\] This proves the result.\n\nExample 2.21: Iteration (a) again, \\(g(x)=\\sqrt{2x+3}\\).We saw before that \\(g'(3)=\\tfrac13\\), so the theorem above shows that convergence will be linear with \\(\\lambda = |g'(3)| = \\tfrac13\\) as we found numerically.\n\n\n\nExample 2.22: Iteration (e) again, \\(g(x) = (x^2+3)/(2x-2)\\).We saw that \\(g'(-1)=0\\), so the theorem above shows that convergence will be superlinear, again consistent with our numerical findings.\n\n\nWe can further classify superlinear convergence by the order of convergence, defined as \\[\n\\alpha = \\sup\\left\\{\\beta \\, : \\, \\lim_{k\\to\\infty}\\frac{|x_*-x_{k+1}|}{|x_*-x_k|^\\beta} &lt; \\infty \\right\\}.\n\\]\nFor example, \\(\\alpha=2\\) is called quadratic convergence and \\(\\alpha=3\\) is called cubic convergence, although for a general sequence \\(\\alpha\\) need not be an integer (e.g. the secant method below).\n\n\n2.2.4 Newton’s method\nThis is a particular fixed point iteration that is very widely used because (as we will see) it usually converges superlinearly.\n\n\n\n\n\nGraphically, the idea of Newton’s method is simple: given \\(x_k\\), draw the tangent line to \\(f\\) at \\(x=x_k\\), and let \\(x_{k+1}\\) be the \\(x\\)-intercept of this tangent. So \\[\n\\frac{0 - f(x_k)}{x_{k+1} - x_k} = f'(x_k) \\quad \\implies\\quad  x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}.\n\\]\n\n\n\n\n\n\nIn fact, Newton only applied the method to polynomial equations, and without using calculus. The general form using derivatives (“fluxions”) was first published by Thomas Simpson in 1740. [See “Historical Development of the Newton-Raphson Method” by T.J. Ypma, SIAM Review 37, 531 (1995).]\n\n\n\nAnother way to derive this iteration is to approximate \\(f(x)\\) by the linear part of its Taylor series centred at \\(x_k\\): \\[\n0 \\approx f(x_{k+1}) \\approx f(x_k) + f'(x_k)(x_{k+1} - x_{k}).\n\\]\nThe iteration function for Newton’s method is \\[\ng(x) = x - \\frac{f(x)}{f'(x)},\n\\] so using \\(f(x_*)=0\\) we see that \\(g(x_*)=x_*\\). To assess the convergence, note that \\[\ng'(x) = 1 - \\frac{f'(x)f'(x) - f(x)f''(x)}{[f'(x)]^2} = \\frac{f(x)f''(x)}{[f'(x)]^2} \\quad \\implies g'(x_*)=0 \\quad \\textrm{if $f'(x_*)\\neq 0$}.\n\\] So if \\(f'(x_*)\\neq 0\\), the Local Convergence Theorem shows that the iteration will converge for \\(x_0\\) close enough to \\(x_*\\). Moreover, since \\(g'(x_*)=0\\), the order theorem shows that this convergence will be superlinear.\n\nExample 2.23: Calculate \\(a^{-1}\\) using \\(f(x)=\\frac1x - a\\) for \\(a&gt;0\\).Newton’s method gives the iterative formula \\[\nx_{k+1} = x_k - \\frac{\\frac{1}{x_k}-a}{-\\frac{1}{x_k^2}} = 2x_k - a x_k^2.\n\\] From the graph of \\(f\\), it is clear that the iteration will converge for any \\(x_0\\in(0,a^{-1})\\), but will diverge if \\(x_0\\) is too large. With \\(a=0.5\\) and \\(x_0=1\\), Python gives\n\n\n\n\n\n\n\n\n\n\\(x_k\\)\n\\(|2 - x_k|\\)\n\\(|2-x_k|/|2-x_{k-1}|\\)\n\\(|2-x_k|/|2-x_{k-1}|^2\\)\n\n\n\n\n1.0\n1.0\n-\n-\n\n\n1.5\n0.5\n0.5\n0.5\n\n\n1.875\n0.125\n0.25\n0.5\n\n\n1.9921875\n0.0078125\n0.0625\n0.5\n\n\n1.999969482\n\\(3.05\\times 10^{-5}\\)\n0.00390625\n0.5\n\n\n2.0\n\\(4.65\\times 10^{-10}\\)\n\\(1.53\\times 10^{-5}\\)\n0.5\n\n\n2.0\n\\(1.08\\times 10^{-19}\\)\n\\(2.33\\times 10^{-10}\\)\n0.5\n\n\n\nIn 6 steps, the error is below \\(\\epsilon_{\\rm M}\\): pretty rapid convergence! The third column shows that the convergence is superlinear. The fourth column shows that \\(|x_*-x_{k+1}|/|x_*-x_k|^2\\) is constant, indicating that the convergence is quadratic (order \\(\\alpha=2\\)).\n\n\n\n\n\n\n\n\nAlthough the solution \\(\\tfrac1a\\) is known exactly, this method is so efficient that it is sometimes used in computer hardware to do division!\n\n\n\nIn practice, it is not usually possible to determine ahead of time whether a given starting value \\(x_0\\) will converge.\nA robust computer implementation should catch any attempt to take too large a step, and switch to a less sensitive (but slower) algorithm (e.g. bisection).\nHowever, it always makes sense to avoid any points where \\(f'(x)=0\\).\n\nExample 2.24: \\(f(x) = x^3-2x+2\\).Here \\(f'(x) = 3x^2-2\\) so there are turning points at \\(x=\\pm\\sqrt{\\tfrac23}\\) where \\(f'(x)=0\\), as well as a single real root at \\(x_*\\approx -1.769\\). The presence of points where \\(f'(x)=0\\) means that care is needed in choosing a starting value \\(x_0\\).\nIf we take \\(x_0=0\\), then \\(x_1 = 0 - f(0)/f'(0) = 1\\), but then \\(x_2 = 1 - f(1)/f'(1)=0\\), so the iteration gets stuck in an infinite loop:\n\n\n\n\n\nOther starting values, e.g. \\(x_0=-0.5\\) can also be sucked into this infinite loop! The correct answer is obtained for \\(x_0=-1.0\\).\n\n\n\n\n\n\n\n\nThe sensitivity of Newton’s method to the choice of \\(x_0\\) is beautifully illustrated by applying it to a complex function such as \\(f(z) = z^3 - 1\\). The following plot colours points \\(z_0\\) in the complex plane according to which root they converge to (\\(1\\), \\(e^{2\\pi i/3}\\), or \\(e^{-2\\pi i/3}\\)):\n\n\n\n\n\nThe boundaries of these basins of attraction are fractal.\n\n\n\n\n\n2.2.5 Newton’s method for systems\nNewton’s method generalizes to higher-dimensional problems where we want to find \\(\\mathbf{x}\\in\\mathbb{R}^m\\) that satisfies \\(\\mathbf{f}(\\mathbf{x})=0\\) for some function \\(\\mathbf{f}:\\mathbb{R}^m\\to\\mathbb{R}^m\\).\nTo see how it works, take \\(m=2\\) so that \\(\\mathbf{x}=(x_1,x_2)^\\top\\) and \\(\\mathbf{f}=[f_1(\\mathbf{x}),f_2(\\mathbf{x})]^\\top\\). Taking the linear terms in Taylor’s theorem for two variables gives \\[\n\\begin{aligned}\n0 &\\approx f_1(\\mathbf{x}_{k+1}) \\approx f_1(\\mathbf{x}_k) + \\left.\\frac{\\partial f_1}{\\partial x_1}\\right|_{\\mathbf{x}_k}(x_{1,k+1} - x_{1,k}) + \\left.\\frac{\\partial f_1}{\\partial x_2}\\right|_{\\mathbf{x}_k}(x_{2,k+1} - x_{2,k}),\\\\\n0 &\\approx f_2(\\mathbf{x}_{k+1}) \\approx f_2(\\mathbf{x}_k) + \\left.\\frac{\\partial f_2}{\\partial x_1}\\right|_{\\mathbf{x}_k}(x_{1,k+1} - x_{1,k}) + \\left.\\frac{\\partial f_2}{\\partial x_2}\\right|_{\\mathbf{x}_k}(x_{2,k+1} - x_{2,k}).\n\\end{aligned}\n\\]\nIn matrix form, we can write \\[\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf_1(\\mathbf{x}_k)\\\\\nf_2(\\mathbf{x}_k)\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x_1}(\\mathbf{x}_k) & \\frac{\\partial f_1}{\\partial x_2}(\\mathbf{x}_k)\\\\\n\\frac{\\partial f_2}{\\partial x_1}(\\mathbf{x}_k) & \\frac{\\partial f_2}{\\partial x_2}(\\mathbf{x}_k)\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{1,k+1} - x_{1,k}\\\\\nx_{2,k+1} - x_{2,k}\n\\end{pmatrix}.\n\\]\nThe matrix of partial derivatives is called the Jacobian matrix \\(J(\\mathbf{x}_k)\\), so (for any \\(m\\)) we have \\[\n\\mathbf{0} = \\mathbf{f}(\\mathbf{x}_k) + J(\\mathbf{x}_k)(\\mathbf{x}_{k+1} - \\mathbf{x}_{k}).\n\\]\nTo derive Newton’s method, we rearrange this equation for \\(\\mathbf{x}_{k+1}\\), \\[\nJ(\\mathbf{x}_k)(\\mathbf{x}_{k+1}-\\mathbf{x}_k) = -\\mathbf{f}(\\mathbf{x}_k) \\quad \\implies \\quad \\mathbf{x}_{k+1} = \\mathbf{x}_k - J^{-1}(\\mathbf{x}_k)\\mathbf{f}(\\mathbf{x}_k).\n\\] So to apply the method, we need the inverse of \\(J\\).\n\n\n\n\n\n\nIf \\(m=1\\), then \\(J(x_k) = \\frac{\\partial f}{\\partial x}(x_k)\\), and \\(J^{-1}=1/J\\), so this reduces to the scalar Newton’s method.\n\n\n\n\nExample 2.25: Apply Newton’s method to the simultaneous equations \\(xy - y^3 - 1 = 0\\) and \\(x^2y + y -5=0\\), with starting values \\(x_0=2\\), \\(y_0=3\\).The Jacobian matrix is \\[\nJ(x,y) = \\begin{pmatrix}\ny & x-3y^2\\\\\n2xy & x^2 + 1\n\\end{pmatrix},\n\\] and hence its inverse is given by \\[\nJ^{-1}(x,y) = \\frac{1}{y(x^2+1)-2xy(x-3y^2)}\\begin{pmatrix}\nx^2 + 1 & 3y^2-x\\\\\n-2xy & y\n\\end{pmatrix}.\n\\]\nThe first iteration of Newton’s method gives \\[\n\\begin{pmatrix}\nx_1\\\\\ny_1\n\\end{pmatrix}\n= \\begin{pmatrix}\n2\\\\\n3\n\\end{pmatrix}\n- \\frac{1}{3(5)-12(2-27)}\n\\begin{pmatrix}\n5 & 25\\\\\n-12 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\n-22\\\\\n10\n\\end{pmatrix}\n= \\begin{pmatrix}\n1.55555556\\\\\n2.06666667\n\\end{pmatrix}.\n\\]\nSubsequent iterations give \\[\n\\begin{pmatrix}\nx_2\\\\\ny_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.54720541\\\\\n1.47779333\n\\end{pmatrix}, \\,\n\\begin{pmatrix}\nx_3\\\\\ny_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.78053503\\\\\n1.15886481\n\\end{pmatrix},\n\\] and \\[\n\\begin{pmatrix}\nx_4\\\\\ny_4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.952843\\\\\n1.02844269\n\\end{pmatrix}, \\\\\n\\begin{pmatrix}\nx_5\\\\\ny_5\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1.99776297\\\\\n1.00124041\n\\end{pmatrix},\n\\]\nso the method is converging accurately to the root \\(x_*=2\\), \\(y_*=1\\), shown in the following plot:\n\n\n\n\n\n\n\nBy generalising the scalar analysis (beyond the scope of this course), it can be shown that the convergence is quadratic for \\(\\mathbf{x}_0\\) sufficiently close to \\(\\mathbf{x}_*\\), provided that \\(J(\\mathbf{x}_*)\\) is non-singular (i.e., \\(\\det[J(\\mathbf{x}_*)]\\neq 0\\)).\n\n\n\n\n\n\nIn general, finding a good starting point in more than one dimension is difficult, particularly because interval bisection is not available. Algorithms that try to mimic bisection in higher dimensions are available, proceeding by a ‘grid search’ approach.\n\n\n\n\n\n2.2.6 Quasi-Newton methods\nA drawback of Newton’s method is that the derivative \\(f'(x_k)\\) must be computed at each iteration. This may be expensive to compute, or may not be available as a formula. For example, the function \\(f\\) might be the right-hand side of some complex partial differential equation, and hence both difficult to differentiate and very high dimensional!\nInstead we can use a quasi-Newton method \\[\nx_{k+1} = x_k - \\frac{f(x_k)}{g_k},\n\\] where \\(g_k\\) is some easily-computed approximation to \\(f'(x_k)\\).\n\nExample 2.26: Steffensen's method\\[\ng_k = \\frac{f\\big(f(x_k) + x_k\\big) - f(x_k)}{f(x_k)}.\n\\] This has the form \\(\\frac{1}{h}\\big(f(x_k+h) - f(x_k)\\big)\\) with \\(h=f(x_k)\\).\n\n\nSteffensen’s method requires two function evaluations per iteration. But once the iteration has started, we already have two nearby points \\(x_{k-1}\\), \\(x_k\\), so we could approximate \\(f'(x_k)\\) by a backward difference \\[\ng_k = \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}} \\quad \\implies \\quad x_{k+1} = x_k - \\frac{f(x_k)(x_k - x_{k-1})}{f(x_k) - f(x_{k-1})}.\n\\] This is called the secant method, and requires only one function evaluation per iteration (once underway). The name comes from its graphical interpretation:\n\n\n\n\n\n\n\n\n\n\n\nThe secant method was introduced by Newton.\n\n\n\n\nExample 2.27: \\(f(x)=\\tfrac1x - 0.5\\).Now we need two starting values, so take \\(x_0=0.25\\), \\(x_1=0.5\\). The secant method gives:\n\n\n\n\\(k\\)\n\\(x_k\\)\n\\(|x_* - x_k|/|x_* - x_{k-1}|\\)\n\n\n\n\n2\n0.6875\n0.75\n\n\n3\n1.01562\n0.75\n\n\n4\n1.354\n0.65625\n\n\n5\n1.68205\n0.492188\n\n\n6\n1.8973\n0.322998\n\n\n7\n1.98367\n0.158976\n\n\n8\n1.99916\n0.0513488\n\n\n\nConvergence to \\(\\epsilon_{\\rm M}\\) is achieved in 12 iterations. Notice that the error ratio is decreasing, so the convergence is superlinear.\n\n\nThe secant method is a two-point method since \\(x_{k+1} = g(x_{k-1},x_k)\\). So theorems about single-point fixed-point iterations do not apply.\nIn general, one can have multipoint methods based on higher-order interpolation.\n\nTheorem 2.12If \\(f'(x_*)\\neq 0\\) then the secant method converges for \\(x_0\\), \\(x_1\\) sufficiently close to \\(x_*\\), and the order of convergence is \\((1+\\sqrt{5})/2 = 1.618\\ldots\\).\n\n\n\n\n\n\n\n\nThis illustrates that orders of convergence need not be integers, and is also an appearance of the golden ratio.\n\n\n\nProof:\nTo simplify the notation, denote the truncation error by \\[\n\\varepsilon_k := x_* - x_k.\n\\] Expanding in Taylor series around \\(x_*\\), and using \\(f(x_*)=0\\), gives \\[\n\\begin{aligned}\nf(x_{k-1}) &= -f'(x_*)\\varepsilon_{k-1} + \\frac{f''(x_*)}{2}\\varepsilon_{k-1}^2 + {\\cal O}(\\varepsilon_{k-1}^3),\\\\\nf(x_{k}) &= -f'(x_*)\\varepsilon_{k} + \\frac{f''(x_*)}{2}\\varepsilon_{k}^2 + {\\cal O}(\\varepsilon_{k}^3).\n\\end{aligned}\n\\] So using the secant formula above we get \\[\n\\begin{aligned}\n\\varepsilon_{k+1} &= \\varepsilon_k - (\\varepsilon_{k}-\\varepsilon_{k-1})\\frac{-f'(x_*)\\varepsilon_{k} + \\frac{f''(x_*)}{2}\\varepsilon_{k}^2 + {\\cal O}(\\varepsilon_{k}^3)}{-f'(x_*)(\\varepsilon_{k}-\\varepsilon_{k-1})+ \\frac{f''(x_*)}{2}(\\varepsilon_{k}^2 - \\varepsilon_{k-1}^2) + {\\cal O}(\\varepsilon_{k-1}^3)}\\\\\n&= \\varepsilon_k - \\frac{ -f'(x_*)\\varepsilon_{k} + \\frac{f''(x_*)}{2}\\varepsilon_{k}^2 + {\\cal O}(\\varepsilon_{k}^3)}{-f'(x_*) + \\frac{f''(x_*)}{2}(\\varepsilon_k + \\varepsilon_{k-1}) + {\\cal O}(\\varepsilon_{k-1}^2)}\\\\\n&= \\varepsilon_k + \\frac{-\\varepsilon_k + \\tfrac12\\varepsilon_k^2f''(x_*)/f'(x_*) + {\\cal O}(\\varepsilon_k^3)}{1 - \\tfrac12(\\varepsilon_k + \\varepsilon_{k-1})f''(x_*)/f'(x_*) + {\\cal O}(\\varepsilon_{k-1}^2)}\\\\\n&= \\varepsilon_k + \\left(-\\varepsilon_k + \\frac{f''(x_*)}{2f'(x_*)}\\varepsilon_k^2 + {\\cal O}(\\varepsilon_k^3) \\right)\\left(1 + (\\varepsilon_k + \\varepsilon_{k-1})\\frac{f''(x_*)}{2f'(x_*)} + {\\cal O}(\\varepsilon_{k-1}^2) \\right)\\\\\n&= \\varepsilon_k - \\varepsilon_k + \\frac{f''(x_*)}{2f'(x_*)}\\varepsilon_k^2 - \\frac{f''(x_*)}{2f'(x_*)}\\varepsilon_k(\\varepsilon_k + \\varepsilon_{k-1}) + {\\cal O}(\\varepsilon_{k-1}^3)\\\\\n&= -\\frac{f''(x_*)}{2f'(x_*)}\\varepsilon_k\\varepsilon_{k-1} + {\\cal O}(\\varepsilon_{k-1}^3).\n\\end{aligned}\n\\] This is similar to the corresponding formula for Newton’s method, where we have \\[\n\\varepsilon_{k+1} = -\\frac{f''(x_*)}{2f'(x_*)}\\varepsilon_k^2 + {\\cal O}(\\varepsilon_k^3).\n\\] The above tells us that the error for the secant method tends to zero faster than linearly, but not quadratically (because \\(\\varepsilon_{k-1} &gt; \\varepsilon_k\\)).\nTo find the order of convergence, note that \\(\\varepsilon_{k+1}\\sim \\varepsilon_k\\varepsilon_{k-1}\\) suggests a power-law relation of the form \\[\n|\\varepsilon_k| = |\\varepsilon_{k-1}|^\\alpha\\left|\\frac{f''(x_*)}{2f'(x_*)}\\right|^\\beta \\quad \\implies \\quad |\\varepsilon_{k-1}| = |\\varepsilon_k|^{1/\\alpha}\\left|\\frac{f''(x_*)}{2f'(x_*)}\\right|^{-\\beta/\\alpha}.\n\\] Putting this in both sides of the previous equation gives \\[\n|\\varepsilon_{k}|^\\alpha\\left|\\frac{f''(x_*)}{2f'(x_*)}\\right|^\\beta = |\\varepsilon_k|^{(1+\\alpha)/\\alpha}\\left|\\frac{f''(x_*)}{2f'(x_*)}\\right|^{(\\alpha-\\beta)/\\alpha}.\n\\] Equating powers gives \\[\n\\alpha = \\frac{1+\\alpha}{\\alpha} \\quad \\implies \\alpha = \\frac{1 + \\sqrt{5}}{2}, \\quad \\beta = \\frac{\\alpha - \\beta}{\\alpha} \\quad \\implies \\beta = \\frac{\\alpha}{\\alpha + 1} = \\frac{1}{\\alpha}.\n\\] It follows that \\[\n\\lim_{k\\to\\infty}\\frac{|x_* - x_{k+1}|}{|x_* - x_k|^\\alpha} = \\lim_{k\\to\\infty}\\frac{|\\varepsilon_{k+1}|}{|\\varepsilon_k|^\\alpha} = \\left|\\frac{f''(x_*)}{2f'(x_*)}\\right|^{1/\\alpha},\n\\] so the secant method has order of convergence \\(\\alpha\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-two.html#knowledge-checklist",
    "href": "chap-two.html#knowledge-checklist",
    "title": "2  Continuous Functions",
    "section": "Knowledge checklist",
    "text": "Knowledge checklist\nKey topics:\n\nPolynomial interpolation, including the Lagrange and Newton divided-difference forms of polynomial interpolation.\nInterpolation error estimates, truncation error, and the significance of node placement (e.g. the Runge phenomenon).\nNumerical rootfinding methods, including interval bisection and fixed point iteration, with discussion of existence, uniqueness, and orders of convergence (linear, superlinear).\n\nKey skills:\n\nConstructing and analyzing polynomial interpolants for a given data set and function, using Taylor, Lagrange, and Newton methods.\nEstimating approximation errors and choosing optimal interpolation nodes to improve numerical stability and convergence.\nImplementing and evaluating iterative algorithms for solving nonlinear equations, including measuring and understanding convergence rates.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Continuous Functions</span>"
    ]
  },
  {
    "objectID": "chap-three.html",
    "href": "chap-three.html",
    "title": "3  Linear Algebra",
    "section": "",
    "text": "3.1 Systems of Linear Equations\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nThe central goal of this chapter is to answer the following seemingly straightforward question:\nLinear systems of the form \\[\n\\begin{aligned}\na_{11}x_1 &+ a_{12}x_2 + \\ldots + a_{1n}x_n = b_1,\\\\\na_{21}x_1 &+ a_{22}x_2 + \\ldots + a_{2n}x_n = b_2,\\\\\n\\vdots    &\\qquad\\vdots\\qquad\\qquad\\vdots\\\\\na_{n1}x_1 &+ a_{n2}x_2 + \\ldots + a_{nn}x_n = b_n\n\\end{aligned}\n\\] occur in many applications (often with very large \\(n\\)). It is convenient to express this in matrix form: \\[\nA\\mathbf{x} = \\mathbf{b},\n\\] where \\(A\\) is an \\(n\\times n\\) square matrix with elements \\(a_{ij}\\), and \\(\\mathbf{x}\\), \\(\\mathbf{b}\\) are \\(n\\times 1\\) vectors.\nWe will need some basic facts from linear algebra:\nIt follows from fact 5 above that \\(A\\mathbf{x} = \\mathbf{b}\\) has a unique solution iff \\(A\\) is non-singular, given by \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\).\nIn this chapter, we will see how to solve \\(A\\mathbf{x} = \\mathbf{b}\\) both efficiently and accurately.\nAlthough this seems like a conceptually easy problem (just use Gaussian elimination!), it is actually a hard one when \\(n\\) gets large. Nowadays, linear systems with \\(n=1\\) million arise routinely in computational problems. And even for small \\(n\\) there are some potential pitfalls, as we will see.\nMany algorithms are based on the idea of rewriting \\(A\\mathbf{x} = \\mathbf{b}\\) in a form where the matrix is easier to invert. Easiest to invert are diagonal matrices, followed by orthogonal matrices (where \\(A^{-1}=A^\\top\\)). However, the most common method for solving \\(A\\mathbf{x} = \\mathbf{b}\\) transforms the system to triangular form.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "chap-three.html#s-lin",
    "href": "chap-three.html#s-lin",
    "title": "3  Linear Algebra",
    "section": "",
    "text": "How do we solve a linear system numerically?\n\n\n\n\n\\(A^\\top\\) is the transpose of \\(A\\), so \\((a^\\top)_{ij} = a_{ji}\\).\n\\(A\\) is symmetric if \\(A=A^\\top\\).\n\\(A\\) is non-singular iff there exists a solution \\(\\mathbf{x}\\in\\mathbb{R}^n\\) for every \\(\\mathbf{b}\\in\\mathbb{R}^n\\).\n\\(A\\) is non-singular iff \\(\\det(A)\\neq 0\\).\n\\(A\\) is non-singular iff there exists a unique inverse \\(A^{-1}\\) such that \\(AA^{-1}=A^{-1}A = I\\).\n\n\n\n\n\n\n\n\n\n\nIf \\(A\\) is instead rectangular (\\(m\\times n\\)), then there are different numbers of equations and unknowns, and we do not expect a unique solution. Nevertheless, we can still look for an approximate solution in this case and there are methods for this problem in the course reading list.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "chap-three.html#triangular-systems",
    "href": "chap-three.html#triangular-systems",
    "title": "3  Linear Algebra",
    "section": "3.2 Triangular systems",
    "text": "3.2 Triangular systems\nIf the matrix \\(A\\) is triangular, then \\(A\\mathbf{x} = \\mathbf{b}\\) is straightforward to solve.\nA matrix \\(L\\) is called lower triangular if all entries above the diagonal are zero: \\[\nL = \\begin{pmatrix}\nl_{11} & 0 & \\cdots & 0\\\\\nl_{21} & l_{22} & \\ddots & \\vdots\\\\\n\\vdots & &\\ddots & 0\\\\\nl_{n1} & \\cdots & \\cdots & l_{nn}\n\\end{pmatrix}.\n\\] The determinant is just \\[\n\\det(L) = l_{11}l_{22}\\cdots l_{nn},\n\\] so the matrix will be non-singular iff all of the diagonal elements are non-zero.\n\nExample 3.1: Solve \\(L\\mathbf{x} = \\mathbf{b}\\) for \\(n=4\\).The system is \\[\n\\begin{pmatrix}\nl_{11} & 0 & 0 & 0\\\\\nl_{21} & l_{22} & 0 & 0\\\\\nl_{31} & l_{32} & l_{33} & 0\\\\\nl_{41} & l_{42} & l_{43} & l_{44}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1\\\\ x_2\\\\ x_3\\\\ x_4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nb_1\\\\ b_2\\\\ b_3\\\\ b_4\n\\end{pmatrix}\n\\] which is equivalent to \\[\n\\begin{aligned}\nl_{11}x_1 &= b_1,\\\\\nl_{21}x_1 + l_{22}x_2 &= b_2,\\\\\nl_{31}x_1 + l_{32}x_2 + l_{33}x_3 &= b_3,\\\\\nl_{41}x_1 + l_{42}x_2 + l_{43}x_3 + l_{44}x_4 &= b_4.\n\\end{aligned}\n\\] We can just solve step-by-step: \\[\nx_1 = \\frac{b_1}{l_{11}}, \\,\\, x_2 = \\frac{b_2 - l_{21}x_1}{l_{22}},\n\\] \\[x_3 = \\frac{b_3 - l_{31}x_1 - l_{32}x_2}{l_{33}}, \\,\\, x_4 = \\frac{b_4 - l_{41}x_1 - l_{42}x_2 - l_{43}x_3}{l_{44}}.\n\\] This is fine since we know that \\(l_{11}\\), \\(l_{22}\\), \\(l_{33}\\), \\(l_{44}\\) are all non-zero when a solution exists.\n\n\nIn general, any lower triangular system \\(L\\mathbf{x}=\\mathbf{b}\\) can be solved by forward substitution \\[\nx_j = \\frac{b_j - \\sum_{k=1}^{j-1}l_{jk}x_k}{l_{jj}}, \\quad j=1,\\ldots,n.\n\\]\nSimilarly, an upper triangular matrix \\(U\\) has the form \\[\nU = \\begin{pmatrix}\nu_{11} & u_{12} & \\cdots & u_{1n}\\\\\n0 & u_{22} & & \\vdots\\\\\n\\vdots & \\ddots & \\ddots & \\vdots\\\\\n0 & \\cdots & 0 & u_{nn}\n\\end{pmatrix},\n\\] and an upper-triangular system \\(U\\mathbf{x} = \\mathbf{b}\\) may be solved by backward substitution \\[\nx_j = \\frac{b_j - \\sum_{k=j+1}^{n}u_{jk}x_k}{u_{jj}}, \\quad j=n,\\ldots,1.\n\\]\nTo estimate the computational cost of forward substitution, we can count the number of floating-point operations (\\(+\\), \\(-\\), \\(\\times\\), \\(\\div\\)).\n\nExample 3.2: Number of operations required for forward substitution.Consider each \\(x_j\\). We have - \\(j=1\\): 1 division - \\(j=2\\): 1 division + [1 subtraction + 1 multiplication] - \\(j=3\\): 1 division + \\(2\\,\\times\\)[1 subtraction + 1 multiplication] - \\(\\vdots\\) - \\(j=n\\): 1 division + \\((n-1)\\,\\times\\)[1 subtraction + 1 multiplication]\nSo the total number of operations required is \\[\n\\sum_{j=1}^n\\Big(1 + 2(j-1)\\Big) = 2\\sum_{j=1}^nj - \\sum_{j=1}^n1 = n(n+1) - n = n^2.\n\\]\n\n\nSo solving a triangular system by forward (or backward) substitution takes \\(n^2\\) operations. We may say that the computational complexity of the algorithm is \\(n^2\\).\n\n\n\n\n\n\nIn practice, this is only a rough estimate of the computational cost, because reading from and writing to the computer’s memory also take time. This can be estimated given a “memory model”, but this depends on the particular computer.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "chap-three.html#gaussian-elimination",
    "href": "chap-three.html#gaussian-elimination",
    "title": "3  Linear Algebra",
    "section": "3.3 Gaussian elimination",
    "text": "3.3 Gaussian elimination\nIf our matrix \\(A\\) is not triangular, we can try to transform it to triangular form. Gaussian elimination uses elementary row operations to transform the system to upper triangular form \\(U\\mathbf{x} = \\mathbf{y}\\).\nElementary row operations include swapping rows and adding multiples of one row to another. They won’t change the solution \\(\\mathbf{x}\\), but will change the matrix \\(A\\) and the right-hand side \\(\\mathbf{b}\\).\n\nExample 3.3: Transform to upper triangular form the system\\[\n\\begin{aligned}\nx_1 + 2x_2 + x_3 &= 0,\\\\\nx_1 - 2x_2 + 2x_3 &= 4,\\\\\n2x_1 + 12x_2 - 2x_3 &= 4.\n\\end{aligned}\n\\] \\[\nA=\\begin{pmatrix}\n1 & 2 & 1\\\\\n1 & -2 & 2\\\\\n2 & 12 & -2\n\\end{pmatrix},\n\\quad\n\\mathbf{b}=\\begin{pmatrix}\n0\\\\ 4\\\\ 4\n\\end{pmatrix}.\n\\]\nStage 1. Subtract \\(1\\) times equation 1 from equation 2, and \\(2\\) times equation 1 from equation 3, so as to eliminate \\(x_1\\) from equations 2 and 3: \\[\n\\begin{aligned}\nx_1 + 2x_2 + x_3 &= 0,\\\\\n-4x_2 + x_3 &= 4,\\\\\n8x_2 - 4x_3 &= 4.\n\\end{aligned}\n\\] \\[\nA^{(2)}=\\begin{pmatrix}\n1 & 2 & 1\\\\\n0 & -4 & 1\\\\\n0 & 8 & -4\n\\end{pmatrix}\n\\quad\n\\mathbf{b}^{(2)}=\\begin{pmatrix}\n0\\\\ 4\\\\ 4\n\\end{pmatrix},\n\\quad m_{21}=1, \\quad m_{31}=2.\n\\]\nStage 2. Subtract \\(-2\\) times equation 2 from equation 3, to eliminate \\(x_2\\) from equation 3: \\[\n\\begin{aligned}\nx_1 + 2x_2 + x_3 &= 0,\\\\\n-4x_2 + x_3 &= 4,\\\\\n-2x_3 = 12.\n\\end{aligned}\n\\] \\[\nA^{(3)}=\\begin{pmatrix}\n1 & 2 & 1\\\\\n0 & -4 & 1\\\\\n0 & 0 & -2\n\\end{pmatrix}\n\\quad\n\\mathbf{b}^{(3)}=\\begin{pmatrix}\n0\\\\ 4\\\\ 12\n\\end{pmatrix},\n\\quad m_{32}=-2.\n\\]\nNow the system is upper triangular, and back substitution gives \\(x_1=11\\), \\(x_2=-\\tfrac{5}{2}\\), \\(x_3=-6\\).\n\n\nWe can write the general algorithm as follows.\n\nAlgorithm 3.1: Gaussian elimination\nLet \\(A^{(1)}=A\\) and \\(\\mathbf{b}^{(1)}=\\mathbf{b}\\). Then for each \\(k\\) from 1 to \\(n-1\\), compute a new matrix \\(A^{(k+1)}\\) and right-hand side \\(\\mathbf{b}^{(k+1)}\\) by the following procedure:\n\nDefine the row multipliers \\[\nm_{ik} = \\frac{a_{ik}^{(k)}}{a_{kk}^{(k)}}, \\quad i=k+1,\\ldots,n.\n\\]\nUse these to remove the unknown \\(x_k\\) from equations \\(k+1\\) to \\(n\\), leaving \\[\na_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik}a_{kj}^{(k)}, \\quad b_i^{(k+1)} = b_i^{(k)} - m_{ik}b_k^{(k)}, \\quad i,j=k+1,\\ldots,n.\n\\] The final matrix \\(A^{(n)}=U\\) will then be upper triangular.\n\n\n\nThis procedure will work providing \\(a_{kk}^{(k)}\\neq 0\\) for every \\(k\\). (We will worry about this later.)\nWhat about the computational cost of Gaussian elimination?\n\nExample 3.4: Number of operations required to find \\(U\\).Computing \\(A^{(k+1)}\\) requires: - \\(n-(k+1)+1 = n-k\\) divisions to compute \\(m_{ik}\\). - \\((n-k)^2\\) subtractions and the same number of multiplications to compute \\(a_{ij}^{(k+1)}\\).\nSo in total \\(A^{(k+1)}\\) requires \\(2(n-k)^2 + n-k\\) operations. Overall, we need to compute \\(A^{(k+1)}\\) for \\(k=1,\\ldots,n-1\\), so the total number of operations is \\[\n\\begin{aligned}\nN &= \\sum_{k=1}^{n-1}\\Big(2n^2 + n - (4n+1)k + 2k^2\\Big) \\\\\n  &= n(2n+1)\\sum_{k=1}^{n-1}1 - (4n+1)\\sum_{k=1}^{n-1}k + 2\\sum_{k=1}^{n-1}k^2.\n\\end{aligned}\n\\] Recalling that \\[\n\\sum_{k=1}^n k = \\tfrac12n(n+1), \\,\\, \\sum_{k=1}^n k^2 = \\tfrac16n(n+1)(2n+1),\n\\] we find \\[\n\\begin{aligned}\nN &= n(2n+1)(n-1) - \\tfrac12(4n+1)(n-1)n + \\tfrac13(n-1)n(2n-1) \\\\\n&= \\tfrac23n^3 - \\tfrac12n^2 - \\tfrac16n.\n\\end{aligned}\n\\] So the number of operations required to find \\(U\\) is \\({\\cal O}(n^3)\\).\n\n\nIt is known that \\({\\cal O}(n^3)\\) is not optimal, and the best theoretical algorithm known for inverting a matrix takes \\({\\cal O}(n^{2.3728639})\\) operations. However, algorithms achieving this bound are highly impractical for most real-world uses due to massive constant factors and implementation overhead. It remains an open conjecture that there exists an \\({\\cal O}(n^{2+\\epsilon})\\) algorithm, for \\(\\epsilon\\) arbitrarily small.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "chap-three.html#lu-decomposition",
    "href": "chap-three.html#lu-decomposition",
    "title": "3  Linear Algebra",
    "section": "3.4 LU decomposition",
    "text": "3.4 LU decomposition\nIn Gaussian elimination, both the final matrix \\(U\\) and the sequence of row operations are determined solely by \\(A\\), and do not depend on \\(\\mathbf{b}\\). We will see that the sequence of row operations that transforms \\(A\\) to \\(U\\) is equivalent to left-multiplying by a matrix \\(F\\), so that \\[\nFA = U, \\qquad U\\mathbf{x} = F\\mathbf{b}.\n\\] To see this, note that step \\(k\\) of Gaussian elimination can be written in the form \\[\nA^{(k+1)} = F^{(k)}A^{(k)}, \\quad \\mathbf{b}^{(k+1)} = F^{(k)}\\mathbf{b}^{(k)},\n\\] where \\[\nF^{(k)} := \\begin{pmatrix}\n1 & 0&\\cdots & \\cdots &\\cdots&0\\\\\n0 & \\ddots & \\ddots&&&\\vdots \\\\\n\\vdots & \\ddots& 1 &\\ddots&&\\vdots \\\\\n\\vdots & & -m_{k+1,k} & \\ddots &\\ddots&\\vdots \\\\\n\\vdots & & \\vdots & \\ddots& \\ddots &0\\\\\n0& \\cdots  & -m_{n,k} &\\cdots&0&1\n\\end{pmatrix}.\n\\] Multiplying by \\(F^{(k)}\\) has the effect of subtracting \\(m_{ik}\\) times row \\(k\\) from row \\(i\\), for \\(i=k+1,\\ldots,n\\).\n\n\n\n\n\n\nA matrix with this structure (the identity except for a single column below the diagonal) is called a Frobenius matrix.\n\n\n\n\nExample 3.5You can check in the earlier example that \\[\nF^{(1)}A=\\begin{pmatrix}\n1 & 0 & 0\\\\\n-1 & 1 & 0\\\\\n-2 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 2 & 1\\\\\n1 & -2 & 2\\\\\n2 & 12 & -2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 2 & 1\\\\\n0 & -4 & 1\\\\\n0 & 8 & -4\n\\end{pmatrix} = A^{(2)},\n\\] and \\[\nF^{(2)}A^{(2)}=\n\\begin{pmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 2 & 1\\\\\n0 & -4 & 1\\\\\n0 & 8 & -4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 2 & 1\\\\\n0 & -4 & 1\\\\\n0 & 0 & -2\n\\end{pmatrix} = A^{(3)}=U.\n\\]\n\n\nIt follows that \\[\nU = A^{(n)} = F^{(n-1)}F^{(n-2)}\\cdots F^{(1)}A.\n\\] Now the \\(F^{(k)}\\) are invertible, and the inverse is just given by adding rows instead of subtracting: \\[\n(F^{(k)})^{-1} = \\begin{pmatrix}\n1 & 0&\\cdots & \\cdots &\\cdots&0\\\\\n0 & \\ddots & \\ddots&&&\\vdots \\\\\n\\vdots & \\ddots& 1 &\\ddots&&\\vdots \\\\\n\\vdots & & m_{k+1,k} & \\ddots &\\ddots&\\vdots \\\\\n\\vdots & & \\vdots & \\ddots& \\ddots &0\\\\\n0& \\cdots  & m_{n,k} &\\cdots&0&1\n\\end{pmatrix}.\n\\] So we could write \\[\nA = (F^{(1)})^{-1}(F^{(2)})^{-1}\\cdots (F^{(n-1)})^{-1}U.\n\\] Since the successive operations don’t “interfere” with each other, we can write \\[\n  (F^{(1)})^{-1}(F^{(2)})^{-1}\\cdots (F^{(n-1)})^{-1} =  \\begin{pmatrix}\n1 & 0& \\cdots&\\cdots&\\cdots&0\\\\\nm_{2,1} & 1 &\\ddots&&&\\vdots\\\\\nm_{3,1} & m_{3,2} & 1&\\ddots&&\\vdots\\\\\nm_{4,1} & m_{4,2} & m_{4,3}&\\ddots&\\ddots&\\vdots\\\\\n\\vdots & \\vdots&\\vdots&&1&0\\\\\nm_{n,1} & m_{n,2} & m_{n,3}&\\cdots&m_{n,n-1} &1\n\\end{pmatrix} := L.\n\\] Thus we have established the following result.\n\nTheorem 3.1: LU decompositionLet \\(U\\) be the upper triangular matrix from Gaussian elimination of \\(A\\) (without pivoting), and let \\(L\\) be the unit lower triangular matrix above. Then \\[\nA = LU.\n\\]\n\n\n\n\n\n\n\n\nUnit lower triangular means that there are all 1’s on the diagonal.\n\n\n\nThe theorem above says that Gaussian elimination is equivalent to factorising \\(A\\) as the product of a lower triangular and an upper triangular matrix. This is not at all obvious from the algorithm! The decomposition is unique up to a scaling \\(LD\\), \\(D^{-1}U\\) for some diagonal matrix \\(D\\).\nThe system \\(A\\mathbf{x}=\\mathbf{b}\\) becomes \\(LU\\mathbf{x}=\\mathbf{b}\\), which we can readily solve by setting \\(U\\mathbf{x}=\\mathbf{y}\\). We first solve \\(L\\mathbf{y}=\\mathbf{b}\\) for \\(\\mathbf{y}\\), then \\(U\\mathbf{x}=\\mathbf{y}\\) for \\(\\mathbf{x}\\). Both are triangular systems.\nMoreover, if we want to solve several systems \\(A\\mathbf{x} = \\mathbf{b}\\) with different \\(\\mathbf{b}\\) but the same matrix, we just need to compute \\(L\\) and \\(U\\) once. This saves time because, although the initial \\(LU\\) factorisation takes \\({\\cal O}(n^3)\\) operations, the evaluation takes only \\({\\cal O}(n^2)\\).\n\n\n\n\n\n\nThis matrix factorisation viewpoint dates only from the 1940s, and LU decomposition was introduced by Alan Turing in a 1948 paper (Q. J. Mechanics Appl. Mat. 1, 287). Other common factorisations used in numerical linear algebra are QR (which we will see later) and Cholesky.\n\n\n\n\nExample 3.6Solve our earlier example by LU decomposition.\n\\[\n\\begin{pmatrix}\n1 & 2 & 1\\\\\n1 & -2 & 2\\\\\n2 & 12 & -2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1\\\\ x_2\\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\\\\ 4\\\\ 4\n\\end{pmatrix}.\n\\]\nWe apply Gaussian elimination as before, but ignore \\(\\mathbf{b}\\) (for now), leading to \\[\nU = \\begin{pmatrix}\n1 & 2 & 1\\\\\n0 & -4 & 1\\\\\n0 & 0 & -2\n\\end{pmatrix}.\n\\] As we apply the elimination, we record the multipliers so as to construct the matrix \\[\nL = \\begin{pmatrix}\n1 & 0 & 0\\\\\n1 & 1 & 0\\\\\n2 & -2 & 1\n\\end{pmatrix}.\n\\] Thus we have the factorisation/decomposition \\[\n\\begin{pmatrix}\n1 & 2 & 1\\\\\n1 & -2 & 2\\\\\n2 & 12 & -2\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 & 0\\\\\n1 & 1 & 0\\\\\n2 & -2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 2 & 1\\\\\n0 & -4 & 1\\\\\n0 & 0 & -2\n\\end{pmatrix}.\n\\]\nWith the matrices \\(L\\) and \\(U\\), we can readily solve for any right-hand side \\(\\mathbf{b}\\). We illustrate for our particular \\(\\mathbf{b}\\). Firstly, solve \\(L\\mathbf{y}=\\mathbf{b}\\): \\[\n\\begin{pmatrix}\n1 & 0 & 0\\\\\n1 & 1 & 0\\\\\n2 & -2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1\\\\y_2\\\\y_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0\\\\ 4\\\\ 4\n\\end{pmatrix}\n\\] \\[\n\\implies y_1 = 0, \\,\\, y_2 = 4-y_1 =4, \\,\\, y_3=4-2y_1+2y_2 = 12.\n\\] Notice that \\(\\mathbf{y}\\) is the right-hand side \\(\\mathbf{b}^{(3)}\\) constructed earlier. Then, solve \\(U\\mathbf{x} = \\mathbf{y}\\): \\[\n\\begin{pmatrix}\n1 & 2 & 1\\\\\n0 & -4 & 1\\\\\n0 & 0 & -2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1\\\\x_2\\\\x_3\n\\end{pmatrix}=\n\\begin{pmatrix}\n0\\\\4\\\\12\n\\end{pmatrix}\n\\] \\[\n\\implies x_3 = -6, \\,\\, x_2=-\\tfrac14(4-x_3)=-\\tfrac52, \\,\\, x_1 = -2x_2 - x_3 = 11.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "chap-three.html#vector-norms",
    "href": "chap-three.html#vector-norms",
    "title": "3  Linear Algebra",
    "section": "3.5 Vector norms",
    "text": "3.5 Vector norms\nTo measure the error when the solution is a vector, as opposed to a scalar, we usually summarize the error in a single number called a norm. A norm effectively gives us a way to define a notion of distance in higher dimensions.\nA vector norm on \\(\\mathbb{R}^n\\) is a real-valued function that satisfies: 1. \\(\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|\\) for every \\(\\mathbf{x},\\mathbf{y}\\in \\mathbb{R}^n\\) (triangle inequality). 2. \\(\\|\\alpha \\mathbf{x}\\| = |\\alpha|\\,\\|\\mathbf{x}\\|\\) for every \\(\\mathbf{x}\\in \\mathbb{R}^n\\) and every \\(\\alpha\\in\\mathbb{R}\\). 3. \\(\\|\\mathbf{x}\\| \\geq 0\\) for every \\(\\mathbf{x}\\in \\mathbb{R}^n\\), and \\(\\|\\mathbf{x}\\|=0\\) implies \\(\\mathbf{x}=0\\).\n\nExample 3.7: There are three common examples:\n\nThe \\(\\ell_2\\)-norm\n\\[\n\\|\\mathbf{x}\\|_2 := \\sqrt{\\sum_{k=1}^n x_k^2} = \\sqrt{\\mathbf{x}^\\top \\mathbf{x}}.\n\\] This is just the usual Euclidean length of \\(\\mathbf{x}\\).\nThe \\(\\ell_1\\)-norm\n\\[\n\\|\\mathbf{x}\\|_1 := \\sum_{k=1}^n |x_k|.\n\\] This is sometimes known as the taxicab or Manhattan norm, because it corresponds to the distance that a taxi has to drive on a rectangular grid of streets to get to \\(\\mathbf{x}\\in\\mathbb{R}^2\\).\nThe \\(\\ell_\\infty\\)-norm\n\\[\n\\|\\mathbf{x}\\|_\\infty := \\max_{k=1,\\ldots,n} |x_k|.\n\\] This is sometimes known as the maximum norm.\n\n\n\nThe norms in the example above are all special cases of the \\(\\ell_p\\)-norm, \\[\n\\|\\mathbf{x}\\|_p = \\left(\\sum_{k=1}^n |x_k|^p\\right)^{1/p},\n\\] which is a norm for any real number \\(p\\geq 1\\). Increasing \\(p\\) means that more and more emphasis is given to the maximum element \\(|x_k|\\).\n\nExample 3.8: Consider the vectors \\(\\mathbf{a}=(1,-2,3)^\\top\\), \\(\\mathbf{b}=(2,0,-1)^\\top\\), and \\(\\mathbf{c}=(0,1,4)^\\top\\).The \\(\\ell_1\\)-, \\(\\ell_2\\)-, and \\(\\ell_\\infty\\)-norms are: \\[\n\\begin{aligned}\n\\|\\mathbf{a}\\|_1 &= 1 + 2 + 3 = 6 \\\\\n\\|\\mathbf{b}\\|_1 &= 2 + 0 + 1 = 3 \\\\\n\\|\\mathbf{c}\\|_1 &= 0 + 1 + 4 = 5 \\\\\n\\\\\n\\|\\mathbf{a}\\|_2 &= \\sqrt{1 + 4 + 9} \\approx 3.74 \\\\\n\\|\\mathbf{b}\\|_2 &= \\sqrt{4 + 0 + 1} \\approx 2.24 \\\\\n\\|\\mathbf{c}\\|_2 &= \\sqrt{0 + 1 + 16} \\approx 4.12 \\\\\n\\\\\n\\|\\mathbf{a}\\|_\\infty &= \\max\\{1,2,3\\} = 3 \\\\\n\\|\\mathbf{b}\\|_\\infty &= \\max\\{2,0,1\\} = 2 \\\\\n\\|\\mathbf{c}\\|_\\infty &= \\max\\{0,1,4\\} = 4\n\\end{aligned}\n\\] Notice that, for a single vector \\(\\mathbf{x}\\), the norms satisfy the ordering \\(\\|\\mathbf{x}\\|_1 \\geq \\|\\mathbf{x}\\|_2 \\geq \\|\\mathbf{x}\\|_\\infty\\), but that vectors may be ordered differently by different norms.\n\n\n\nExample 3.9: Sketch the ‘unit circles’ \\(\\{\\mathbf{x}\\in\\mathbb{R}^2 : \\|\\mathbf{x}\\|_p=1\\}\\) for \\(p=1,2,\\infty\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "chap-three.html#matrix-norms",
    "href": "chap-three.html#matrix-norms",
    "title": "3  Linear Algebra",
    "section": "3.6 Matrix norms",
    "text": "3.6 Matrix norms\nWe also use norms to measure the “size” of matrices. Since the set \\(\\mathbb{R}^{n\\times n}\\) of \\(n\\times n\\) matrices with real entries is a vector space, we could just use a vector norm on this space. But usually we add an additional axiom.\nA matrix norm is a real-valued function \\(\\|\\cdot\\|\\) on \\(\\mathbb{R}^{n\\times n}\\) that satisfies:\n\n\\(\\|A + B\\| \\leq \\|A\\| + \\|B\\|\\) for every \\(A,B\\in\\mathbb{R}^{n\\times n}\\).\n\\(\\|\\alpha A\\| = |\\alpha|\\,\\|A\\|\\) for every \\(A\\in \\mathbb{R}^{n\\times n}\\) and every \\(\\alpha\\in\\mathbb{R}\\).\n\\(\\|A\\| \\geq 0\\) for every \\(A\\in \\mathbb{R}^{n\\times n}\\) and \\(\\|A\\|=0\\) implies \\(A=0\\).\n\\(\\|AB\\| \\leq \\|A\\|\\|B\\|\\) for every \\(A,B\\in\\mathbb{R}^{n\\times n}\\) (consistency).\n\n\n\n\n\n\n\nWe usually want this additional axiom because matrices are more than just vectors. Some books call this a submultiplicative norm and define a “matrix norm” to satisfy just the first three properties, perhaps because (4) only works for square matrices.\n\n\n\n\nExample 3.10: Frobenius normIf we treat a matrix as a big vector with \\(n^2\\) components, then the \\(\\ell_2\\)-norm is called the Frobenius norm of the matrix: \\[\n\\|A\\|_F = \\sqrt{\\sum_{i=1}^n\\sum_{j=1}^n a_{ij}^2}.\n\\] This norm is rarely used in numerical analysis because it is not induced by any vector norm (as we are about to define).\n\n\nThe most important matrix norms are so-called induced or operator norms. Remember that \\(A\\) is a linear map on \\(\\mathbb{R}^n\\), meaning that it maps every vector to another vector. So we can measure the size of \\(A\\) by how much it can stretch vectors with respect to a given vector norm. Specifically, if \\(\\|\\cdot\\|_p\\) is a vector norm, then the induced norm is defined as \\[\n\\|A\\|_p := \\sup_{\\mathbf{x}\\neq \\boldsymbol{0}}\\frac{\\|A\\mathbf{x}\\|_p}{\\|\\mathbf{x}\\|_p} = \\max_{\\|\\mathbf{x}\\|_p=1}\\|A\\mathbf{x}\\|_p.\n\\] To see that the two definitions here are equivalent, use the fact that \\(\\|\\cdot\\|_p\\) is a vector norm. So by property (2) we have \\[\n\\sup_{\\mathbf{x}\\neq \\boldsymbol{0}}\\frac{\\|A\\mathbf{x}\\|_p}{\\|\\mathbf{x}\\|_p} = \\sup_{\\mathbf{x}\\neq \\boldsymbol{0}}\\left\\| A\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_p}\\right\\|_p = \\sup_{\\|\\mathbf{y}\\|_p=1}\\|A\\mathbf{y}\\|_p = \\max_{\\|\\mathbf{y}\\|_p=1}\\|A\\mathbf{y}\\|_p.\n\\]\n\n\n\n\n\n\nUsually we use the same notation for the induced matrix norm as for the original vector norm. The meaning should be clear from the context.\n\n\n\n\nExample 3.11Let \\[\nA = \\begin{pmatrix}\n0 & 1\\\\\n3 & 0\n\\end{pmatrix}.\n\\] In the \\(\\ell_2\\)-norm, a unit vector in \\(\\mathbb{R}^2\\) has the form \\(\\mathbf{x}=(\\cos\\theta,\\sin\\theta)^\\top\\), so the image of the unit circle is \\[\nA\\mathbf{x} = \\begin{pmatrix}\n\\sin\\theta\\\\\n3\\cos\\theta\n\\end{pmatrix}.\n\\] This is illustrated below:\n\n\n\n\n\nThe induced matrix norm is the maximum stretching of this unit circle, which is \\[\n\\|A\\|_2 = \\max_{\\|\\mathbf{x}\\|_2=1}\\|A\\mathbf{x}\\|_2 = \\max_\\theta\\big(\\sin^2\\theta + 9\\cos^2\\theta \\big)^{1/2} = \\max_\\theta\\big(1 + 8\\cos^2\\theta\\big)^{1/2} = 3.\n\\]\n\n\n\nTheorem 3.2: Induced norms are matrix normsThe induced norm corresponding to any vector norm is a matrix norm, and the two norms satisfy \\(\\|A\\mathbf{x}\\| \\leq \\|A\\|\\|\\mathbf{x}\\|\\) for any matrix \\(A\\in\\mathbb{R}^{n\\times n}\\) and any vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\).\n\n\nProof:\nProperties (1)-(3) follow from the fact that the vector norm satisfies the corresponding properties. To show (4), note that, by the definition above, we have for any vector \\(\\mathbf{y}\\in\\mathbb{R}^n\\) that \\[\n\\|A\\| \\geq \\frac{\\|A\\mathbf{y}\\|}{\\|\\mathbf{y}\\|} \\quad \\implies \\quad\n\\|A\\mathbf{y}\\| \\leq \\|A\\|\\|\\mathbf{y}\\|.\n\\] Taking \\(\\mathbf{y} = B\\mathbf{x}\\) for some \\(\\mathbf{x}\\) with \\(\\|\\mathbf{x}\\|=1\\), we get \\[\n\\|AB\\mathbf{x}\\|\\leq\\|A\\|\\|B\\mathbf{x}\\| \\leq \\|A\\|\\|B\\|.\n\\] This holds in particular for the vector \\(\\mathbf{x}\\) that maximises \\(\\|AB\\mathbf{x}\\|\\), so \\[\n\\|AB\\| = \\max_{\\|\\mathbf{x}\\|=1}\\|AB\\mathbf{x}\\| \\leq \\|A\\|\\|B\\|.\n\\]\nIt is cumbersome to compute the induced norms from their definition, but fortunately there are some very useful alternative formulae.\n\nTheorem 3.3: Matrix norms induced by \\(\\ell_1\\) and \\(\\ell_\\infty\\)The matrix norms induced by the \\(\\ell_1\\)-norm and \\(\\ell_\\infty\\)-norm satisfy \\[\n\\|A\\|_1 = \\max_{j=1,\\ldots,n}\\sum_{i=1}^n|a_{ij}|, \\quad \\text{(maximum column sum)}\n\\] \\[\n\\|A\\|_\\infty = \\max_{i=1,\\ldots,n}\\sum_{j=1}^n|a_{ij}|. \\quad \\text{(maximum row sum)}\n\\]\n\n\nProof:\nWe will prove the result for the \\(\\ell_1\\)-norm as an illustration of the method: \\[\n\\|A\\mathbf{x}\\|_1 = \\sum_{i=1}^n\\left|\\sum_{j=1}^n a_{ij}x_j\\right| \\leq \\sum_{i=1}^n\\sum_{j=1}^n|a_{ij}|\\,|x_j| = \\sum_{j=1}^n|x_j|\\sum_{i=1}^n|a_{ij}|.\n\\] If we let \\[\nc = \\max_{j=1,\\ldots,n}\\sum_{i=1}^n|a_{ij}|,\n\\] then \\[\n\\|A\\mathbf{x}\\|_1 \\leq c\\|\\mathbf{x}\\|_1 \\quad \\implies \\|A\\|_1 \\leq c.\n\\] Now let \\(m\\) be the column where the maximum sum is attained. If we choose \\(\\mathbf{y}\\) to be the vector with components \\(y_k=\\delta_{km}\\), then we have \\(\\|A\\mathbf{y}\\|_1 = c\\). Since \\(\\|\\mathbf{y}\\|_1=1\\), we must have that \\[\n\\max_{\\|\\mathbf{x}\\|_1=1}\\|A\\mathbf{x}\\|_1 \\geq \\|A\\mathbf{y}\\|_1=c \\quad \\implies \\|A\\|_1 \\geq c.\n\\] The only way to satisfy both inequalities is if \\(\\|A\\|_1=c\\).\n\nExample 3.12For the matrix \\[\nA = \\begin{pmatrix}\n-7 & 3 & -1\\\\\n2 & 4 & 5\\\\\n-4 & 6 & 0\n\\end{pmatrix}\n\\] we have \\[\n\\|A\\|_1 = \\max\\{13, 13, 6\\} = 13, \\qquad \\|A\\|_\\infty = \\max\\{11,11,10\\} = 11.\n\\]\n\n\nWhat about the matrix norm induced by the \\(\\ell_2\\)-norm? This turns out to be related to the eigenvalues of \\(A\\). Recall that \\(\\lambda\\in\\mathbb{C}\\) is an eigenvalue of \\(A\\) with associated eigenvector \\(\\mathbf{u}\\) if \\[\nA\\mathbf{u} = \\lambda\\mathbf{u}.\n\\] We define the spectral radius \\(\\rho(A)\\) of \\(A\\) to be the maximum \\(|\\lambda|\\) over all eigenvalues \\(\\lambda\\) of \\(A\\).\n\nTheorem 3.4: Spectral normThe matrix norm induced by the \\(\\ell_2\\)-norm satisfies \\[\n\\|A\\|_2 = \\sqrt{\\rho(A^\\top A)}.\n\\]\n\n\nAs a result of the theorem above, this norm is sometimes known as the spectral norm.\n\nExample 3.13For our matrix \\[\nA = \\begin{pmatrix}\n0 & 1\\\\\n3 & 0\n\\end{pmatrix},\n\\] we have \\[\nA^\\top A = \\begin{pmatrix}\n0 & 3\\\\\n1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 1\\\\\n3 & 0\n\\end{pmatrix}\n=\\begin{pmatrix}\n9 & 0\\\\\n0 & 1\n\\end{pmatrix}.\n\\] We see that the eigenvalues of \\(A^\\top A\\) are \\(\\lambda=1,9\\), so \\(\\|A\\|_2=\\sqrt{9}=3\\) (as we calculated earlier).\n\n\nProof:\nWe want to show that \\[\n\\max_{\\|\\mathbf{x}\\|_2 = 1}\\|A\\mathbf{x}\\|_2  = \\max\\{\\sqrt{|\\lambda|} \\,: \\,\\textrm{$\\lambda$ eigenvalue of $A^\\top A$} \\}.\n\\] For \\(A\\) real, \\(A^\\top A\\) is symmetric, so has real eigenvalues \\(\\lambda_1 \\leq\\lambda_2 \\leq \\ldots \\leq \\lambda_n\\) with corresponding orthonormal eigenvectors \\(\\mathbf{u}_1, \\ldots,\\mathbf{u}_n\\) in \\(\\mathbb{R}^n\\). (Orthonormal means that \\(\\mathbf{u}_j^\\top \\mathbf{u}_k = \\delta_{jk}\\).) Note also that all of the eigenvalues are non-negative, since \\[\nA^\\top A\\mathbf{u}_1 = \\lambda_1\\mathbf{u}_1 \\quad \\implies \\lambda_1 = \\frac{\\mathbf{u}_1^\\top A^\\top A\\mathbf{u}_1}{\\mathbf{u}_1^\\top\\mathbf{u}_1} = \\frac{\\|A\\mathbf{u}_1\\|_2^2}{\\|\\mathbf{u}_1\\|_2^2} \\geq 0.\n\\] So we want to show that \\(\\|A\\|_2=\\sqrt{\\lambda_n}\\). The eigenvectors form a basis, so every vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) can be expressed as a linear combination \\(\\mathbf{x} = \\sum_{k=1}^n\\alpha_k\\mathbf{u}_k\\). Therefore \\[\n\\|A\\mathbf{x}\\|_2^2 = \\mathbf{x}^\\top A^\\top A\\mathbf{x} = \\mathbf{x}^\\top\\sum_{k=1}^n\\alpha_k\\lambda_k\\mathbf{u}_k = \\sum_{j=1}^n\\alpha_j\\mathbf{u}_j^\\top\\sum_{k=1}^n\\alpha_k\\lambda_k\\mathbf{u}_k = \\sum_{k=1}^n\\alpha_k^2\\lambda_k,\n\\] where the last step uses orthonormality of the \\(\\mathbf{u}_k\\). It follows that \\[\n\\|A\\mathbf{x}\\|_2^2 \\leq \\lambda_n\\sum_{k=1}^n\\alpha_k^2.\n\\] But if \\(\\|\\mathbf{x}\\|_2=1\\), then \\(\\|\\mathbf{x}\\|_2^2=\\sum_{k=1}^n\\alpha_k^2 = 1\\), so \\(\\|A\\mathbf{x}\\|_2^2 \\leq \\lambda_n\\). To show that the maximum of \\(\\|A\\mathbf{x}\\|_2^2\\) is equal to \\(\\lambda_n\\), we can choose \\(\\mathbf{x}\\) to be the corresponding eigenvector \\(\\mathbf{x}=\\mathbf{u}_n\\). In that case, \\(\\alpha_1=\\ldots=\\alpha_{n-1}=0\\) and \\(\\alpha_n=1\\), so \\(\\|A\\mathbf{x}\\|_2^2 =\\lambda_n\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "chap-three.html#conditioning",
    "href": "chap-three.html#conditioning",
    "title": "3  Linear Algebra",
    "section": "3.7 Conditioning",
    "text": "3.7 Conditioning\nSome linear systems are inherently more difficult to solve than others, because the solution is sensitive to small perturbations in the input. We will examine how to quantify this sensitivity and how to adjust our methods to control for it.\n\nExample 3.14Consider the linear system \\[\n\\begin{pmatrix}\n1 & 1\\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1\\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n1\\\\ 1\n\\end{pmatrix}\n\\implies\n\\begin{pmatrix}\nx_1\\\\ x_2\n\\end{pmatrix}=\n\\begin{pmatrix}\n0\\\\ 1\n\\end{pmatrix}.\n\\] If we add a small rounding error \\(0&lt;\\delta \\ll 1\\) to the data \\(b_1\\) then \\[\n\\begin{pmatrix}\n1 & 1\\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1\\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 + \\delta\\\\ 1\n\\end{pmatrix}\n\\implies\n\\begin{pmatrix}\nx_1\\\\ x_2\n\\end{pmatrix}=\n\\begin{pmatrix}\n\\delta\\\\ 1\n\\end{pmatrix}.\n\\] The solution is within rounding error of the true solution, so the system is called well conditioned.\n\n\n\nExample 3.15Now let \\(\\epsilon \\ll 1\\) be a fixed positive number, and consider the linear system \\[\n\\begin{pmatrix}\n\\epsilon & 1\\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1\\\\ x_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 + \\delta\\\\ 1\n\\end{pmatrix}\n\\implies\n\\begin{pmatrix}\nx_1\\\\ x_2\n\\end{pmatrix}=\n\\begin{pmatrix}\n\\delta/\\epsilon\\\\ 1\n\\end{pmatrix}.\n\\] The true solution is still \\((0,1)^\\top\\), but if the error \\(\\delta\\) is as big as the matrix entry \\(\\epsilon\\), then the solution for \\(x_1\\) will be completely wrong. This system is much more sensitive to errors in \\(\\mathbf{b}\\), so is called ill-conditioned.\nGraphically, this system (right) is more sensitive to \\(\\delta\\) than the first system (left) because the two lines are closer to parallel:\n\n\n\n\n\n\n\nTo measure the conditioning of a linear system, consider the following estimate of the ratio of the relative errors in the output (\\(x\\)) versus the input (\\(b\\)): \\[\n\\begin{aligned}\n\\frac{|\\textrm{relative error in }\\mathbf{x}|}{|\\textrm{relative error in }\\mathbf{b}|}\n&= \\frac{\\|\\delta\\mathbf{x}\\|/\\|\\mathbf{x}\\|}{\\|\\delta\\mathbf{b}\\|/\\|\\mathbf{b}\\|}\n= \\left(\\frac{\\|\\delta\\mathbf{x}\\|}{\\|\\mathbf{x}\\|}\\right)\\left(\\frac{\\|\\mathbf{b}\\|}{\\|\\delta\\mathbf{b}\\|} \\right) \\\\\n&= \\left(\\frac{\\|A^{-1}\\delta\\mathbf{b}\\|}{\\|\\mathbf{x}\\|}\\right)\\left(\\frac{\\|\\mathbf{b}\\|}{\\|\\delta\\mathbf{b}\\|} \\right) \\\\\n&\\leq \\frac{\\|A^{-1}\\|\\|\\delta\\mathbf{b}\\|}{\\|\\mathbf{x}\\|}\\left(\\frac{\\|\\mathbf{b}\\|}{\\|\\delta\\mathbf{b}\\|} \\right) \\\\\n&= \\frac{\\|A^{-1}\\|\\|\\mathbf{b}\\|}{\\|\\mathbf{x}\\|} = \\frac{\\|A^{-1}\\|\\|A\\mathbf{x}\\|}{\\|\\mathbf{x}\\|}\\\\\n&\\leq \\|A^{-1}\\|\\|A\\|.\n\\end{aligned}\n\\]\nWe define the condition number of a matrix \\(A\\) in some induced norm \\(\\|\\cdot\\|_*\\) to be \\[\n\\kappa_*(A) = \\|A^{-1}\\|_*\\|A\\|_*.\n\\] If \\(\\kappa_*(A)\\) is large, then the solution will be sensitive to errors in \\(\\mathbf{b}\\), at least for some \\(\\mathbf{b}\\). A large condition number means that the matrix is close to being non-invertible (i.e. two rows are close to being linearly dependent).\n\n\n\n\n\n\nThis is a “worst case” amplification of the error by a given matrix. The actual result will depend on \\(\\delta\\mathbf{b}\\) (which we usually don’t know if it arises from previous rounding error).\n\n\n\nNote that \\(\\det(A)\\) will tell you whether a matrix is singular or not, but not whether it is ill-conditioned. Since \\(\\det(\\alpha A) = \\alpha^n\\det(A)\\), the determinant can be made arbitrarily large or small by scaling (which does not change the condition number). For instance, the matrix \\[\n\\begin{pmatrix}\n10^{-50} & 0\\\\\n0 & 10^{-50}\n\\end{pmatrix}\n\\] has tiny determinant but is well-conditioned.\n\nExample 3.16Return to our earlier examples and consider the condition numbers in the 1-norm.\nWe have (assuming \\(0&lt; \\epsilon \\ll 1\\)) that \\[\nA = \\begin{pmatrix}\n1 & 1\\\\\n0 & 1\n\\end{pmatrix} \\implies\nA^{-1} = \\begin{pmatrix}\n1 & -1\\\\\n0 & 1\n\\end{pmatrix} \\implies\n\\|A\\|_1 = \\|A^{-1}\\|_1 = 2 \\implies \\kappa_1(A) = 4,\n\\] \\[\nB = \\begin{pmatrix}\n\\epsilon & 1\\\\\n0 & 1\n\\end{pmatrix} \\implies\nB^{-1} = \\frac{1}{\\epsilon}\\begin{pmatrix}\n1 & -1\\\\\n0 & \\epsilon\n\\end{pmatrix}\n\\] \\[\n\\implies\n\\|B\\|_1 = 2, \\,\\, \\|B^{-1}\\|_1 = \\frac{1 + \\epsilon}{\\epsilon} \\implies \\kappa_1(B) = \\frac{2(1+\\epsilon)}{\\epsilon}.\n\\] For matrix \\(B\\), \\(\\kappa_1(B)\\to\\infty\\) as \\(\\epsilon\\to 0\\), showing that the matrix \\(B\\) is ill-conditioned.\n\n\n\nExample 3.17The Hilbert matrix \\(H_n\\) is the \\(n\\times n\\) symmetric matrix with entries \\[\n(h_n)_{ij} = \\frac{1}{i+j-1}.\n\\] These matrices are notoriously ill-conditioned. For example, \\(\\kappa_2(H_5) \\approx 4.8\\times 10^5\\), and \\(\\kappa_2(H_{20})\\approx 2.5\\times 10^{28}\\). Solving an associated linear system in floating-point arithmetic would be hopeless.\n\n\nA practical limitation of the condition number is that you have to know \\(A^{-1}\\) before you can calculate it. We can always estimate \\(\\|A^{-1}\\|\\) by taking some arbitrary vectors \\(\\mathbf{x}\\) and using \\[\n\\|A^{-1}\\| \\geq \\frac{\\|\\mathbf{x}\\|}{\\|\\mathbf{b}\\|}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "chap-three.html#iterative-methods",
    "href": "chap-three.html#iterative-methods",
    "title": "3  Linear Algebra",
    "section": "3.8 Iterative methods",
    "text": "3.8 Iterative methods\nFor large systems, the \\({\\cal O}(n^3)\\) cost of Gaussian elimination is prohibitive. Fortunately, many such systems that arise in practice are sparse, meaning that most of the entries of the matrix \\(A\\) are zero. In this case, we can often use iterative algorithms to do better than \\({\\cal O}(n^3)\\).\nIn this course, we will only study algorithms for symmetric positive definite matrices. A matrix \\(A\\) is called symmetric positive definite (or SPD) if \\(\\mathbf{x}^\\top A\\mathbf{x}&gt;0\\) for every vector \\(\\mathbf{x}\\neq 0\\).\n\n\n\n\n\n\nRecall that a symmetric matrix has real eigenvalues. It is positive definite iff all of its eigenvalues are positive.\n\n\n\n\nExample 3.18Show that the following matrix is SPD: \\[\nA = \\begin{pmatrix}\n3 & 1 & -1\\\\\n1 & 4 & 2\\\\\n-1 & 2 & 5\n\\end{pmatrix}.\n\\] With \\(\\mathbf{x} = (x_1,x_2,x_3)^\\top\\), we have \\[\n\\begin{aligned}\n\\mathbf{x}^\\top A\\mathbf{x} &= 3x_1^2 + 4x_2^2 + 5x_3^2 + 2x_1x_2 + 4x_2x_3 - 2x_1x_3\\\\\n&= x_1^2 + x_2^2 + 2x_3^2 + (x_1+x_2)^2 + (x_1-x_3)^2 + 2(x_2+x_3)^2.\n\\end{aligned}\n\\] This is positive for any non-zero vector \\(\\mathbf{x}\\in\\mathbb{R}^3\\), so \\(A\\) is SPD (eigenvalues \\(1.29\\), \\(4.14\\) and \\(6.57\\)).\n\n\nIf \\(A\\) is SPD, then solving \\(A\\mathbf{x} = \\mathbf{b}\\) is equivalent to minimizing the quadratic functional \\[\nf:\\mathbb{R}^n \\to \\mathbb{R}, \\qquad f(\\mathbf{x}) = \\tfrac12\\mathbf{x}^\\top A\\mathbf{x} -\\mathbf{b}^\\top\\mathbf{x}.\n\\] When \\(A\\) is SPD, this functional behaves like a U-shaped parabola, and has a unique finite global minimizer \\(\\mathbf{x}_*\\) such that \\(f(\\mathbf{x}_*)&lt; f(\\mathbf{x})\\) for all \\(\\mathbf{x}\\in\\mathbb{R}^n\\), \\(\\mathbf{x}\\neq\\mathbf{x}_*\\).\nTo find \\(\\mathbf{x}_*\\), we need to set \\(\\nabla f = \\boldsymbol{0}\\). We have \\[\nf(\\mathbf{x}) = \\tfrac12\\sum_{i=1}^n x_i\\left(\\sum_{j=1}^n a_{ij}x_j\\right) - \\sum_{j=1}^nb_jx_j\n\\] so \\[\n\\begin{aligned}\n\\frac{\\partial f}{\\partial x_k} &= \\tfrac12\\left(\\sum_{i=1}^n x_ia_{ik} + \\sum_{j=1}^na_{kj}x_j \\right) - b_k \\\\ &= \\tfrac12\\left(\\sum_{i=1}^n a_{ki}x_i + \\sum_{j=1}^na_{kj}x_j \\right) - b_k = \\sum_{j=1}^na_{kj}x_j - b_k.\n\\end{aligned}\n\\] In the penultimate step we used the symmetry of \\(A\\) to write \\(a_{ik}=a_{ki}\\). It follows that \\[\n\\nabla f = A\\mathbf{x} - \\mathbf{b},\n\\] so locating the minimum of \\(f(\\mathbf{x})\\) is indeed equivalent to solving \\(A\\mathbf{x}=\\mathbf{b}\\).\n\n\n\n\n\n\nMinimizing functions is a vast sub-field of numerical analysis known as optimization. We will only cover this specific case.\n\n\n\nA popular class of methods for optimization are line search methods, where at each iteration the search is restricted to a single search direction \\(\\mathbf{d}_k\\). The iteration takes the form \\[\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k\\mathbf{d}_k.\n\\] The step size \\(\\alpha_k\\) is chosen by minimizing \\(f(\\mathbf{x})\\) along the line \\(\\mathbf{x} = \\mathbf{x}_k + \\alpha\\mathbf{d}_k\\). For our functional above, we have \\[\n\\begin{aligned}\nf(\\mathbf{x}_{k} + \\alpha\\mathbf{d}_{k}) &= \\big(\\tfrac12\\mathbf{d}_{k}^\\top A\\mathbf{d}_{k}\\big)\\alpha^2 + \\mathbf{d}_{k}^\\top\\big(  A\\mathbf{x}_k - \\mathbf{b} \\big)\\alpha + \\tfrac12\\mathbf{x}_k^\\top A\\mathbf{x}_k - \\mathbf{b}^\\top\\mathbf{x}_k.\n\\end{aligned}\n\\] This is a quadratic in \\(\\alpha\\), and the coefficient of \\(\\alpha^2\\) is positive because \\(A\\) is positive definite. It is therefore a U-shaped parabola and achieves its minimum when \\[\n\\frac{\\partial f}{\\partial\\alpha} = \\mathbf{d}_k^\\top A\\mathbf{d}_k \\alpha + \\mathbf{d}_k^\\top\\big(A\\mathbf{x}_k - \\mathbf{b}\\big) = 0.\n\\] Defining the residual \\(\\mathbf{r}_k := A\\mathbf{x}_k - \\mathbf{b}\\), we see that the desired choice of step size is \\[\n\\alpha_k = - \\frac{\\mathbf{d}_k^\\top\\mathbf{r}_k}{\\mathbf{d}_k^\\top A\\mathbf{d}_k}.\n\\]\nDifferent line search methods differ in how the search direction \\(\\mathbf{d}_k\\) is chosen at each iteration. For example, the method of steepest descent sets \\[\n\\mathbf{d}_k = - \\nabla f (\\mathbf{x}_k) = -\\mathbf{r}_k,\n\\] where we have remembered the gradient formula above.\n\nExample 3.19Use the method of steepest descent to solve the system \\[\n\\begin{pmatrix}\n3 & 2\\\\ 2 & 6\n\\end{pmatrix}\\begin{pmatrix}\nx_1\\\\ x_2\n\\end{pmatrix}=\\begin{pmatrix}\n2\\\\ -8\n\\end{pmatrix}.\n\\] Starting from \\(\\mathbf{x}_0=(-2,-2)^\\top\\), we get \\[\n\\begin{aligned}\n\\mathbf{d}_0 = \\mathbf{b} - A\\mathbf{x}_0 = \\begin{pmatrix}\n12\\\\ 8\n\\end{pmatrix} &\\implies \\alpha_0 = \\frac{\\mathbf{d}_0^\\top\\mathbf{d}_0}{\\mathbf{d}_0^\\top A\\mathbf{d}_0} = \\frac{208}{1200} \\\\ &\\implies \\mathbf{x}_1 = \\mathbf{x}_0 + \\alpha_0\\mathbf{d}_0 \\approx \\begin{pmatrix}\n0.08\\\\ -0.613\n\\end{pmatrix}.\n\\end{aligned}\n\\] Continuing the iteration, \\(\\mathbf{x}_k\\) proceeds towards the solution \\((2,-2)^\\top\\) as illustrated below. The coloured contours show the value of \\(f(x_1,x_2)\\).\n\n\n\n\n\n\n\nUnfortunately, the method of steepest descent can be slow to converge. In the conjugate gradient method, we still take \\(\\mathbf{d}_0=-\\mathbf{r}_0\\), but subsequent search directions \\(\\mathbf{d}_k\\) are chosen to be \\(A\\)-conjugate, meaning that \\[\n\\mathbf{d}_{k+1}^\\top A \\mathbf{d}_k = 0.\n\\] This means that minimization in one direction does not undo the previous minimizations.\nIn particular, we construct \\(\\mathbf{d}_{k+1}\\) by writing \\[\n\\mathbf{d}_{k+1} = -\\mathbf{r}_{k+1} + \\beta_k\\mathbf{d}_k,\n\\] then choosing the scalar \\(\\beta_k\\) such that \\(\\mathbf{d}_{k+1}^\\top A\\mathbf{d}_k = 0\\). This gives \\[\n0 = \\big(-\\mathbf{r}_{k+1} + \\beta_k\\mathbf{d}_k\\big)^\\top A \\mathbf{d}_{k} = -\\mathbf{r}_{k+1}^\\top A \\mathbf{d}_k + \\beta_k\\mathbf{d}_k^\\top A \\mathbf{d}_k\n\\] and hence \\[\n\\beta_k = \\frac{\\mathbf{r}_{k+1}^\\top A\\mathbf{d}_k}{\\mathbf{d}_k^\\top A \\mathbf{d}_k}.\n\\]\nThus we get the basic conjugate gradient algorithm.\n\nAlgorithm 3.2: Conjugate gradient method\nStart with an initial guess \\(\\mathbf{x}_0\\) and initial search direction \\(\\mathbf{d}_0 = -\\mathbf{r}_0 = \\mathbf{b} - A\\mathbf{x}_0\\). For each \\(k=0,1,\\ldots\\), do the following:\n\nCompute step size \\[\n\\alpha_k = -\\frac{\\mathbf{d}_k^\\top\\mathbf{r}_k}{\\mathbf{d}_k^\\top A \\mathbf{d}_k}.\n\\]\nCompute \\(\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k\\mathbf{d}_k\\).\nCompute residual \\(\\mathbf{r}_{k+1} = A\\mathbf{x}_{k+1} - \\mathbf{b}\\).\nIf \\(\\|\\mathbf{r}_{k+1}\\| &lt;\\) tolerance, output \\(\\mathbf{x}_{k+1}\\) and stop.\nDetermine new search direction \\[\n\\mathbf{d}_{k+1} = -\\mathbf{r}_{k+1} + \\beta_k\\mathbf{d}_k \\quad \\textrm{where} \\quad\\beta_k = \\frac{\\mathbf{r}_{k+1}^\\top A \\mathbf{d}_k}{\\mathbf{d}_k^\\top A \\mathbf{d}_k}.\n\\]\n\n\n\n\nExample 3.20Solve our previous example with the conjugate gradient method.\nStarting with \\(\\mathbf{x}_0=(-2,-2)^\\top\\), the first step is the same as in steepest descent, giving \\(\\mathbf{x}_1 = (0.08, -0.613)^\\top\\). But then we take \\[\n\\mathbf{r}_1 = A\\mathbf{x}_1 -\\mathbf{b} = \\begin{pmatrix}\n-2.99\\\\ 4.48\n\\end{pmatrix}, \\quad \\beta_0 = \\frac{\\mathbf{r}_1^\\top A\\mathbf{d}_0}{\\mathbf{d}_0^\\top A\\mathbf{d}_0} = 0.139, \\quad \\mathbf{d}_1 = -\\mathbf{r}_1 + \\beta_0\\mathbf{d}_0 = \\begin{pmatrix}\n4.66\\\\-3.36\n\\end{pmatrix}.\n\\] The second iteration then gives \\[\n\\alpha_1 = -\\frac{\\mathbf{d}_1^\\top\\mathbf{r}_1}{\\mathbf{d}_1^\\top A\\mathbf{d}_1} = 0.412 \\implies \\mathbf{x}_2 = \\mathbf{x}_1 + \\alpha_1\\mathbf{d}_1 = \\begin{pmatrix}\n2\\\\-2\n\\end{pmatrix}.\n\\] This time there is no zig-zagging and the solution is reached in just two iterations:\n\n\n\n\n\n\n\nIn exact arithmetic, the conjugate gradient method will always give the exact answer in \\(n\\) iterations – one way to see this is to use the following.\n\nTheorem 3.5The residuals \\(\\mathbf{r}_k:=A\\mathbf{x}_k - \\mathbf{b}\\) at each stage of the conjugate gradient method are mutually orthogonal, meaning \\(\\mathbf{r}_j^\\top \\mathbf{r}_k = 0\\) for \\(j=0,\\ldots,k-1\\).\n\n\nAfter \\(n\\) iterations, the only residual vector that can be orthogonal to all of the previous ones is \\(\\mathbf{r}_n=\\boldsymbol{0}\\), so \\(\\mathbf{x}_n\\) must be the exact solution.\nIn practice, conjugate gradients is not competitive as a direct method. It is computationally intensive, and rounding errors can destroy the orthogonality, meaning that more than \\(n\\) iterations may be required. Instead, its main use is for large sparse systems. For suitable matrices (perhaps after preconditioning), it can converge very rapidly.\nWe can save computation by using the alternative formulae \\[\n\\mathbf{r}_{k+1} = \\mathbf{r}_k + \\alpha_k A\\mathbf{d}_k, \\quad\n\\alpha_k = \\frac{\\mathbf{r}_k^\\top \\mathbf{r}_k}{\\mathbf{d}_k^\\top A \\mathbf{d}_k}, \\quad\n\\beta_k = \\frac{\\mathbf{r}_{k+1}^\\top\\mathbf{r}_{k+1}}{\\mathbf{r}_k^\\top\\mathbf{r}_k}.\n\\] With these formulae, each iteration requires only one matrix-vector product, two vector-vector products, and three vector additions. Compare this to the basic algorithm above which requires two matrix-vector products, four vector-vector products and three vector additions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "chap-three.html#knowledge-checklist",
    "href": "chap-three.html#knowledge-checklist",
    "title": "3  Linear Algebra",
    "section": "Knowledge checklist",
    "text": "Knowledge checklist\nKey topics:\n\nDirect and iterative methods for solving linear systems: triangular systems, Gaussian elimination, LU decomposition, and iterative algorithms.\nVector and matrix norms, including induced matrix norms and their role in error analysis.\nConditioning and the condition number: sensitivity of solutions to input errors and implications for numeric stability.\n\nKey skills:\n\nFormulate and solve linear systems using direct and iterative methods (e.g., Gaussian elimination, LU decomposition).\nApply norms to measure errors and conditioning, and calculate condition numbers.\nAnalyze computational complexity and efficiency of matrix algorithms.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "chap-four.html",
    "href": "chap-four.html",
    "title": "4  Calculus",
    "section": "",
    "text": "4.1 Differentiation\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculus</span>"
    ]
  },
  {
    "objectID": "chap-four.html#s-diff",
    "href": "chap-four.html#s-diff",
    "title": "4  Calculus",
    "section": "",
    "text": "How do we differentiate functions numerically?\n\n\n4.1.1 Basics\nThe definition of the derivative as \\[\nf'(x_0) = \\lim_{h\\to 0}\\frac{f(x_0+h) - f(x_0)}{h},\n\\] suggests an obvious approximation: just pick some small finite \\(h\\) to give the estimate \\[\nf'(x_0) \\approx \\frac{f(x_0 + h) - f(x_0)}{h}.\n\\] For \\(h&gt;0\\) this is called a forward difference (and, for \\(h&lt;0\\), a backward difference). It is an example of a finite-difference formula.\n\n\n\n\n\nOf course, what we are doing with the forward difference is approximating \\(f'(x_0)\\) by the slope of the linear interpolant for \\(f\\) at the nodes \\(x_0\\) and \\(x_1=x_0 + h\\). So we could also have derived the forward difference formula by starting with the Lagrange form of the interpolating polynomial, \\[\nf(x) = \\frac{x-x_1}{x_0-x_1}f(x_0) + \\frac{x-x_0}{x_1-x_0}f(x_1) + \\frac{f''(\\xi)}{2}(x-x_0)(x-x_1)\n\\] for some \\(\\xi\\in[x_0,x_1]\\). Differentiating – and remembering that \\(\\xi\\) depends on \\(x\\), so that we need to use the chain rule – we get \\[\n\\begin{aligned}\nf'(x) &= \\frac{1}{x_0-x_1}f(x_0) + \\frac{1}{x_1-x_0}f(x_1) + \\frac{f''(\\xi)}{2}(2x - x_0 - x_1) \\\\\n&\\quad + \\frac{f'''(\\xi)}{2}\\left(\\frac{\\mathrm{d}\\xi}{\\mathrm{d}x}\\right)(x-x_0)(x-x_1),\\\\\n&\\implies f'(x_0) = \\frac{f(x_1) - f(x_0)}{x_1-x_0} + f''(\\xi)\\frac{x_0-x_1}{2}.\n\\end{aligned}\n\\] Equivalently, \\[\nf'(x_0) = \\frac{f(x_0+h)-f(x_0)}{h} - f''(\\xi)\\frac{h}{2}.\n\\] This shows that the truncation error for our forward difference approximation is \\(-f''(\\xi)h/2\\), for some \\(\\xi\\in[x_0,x_0+h]\\). In other words, a smaller interval or a less “wiggly” function will lead to a better estimate, as you would expect.\nAnother way to estimate the truncation error is to use Taylor’s theorem, which tells us that \\[\nf(x_0+h) = f(x_0) + h f'(x_0) + h^2\\frac{f''(\\xi)}{2},\n\\] for some \\(\\xi\\) between \\(x_0\\) and \\(x_0+h\\). Rearranging this will give back the forward difference error formula.\n\nExample 4.1: Derivative of \\(f(x)=\\log(x)\\) at \\(x_0=2\\).Using a forward-difference, we get the following sequence of approximations:\n\n\n\n\\(h\\)\nForward difference\nTruncation error\n\n\n\n\n1\n0.405465\n0.0945349\n\n\n0.1\n0.487902\n0.0120984\n\n\n0.01\n0.498754\n0.00124585\n\n\n0.001\n0.499875\n0.000124958\n\n\n\nIndeed the error is linear in \\(h\\), and we estimate that it is approximately \\(0.125 h\\) when \\(h\\) is small. This agrees with the error formula above, since \\(f''(x) = -x^{-2}\\), so we expect \\(-f''(\\xi)/2\\approx \\tfrac18\\).\n\n\nSince the error is linearly proportional to \\(h\\), the approximation is called linear, or first order.\n\n\n4.1.2 Higher-order finite differences\nTo get a higher-order approximation, we can differentiate a higher degree interpolating polynomial. This means that we need more nodes.\n\nExample 4.2: Central differenceTake three nodes \\(x_0\\), \\(x_1=x_0+h\\), and \\(x_2=x_0+2h\\). Then the Lagrange form of the interpolating polynomial is \\[\n\\begin{aligned}\nf(x) &= \\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}f(x_0) + \\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}f(x_1)\\\\ &+ \\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}f(x_2) \\\\\n&+ \\frac{f'''(\\xi)}{3!}(x-x_0)(x-x_1)(x-x_2).\n\\end{aligned}\n\\] Differentiating, we get \\[\n\\begin{aligned}\nf'(x) &= \\frac{2x - x_1 - x_2}{(x_0-x_1)(x_0-x_2)}f(x_0)\\\\ &+ \\frac{2x - x_0 - x_2}{(x_1-x_0)(x_1-x_2)}f(x_1)\\\\ &+ \\frac{2x-x_0-x_1}{(x_2-x_0)(x_2-x_1)}f(x_2)\\\\\n&+ \\frac{f'''(\\xi)}{6}\\Big((x-x_1)(x-x_2) + (x-x_0)(x-x_2) + (x-x_0)(x-x_1) \\Big) \\\\\n&+ \\frac{f^{(4)}(\\xi)}{6}\\left(\\frac{\\mathrm{d}\\xi}{\\mathrm{d}x}\\right)(x-x_0)(x-x_1)(x-x_2).\n\\end{aligned}\n\\] Now substitute in \\(x=x_1\\) to evaluate this at the central point: \\[\n\\begin{aligned}\nf'(x_1) &= \\frac{x_1 - x_2}{(x_0-x_1)(x_0-x_2)}f(x_0)\\\\ &+ \\frac{2x_1 - x_0 - x_2}{(x_1-x_0)(x_1-x_2)}f(x_1) + \\frac{x_1-x_0}{(x_2-x_0)(x_2-x_1)}f(x_2)\\\\\n&+ \\frac{f'''(\\xi)}{6}(x_1-x_0)(x_1-x_2),\\\\\n&= \\frac{-h}{2h^2}f(x_0) + 0 + \\frac{h}{2h^2}f(x_2) - \\frac{f'''(\\xi)}{6}h^2\\\\\n&= \\frac{f(x_1+h) - f(x_1-h)}{2h} - \\frac{f'''(\\xi)}{6}h^2.\n\\end{aligned}\n\\] This is called a central difference approximation for \\(f'(x_1)\\), and is frequently used in practice.\n\n\nTo see the quadratic behaviour of the truncation error, go back to our earlier example.\n\nExample 4.3: Derivative of \\(f(x)=\\log(x)\\) at \\(x=2\\)\n\n\n\n\n\n\n\n\n\n\n\\(h\\)\nForward difference\nTruncation error\nCentral difference\nTruncation error\n\n\n\n\n1\n0.405465\n0.0945349\n0.549306\n-0.0493061\n\n\n0.1\n0.487902\n0.0120984\n0.500417\n-0.000417293\n\n\n0.01\n0.498754\n0.00124585\n0.500004\n-4.16673e-06\n\n\n0.001\n0.499875\n0.000124958\n0.500000\n-4.16666e-08\n\n\n\nThe truncation error for the central difference is about \\(0.04h^2\\), which agrees with the formula since \\(f'''(\\xi)\\approx 2/2^3 = \\tfrac14\\) when \\(h\\) is small.\n\n\n\n\n4.1.3 Rounding error\nThe problem with numerical differentiation is that it involves subtraction of nearly-equal numbers. As \\(h\\) gets smaller, the problem gets worse.\nTo quantify this for the central difference, suppose that we have the correctly rounded values of \\(f(x_1\\pm h)\\), so that \\[\n\\text{fl}[f(x_1+h)] = (1+\\delta_1)f(x_1+h),\\qquad\n\\text{fl}[f(x_1-h)]=(1+\\delta_2)f(x_1-h),\n\\] where \\(|\\delta_1|,|\\delta_2|\\leq \\epsilon_{\\rm M}\\). Ignoring the rounding error in dividing by \\(2h\\), we then have that \\[\n\\begin{aligned}\n&\\left| f'(x_1) - \\frac{\\text{fl}[f(x_1+h)] - \\text{fl}[f(x_1-h)]}{2h}\\right| \\\\ = &\\left| - \\frac{f'''(\\xi)}{6}h^2 - \\frac{\\delta_1f(x_1+h) - \\delta_2f(x_1-h)}{2h}\\right|\\\\\n\\leq &\\frac{|f'''(\\xi)|}{6}h^2 + \\epsilon_{\\rm M}\\frac{|f(x_1+h)| + |f(x_1-h)|}{2h}\\\\\n\\leq &\\frac{h^2}{6}\\max_{[x_1-h, x_1+h]} |f'''(\\xi)| + \\frac{\\epsilon_{\\rm M}}{h}\\max_{[x_1-h, x_1+h]}|f(\\xi)|.\n\\end{aligned}\n\\] The first term is the truncation error, which tends to zero as \\(h\\to 0\\). But the second term is the rounding error, which tends to infinity as \\(h\\to 0\\).\n\nExample 4.4: Derivative of \\(f(x)=\\log(x)\\) at \\(x=2\\) againHere is a comparison of the terms in the inequality above (the red points are the left-hand side), shown on logarithmic scales, using \\(\\xi=2\\) to estimate the maxima.\n\n\n\n\n\nYou see that once \\(h\\) is small enough, rounding error takes over and the error in the computed derivative starts to increase again.\n\n\n\n\n4.1.4 Richardson extrapolation\nFinding higher-order formulae by differentiating Lagrange polynomials is tedious, and there is a simpler trick to obtain higher-order formulae, called Richardson extrapolation.\nWe begin from the central-difference formula. Since we will use formulae with different \\(h\\), let us define the notation \\[\nD_h := \\frac{f(x_1+h) - f(x_1-h)}{2h}.\n\\]\nNow use Taylor’s theorem to expand more terms in the truncation error: \\[\n\\begin{aligned}\nf(x_1 \\pm h) = f(x_1) &\\pm f'(x_1)h + \\frac{f''(x_1)}{2}h^2 \\pm \\frac{f'''(x_1)}{3!}h^3 \\\\ &+ \\frac{f^{(4)}(x_1)}{4!}h^4 \\pm \\frac{f^{(5)}(x_1)}{5!}h^5 + {\\cal O}(h^6).\n\\end{aligned}\n\\] Substituting into the formula for \\(D_h\\), the even powers of \\(h\\) cancel and we get \\[\n\\begin{aligned}\nD_h &= \\frac{1}{2h}\\Big(2f'(x_1)h + 2f'''(x_1)\\frac{h^3}{6} +   2f^{(5)}(x_1)\\frac{h^5}{120} + {\\cal O}(h^7)  \\Big)\\\\\n&=f'(x_1) + f'''(x_1)\\frac{h^2}{6} + f^{(5)}(x_1)\\frac{h^4}{120} + {\\cal O}(h^6).\n\\end{aligned}\n\\]\n\n\n\n\n\n\nYou may not have seen the big-Oh notation. When we write \\(f(x) = {\\cal O}(g(x))\\), we mean \\[\n\\lim_{x\\to 0}\\frac{|f(x)|}{|g(x)|} \\leq M &lt; \\infty.\n\\] So the error is \\({\\cal O}(h^6)\\) if it gets smaller at least as fast as \\(h^6\\) as \\(h\\to 0\\) (essentially, it contains no powers of \\(h\\) less than 6).\n\n\n\nThe leading term in the error here has the same coefficient \\(h^2/6\\) as the truncation error we derived earlier, although we have now expanded the error to higher powers of \\(h\\).\nThe trick is to apply the same formula with different step-sizes, typically \\(h\\) and \\(h/2\\): \\[\n\\begin{aligned}\nD_h &= f'(x_1) + f'''(x_1)\\frac{h^2}{6} + f^{(5)}(x_1)\\frac{h^4}{120} + {\\cal O}(h^6),\\\\\nD_{h/2} &= f'(x_1) + f'''(x_1)\\frac{h^2}{2^2(6)} + f^{(5)}(x_1)\\frac{h^4}{2^4(120)} + {\\cal O}(h^6).\n\\end{aligned}\n\\] We can then eliminate the \\(h^2\\) term by simple algebra: \\[\n\\begin{aligned}\nD_h - 2^2D_{h/2} =& -3f'(x_1) + \\left(1 - \\frac{2^2}{2^4}\\right)f^{(5)}(x_1)\\frac{h^4}{120} + {\\cal O}(h^6),\\\\\n\\implies \\quad D_h^{(1)} :=& \\frac{2^2D_{h/2} - D_h}{3} = f'(x_1) - f^{(5)}(x_1)\\frac{h^4}{480} + {\\cal O}(h^6).\n\\end{aligned}\n\\] The new formula \\(D_h^{(1)}\\) is 4th-order accurate.\n\nExample 4.5: Derivative of \\(f(x)=\\log(x)\\) at \\(x=2\\) (central difference).\n\n\n\n\n\n\n\n\n\n\n\\(h\\)\n\\(D_h\\)\nError\n\\(D_h^{(1)}\\)\nError\n\n\n\n\n1.0\n0.5493061443\n0.04930614433\n0.4979987836\n0.00200121642\n\n\n0.1\n0.5004172928\n0.00041729278\n0.4999998434\n1.56599487e-07\n\n\n0.01\n0.5000041667\n4.16672916e-06\n0.5000000000\n1.56388791e-11\n\n\n0.001\n0.5000000417\n4.16666151e-08\n0.5000000000\n9.29256672e-14\n\n\n\n\n\nIn fact, we could have applied this Richardson extrapolation procedure without knowing the coefficients of the error series. If we have some general order-\\(n\\) approximation \\[\nD_h = f'(x) + Ch^n + {\\cal O}(h^{n+1}),\n\\] then we can always evaluate it with \\(h/2\\) to get \\[\nD_{h/2} = f'(x) + C\\frac{h^n}{2^n} + {\\cal O}(h^{n+1})\n\\] and then eliminate the \\(h^n\\) term to get a new approximation \\[\nD_h^{(1)} := \\frac{2^nD_{h/2} - D_h}{2^n - 1} = f'(x) + {\\cal O}(h^{n+1}).\n\\]\n\n\n\n\n\n\nThe technique is used not only in differentiation but also in Romberg integration and the Bulirsch-Stoer method for solving ODEs.\n\n\n\n\n\n\n\n\n\nThere is nothing special about taking \\(h/2\\); we could have taken \\(h/3\\) or even \\(2h\\), and modified the formula accordingly. But \\(h/2\\) is usually convenient.\n\n\n\nFurthermore, Richardson extrapolation can be applied iteratively. In other words, we can now combine \\(D_{h}^{(1)}\\) and \\(D_{h/2}^{(1)}\\) to get an even higher order approximation \\(D_h^{(2)}\\), and so on.\n\nExample 4.6: Iterated Richardson extrapolation for central differences.From \\[\n\\begin{aligned}\nD_h^{(1)} &= f'(x_1) + C_1h^4 + {\\cal O}(h^6),\\\\\nD_{h/2}^{(1)} &= f'(x_1) + C_1\\frac{h^4}{2^4} + {\\cal O}(h^6),\n\\end{aligned}\n\\] we can eliminate the \\(h^4\\) term to get the 6th-order approximation \\[\nD_h^{(2)} := \\frac{2^4D_{h/2}^{(1)} - D_h^{(1)}}{2^4 - 1}.\n\\]\n\n\n\n\n\n\n\n\nLewis Fry Richardson (1881–1953) was from Newcastle and an undergraduate there. He was the first person to apply mathematics (finite differences) to weather prediction, and was ahead of his time: in the absence of electronic computers, he estimated that 60,000 people would be needed to predict the next day’s weather!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculus</span>"
    ]
  },
  {
    "objectID": "chap-four.html#s-int",
    "href": "chap-four.html#s-int",
    "title": "4  Calculus",
    "section": "4.2 Numerical integration",
    "text": "4.2 Numerical integration\n\nHow do we calculate integrals numerically?\n\nThe definite integral \\[\nI(f) := \\int_a^b f(x)\\,\\mathrm{d}x\n\\] can usually not be evaluated in closed form. To approximate it numerically, we can use a quadrature formula \\[\nI_n(f) := \\sum_{k=0}^n \\sigma_k f(x_k),\n\\] where \\(x_0,\\ldots,x_n\\) are a set of nodes and \\(\\sigma_0,\\ldots,\\sigma_n\\) are a set of corresponding weights.\n\n\n\n\n\n\nThe nodes are also known as quadrature points or abscissas, and the weights as coefficients.\n\n\n\n\nExample 4.7: The trapezium rule\\[\nI_1(f) = \\frac{b-a}{2}\\Big( f(a) + f(b) \\Big).\n\\]\nThis is the quadrature formula above with \\(x_0=a\\), \\(x_1=b\\), \\(\\sigma_0=\\sigma_1=\\tfrac12(b-a)\\).\nFor example, with \\(a=0\\), \\(b=2\\), \\(f(x)=\\mathrm{e}^x\\), we get \\[\nI_1(f) = \\frac{2 - 0}{2}\\big(\\mathrm{e}^0 + \\mathrm{e}^{2}\\big) = 8.389 \\quad \\textrm{to 4 s.f.}\n\\] The exact answer is \\[\nI(f) = \\int_0^{2}\\mathrm{e}^x\\,\\mathrm{d}x = \\mathrm{e}^{2} - \\mathrm{e}^0 = 6.389 \\quad \\textrm{to 4 s.f.}\n\\] Graphically, \\(I_1(f)\\) measures the area under the straight line that interpolates \\(f\\) at the ends:\n\n\n\n\n\n\n\n\n4.2.1 Newton-Cotes formulae\nWe can derive a family of “interpolatory” quadrature formulae by integrating interpolating polynomials of different degrees. We will also get error estimates using the interpolation error theorem.\nLet \\(x_0, \\ldots, x_n \\in [a,b]\\), where \\(x_0 &lt; x_1 &lt; \\cdots &lt; x_n\\), be a set of \\(n+1\\) nodes, and let \\(p_n\\in{\\cal P}_n\\) be the polynomial that interpolates \\(f\\) at these nodes. This may be written in Lagrange form as \\[\np_n(x) = \\sum_{k=0}^n f(x_k)\\ell_k(x), \\quad \\textrm{where}\\quad \\ell_k(x) = \\prod_{\\substack{j=0\\\\j\\neq k}}^n\\frac{x - x_j}{x_k - x_j}.\n\\] To approximate \\(I(f)\\), we integrate \\(p_n(x)\\) to define the quadrature formula \\[\nI_n(f) := \\int_a^b\\sum_{k=0}^n f(x_k)\\ell_k(x)\\,\\mathrm{d}x = \\sum_{k=0}^n f(x_k)\\int_a^b\\ell_k(x)\\,\\mathrm{d}x.\n\\] In other words, \\[\nI_n(f) := \\sum_{k=0}^n\\sigma_k f(x_k), \\quad \\textrm{where} \\quad \\sigma_k = \\int_a^b\\ell_k(x)\\,\\mathrm{d}x.\n\\] When the nodes are equidistant, this is called a Newton-Cotes formula. If \\(x_0=a\\) and \\(x_n=b\\), it is called a closed Newton-Cotes formula.\n\n\n\n\n\n\nAn open Newton-Cotes formula has nodes \\(x_i = a + (i+1)h\\) for \\(h = (b-a)/(n+2)\\).\n\n\n\n\nExample 4.8: Trapezium ruleThis is the closed Newton-Cotes formula with \\(n=1\\). To see this, let \\(x_0=a\\), \\(x_1=b\\). Then \\[\n\\begin{aligned}\n\\ell_0(x) = \\frac{x-b}{a-b} \\implies \\sigma_0 &= \\int_a^b\\ell_0(x)\\,\\mathrm{d}x \\\\ &= \\frac{1}{a-b}\\int_a^b(x-b)\\,\\mathrm{d}x \\\\ &= \\frac{1}{2(a-b)}(x-b)^2\\big|_a^b \\\\ &= \\frac{b-a}{2},\n\\end{aligned}\n\\] and \\[\n\\begin{aligned}\n\\ell_1(x) = \\frac{x-a}{b-a} \\implies \\sigma_1 &= \\int_a^b\\ell_1(x)\\,\\mathrm{d}x \\\\ &= \\frac{1}{b-a}\\int_a^b(x-a)\\,\\mathrm{d}x \\\\ &= \\frac{1}{2(b-a)}(x-a)^2\\big|_a^b = \\frac{b-a}{2}.\n\\end{aligned}\n\\] This gives \\[\nI_1(f) = \\sigma_0f(a) + \\sigma_1f(b) = \\frac{b-a}{2}\\big(f(a) + f(b)\\big).\n\\]\n\n\n\nTheorem 4.1Let \\(f\\) be continuous on \\([a,b]\\) with \\(n+1\\) continuous derivatives on \\((a,b)\\). Then the Newton-Cotes formula above satisfies the error bound \\[\n\\begin{aligned}\n&\\big|I(f) - I_n(f)\\big| \\leq \\\\ &\\frac{\\max_{\\xi\\in[a,b]}|f^{(n+1)}(\\xi)|}{(n+1)!}\\int_a^b\\big|(x-x_0)(x-x_1)\\cdots(x-x_{n}) \\big|\\,\\mathrm{d}x.\n\\end{aligned}\n\\]\n\n\nProof:\nFirst note that the error in the Newton-Cotes formula may be written \\[\n\\begin{aligned}\n\\big|I(f) - I_n(f)\\big| &= \\left|\\int_a^bf(x)\\,\\mathrm{d}x - \\int_a^bp_n(x)\\,\\mathrm{d}x \\right| \\\\\n&= \\left| \\int_a^b\\big[f(x) - p_n(x)\\big]\\,\\mathrm{d}x\\right| \\\\\n&\\leq \\int_a^b\\big|f(x) - p_n(x)\\big|\\,\\mathrm{d}x.\n\\end{aligned}\n\\] Now recall the interpolation error theorem, which says that, for each \\(x\\in[a,b]\\), we can write \\[\nf(x) - p_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)(x-x_1)\\cdots(x-x_{n})\n\\] for some \\(\\xi\\in(a,b)\\). The theorem simply follows by inserting this into the inequality above. \\(\\Box\\)\n\nExample 4.9: Trapezium rule errorLet \\(M_2 = \\max_{\\xi\\in[a,b]}|f''(\\xi)|\\). Here the theorem reduces to \\[\n\\begin{aligned}\n\\big|I(f) - I_1(f)\\big| &\\leq \\frac{M_2}{(1+1)!}\\int_a^b\\big|(x-a)(x-b)\\big|\\,\\mathrm{d}x \\\\ &= \\frac{M_2}{2!}\\int_a^b(x-a)(b-x)\\,\\mathrm{d}x \\\\ &= \\frac{(b-a)^3}{12}M_2.\n\\end{aligned}\n\\] For our earlier example with \\(a=0\\), \\(b=2\\), \\(f(x)=\\mathrm{e}^x\\), the estimate gives \\[\n\\big|I(f) - I_1(f)\\big| \\leq \\tfrac1{12}(2^3)\\mathrm{e}^{2} \\approx 4.926.\n\\] This is an overestimate of the actual error which was \\(\\approx 2.000\\).\n\n\nThe theorem suggests that the accuracy of \\(I_n\\) is limited both by the smoothness of \\(f\\) (outside our control) and by the location of the nodes \\(x_k\\). If the nodes are free to be chosen, then we can use Gaussian integration (more to follow on that topic).\n\n\n\n\n\n\nAs with interpolation, taking a high \\(n\\) is not usually a good idea. One can prove for the closed Newton-Cotes formula that \\[\n\\sum_{k=0}^n|\\sigma_k| \\to \\infty \\quad \\textrm{as} \\quad n\\to\\infty.\n\\] This makes the quadrature vulnerable to rounding errors for large \\(n\\).\n\n\n\n\n\n4.2.2 Composite Newton-Cotes formulae\nSince the Newton-Cotes formulae are based on polynomial interpolation at equally-spaced points, the results do not converge as the number of nodes increases. A better way to improve accuracy is to divide the interval \\([a,b]\\) into \\(m\\) subintervals \\([x_{i-1},x_i]\\) of equal length \\[\nh := \\frac{b-a}{m},\n\\] and use a Newton-Cotes formula of small degree \\(n\\) on each subinterval.\n\nExample 4.10: Composite trapezium ruleApplying the trapezium rule \\(I_1(f)\\) on each subinterval gives \\[\n\\begin{aligned}\nC_{1,m}(f) &= \\frac{h}{2}\\left[f(x_0) + f(x_1) + f(x_1) + f(x_2) + \\ldots + f(x_{m-1}) + f(x_m) \\right],\\\\\n&= h\\left[\\tfrac12 f(x_0) + f(x_1) + f(x_2) + \\ldots + f(x_{m-1}) + \\tfrac12 f(x_m) \\right].\n\\end{aligned}\n\\] We are effectively integrating a piecewise-linear approximation of \\(f(x)\\); here we show \\(m=3\\) for our test problem \\(f(x)=\\mathrm{e}^x\\) on \\([0,2]\\):\n\n\n\n\n\nLook at what happens as we increase \\(m\\) for our test problem:\n\n\n\n\\(m\\)\n\\(h\\)\n\\(C_{1,m}(f)\\)\n\\(|I(f) - C_{1,m}(f)|\\)\n\n\n\n\n1\n2\n8.389\n2.000\n\n\n2\n1\n6.912\n0.524\n\n\n4\n0.5\n6.522\n0.133\n\n\n8\n0.25\n6.422\n0.033\n\n\n16\n0.125\n6.397\n0.008\n\n\n32\n0.0625\n6.391\n0.002\n\n\n\nWhen we halve the sub-interval \\(h\\), the error goes down by a factor \\(4\\), suggesting that we have quadratic convergence, i.e., \\({\\cal O}(h^2)\\).\nTo show this theoretically, we can apply the Newton-Cotes error theorem in each subinterval. In \\([x_{i-1},x_i]\\) we have \\[\n\\big|I(f) - I_1(f)\\big| \\leq \\frac{\\max_{\\xi\\in[x_{i-1},x_i]}|f''(\\xi)|}{2!}\\int_{x_{i-1}}^{x_i}\\big|(x-x_{i-1})(x-x_i)\\big|\\,\\mathrm{d}x\n\\] Note that \\[\n\\begin{aligned}\n\\int_{x_{i-1}}^{x_i}\\big|(x-x_{i-1})(x-x_i)\\big|\\,\\mathrm{d}x &= \\int_{x_{i-1}}^{x_i}(x-x_{i-1})(x_i-x)\\,\\mathrm{d}x \\\\\n&= \\int_{x_{i-1}}^{x_i}\\big[-x^2 + (x_{i-1} + x_i)x - x_{i-1}x_i\\big]\\,\\mathrm{d}x\\\\\n&= \\big[-\\tfrac13x^3 + \\tfrac12(x_{i-1}+x_i)x^2 - x_{i-1}x_ix\\big]_{x_{i-1}}^{x_i} \\\\\n&= \\tfrac16 x_{i}^3 - \\tfrac12x_{i-1}x_i^2 + \\tfrac12x_{i-1}^2x_i - \\tfrac16x_{i-1}^3\\\\\n&= \\tfrac16(x_i - x_{i-1})^3 = \\tfrac16 h^3.\n\\end{aligned}\n\\] So overall \\[\n\\begin{aligned}\n\\big| I(f) - C_{1,m}(f) \\big| &\\leq \\tfrac12 \\max_i\\left(\\max_{\\xi\\in[x_{i-1},x_i]}|f''(\\xi)|\\right) m\\frac{h^3}{6}\\\\\n  &= \\frac{mh^3}{12}\\max_{\\xi\\in[a,b]}|f''(\\xi)| =  \\frac{b-a}{12}h^2\\max_{\\xi\\in[a,b]}|f''(\\xi)|.\n\\end{aligned}\n\\] As long as \\(f\\) is sufficiently smooth, this shows that the composite trapezium rule will converge as \\(m\\to\\infty\\). Moreover, the convergence will be \\({\\cal O}(h^2)\\).\n\n\n\n\n4.2.3 Exactness\nFrom the Newton-Cotes error theorem, we see that the Newton-Cotes formula \\(I_n(f)\\) will give the exact answer if \\(f^{(n+1)}=0\\). In other words, it will be exact if \\(f \\in{\\cal P}_n\\).\n\nExample 4.11The trapezium rule \\(I_1(f)\\) is exact for all linear polynomials \\(f\\in{\\cal P}_1\\).\n\n\nThe degree of exactness of a quadrature formula is the largest integer \\(n\\) for which the formula is exact for all polynomials in \\({\\cal P}_n\\).\nTo check whether a quadrature formula has degree of exactness \\(n\\), it suffices to check whether it is exact for the basis \\(1\\), \\(x\\), \\(x^2, \\ldots, x^n\\).\n\nExample 4.12: Simpson’s ruleThis is the \\(n=2\\) closed Newton-Cotes formula \\[\nI_2(f) = \\frac{b-a}{6}\\left[f(a) + 4f\\left(\\frac{a+b}{2}\\right) + f(b)\\right],\n\\] derived by integrating a quadratic interpolating polynomial. Let us find its degree of exactness: \\[\n\\begin{aligned}\nI(1) &= \\int_a^b\\,\\mathrm{d}x = (b-a), \\\\\nI_2(1) &= \\frac{b-a}{6}[1 + 4 + 1] = b-a = I(1),\\\\\nI(x) &= \\int_a^b x\\,\\mathrm{d}x = \\frac{b^2-a^2}{2}, \\\\\nI_2(x) &= \\frac{b-a}{6}\\left[a + 2(a+b) + b\\right] = \\frac{(b-a)(b+a)}{2} = I(x),\\\\\nI(x^2) &= \\int_a^b x^2\\,\\mathrm{d}x = \\frac{b^3 - a^3}{3}, \\\\\nI_2(x^2) &= \\frac{b-a}{6}\\left[a^2 + (a+b)^2 + b^2\\right] = \\frac{2(b^3 - a^3)}{6} = I(x^2),\\\\\nI(x^3) &= \\int_a^b x^3\\,\\mathrm{d}x = \\frac{b^4 - a^4}{4}, \\\\\nI_2(x^3) &= \\frac{b-a}{6}\\left[a^3 + \\tfrac12(a+b)^3 + b^3 \\right] = \\frac{b^4-a^4}{4} = I(x^3).\n\\end{aligned}\n\\] This shows that the degree of exactness is at least 3 (contrary to what might be expected from the interpolation picture). You can verify that \\(I_2(x^4)\\neq I(x^4)\\), so the degree of exactness is exactly 3.\n\n\nThis shows that the term \\(f'''(\\xi)\\) in the error formula for Simpson’s rule is misleading. In fact, it is possible to write an error bound proportional to \\(f^{(4)}(\\xi)\\).\nIn terms of degree of exactness, Simpson’s formula does better than expected. In general, Newton-Cotes formulae with even \\(n\\) have degree of exactness \\(n+1\\). But this is by no means the highest possible (see next section).\n\n\n4.2.4 Gaussian quadrature\nThe idea of Gaussian quadrature is to choose not only the weights \\(\\sigma_k\\) but also the nodes \\(x_k\\), in order to achieve the highest possible degree of exactness.\nFirstly, we will illustrate the brute force method of undetermined coefficients.\n\nExample 4.13: Gaussian quadrature formula \\(G_1(f)=\\sum_{k=0}^1\\sigma_kf(x_k)\\) on the interval \\([-1,1]\\)Here we have four unknowns \\(x_0\\), \\(x_1\\), \\(\\sigma_0\\) and \\(\\sigma_1\\), so we can impose four conditions: \\[\n\\begin{aligned}\nG_1(1) &= I(1) \\implies \\sigma_0 + \\sigma_1 = \\int_{-1}^1\\,\\mathrm{d}x = 2,\\\\\nG_1(x) &= I(x) \\implies \\sigma_0x_0 + \\sigma_1x_1 = \\int_{-1}^1x\\,\\mathrm{d}x = 0,\\\\\nG_1(x^2) &= I(x^2) \\implies \\sigma_0x_0^2 + \\sigma_1x_1^2 = \\int_{-1}^1x^2\\,\\mathrm{d}x = \\tfrac23,\\\\\nG_1(x^3) &= I(x^3) \\implies \\sigma_0x_0^3 + \\sigma_1x_1^3 = \\int_{-1}^1x^3\\,\\mathrm{d}x = 0.\n\\end{aligned}\n\\] To solve this system, the symmetry suggests that \\(x_1=-x_0\\) and \\(\\sigma_0=\\sigma_1\\). This will automatically satisfy the equations for \\(x\\) and \\(x^3\\), leaving the two equations \\[\n2\\sigma_0 = 2, \\qquad 2\\sigma_0x_0^2 = \\tfrac23,\n\\] so that \\(\\sigma_0=\\sigma_1=1\\) and \\(x_1=-x_0 = 1/\\sqrt{3}\\). The resulting Gaussian quadrature formula is \\[\nG_1(f) = f\\left(-\\frac{1}{\\sqrt{3}}\\right) + f\\left(\\frac{1}{\\sqrt{3}}\\right).\n\\] This formula has degree of exactness 3.\n\n\nIn general, the Gaussian quadrature formula with \\(n\\) nodes will have degree of exactness \\(2n+1\\).\nThe method of undetermined coefficients becomes unworkable for larger numbers of nodes, because of the nonlinearity of the equations. A much more elegant method uses orthogonal polynomials. In addition to what we learned before, we will need the following result.\n\nTheorem 4.2If \\(\\{\\phi_0,\\phi_1,\\ldots,\\phi_n\\}\\) is a set of orthogonal polynomials on \\([a,b]\\) under the inner product \\((f,g) = \\int_a^b f(x)g(x)w(x)\\,\\mathrm{d}x\\) and \\(\\phi_k\\) is of degree \\(k\\) for each \\(k=0,1,\\ldots,n\\), then \\(\\phi_k\\) has \\(k\\) distinct real roots, and these roots lie in the interval \\([a,b]\\).\n\n\nProof:\nLet \\(x_1,\\ldots,x_j\\) be the points where \\(\\phi_k(x)\\) changes sign in \\([a,b]\\). If \\(j=k\\) then we are done. Otherwise, suppose \\(j&lt;k\\), and consider the polynomial \\[\nq_j(x) = (x-x_1)(x-x_2)\\cdots(x-x_j).\n\\] Since \\(q_j\\) has lower degree than \\(\\phi_k\\), they must be orthogonal, meaning \\[\n(q_j,\\phi_k)=0 \\implies \\int_a^b q_j(x)\\phi_k(x)w(x)\\,\\mathrm{d}x = 0.\n\\] On the other hand, notice that the product \\(q_j(x)\\phi_k(x)\\) cannot change sign in \\([a,b]\\), because each sign change in \\(\\phi_k(x)\\) is cancelled out by one in \\(q_j(x)\\). This means that \\[\n\\int_a^b q_j(x)\\phi_k(x)w(x)\\,\\mathrm{d}x \\neq 0,\n\\] which is a contradiction. \\(\\Box\\)\nRemarkably, these roots are precisely the optimum choice of nodes for a quadrature formula to approximate the (weighted) integral \\[\nI_{w}(f) = \\int_a^b f(x)w(x)\\,\\mathrm{d}x.\n\\]\n\nTheorem 4.3: Gaussian quadratureLet \\(\\phi_{n+1}\\) be a polynomial in \\({\\cal P}_{n+1}\\) that is orthogonal on \\([a,b]\\) to all polynomials in \\({\\cal P}_{n}\\), with respect to the weight function \\(w(x)\\). If \\(x_0,x_1,\\ldots,x_n\\) are the roots of \\(\\phi_{n+1}\\), then the quadrature formula \\[\nG_{n,w}(f) := \\sum_{k=0}^n\\sigma_kf(x_k), \\qquad \\sigma_k = \\int_a^b\\ell_k(x)w(x)\\,\\mathrm{d}x\n\\] approximates \\(I_w(f)\\) with degree of exactness \\({2n+1}\\) (the largest possible).\n\n\nLike Newton-Cotes, we see that Gaussian quadrature is based on integrating an interpolating polynomial, but now the nodes are the roots of an orthogonal polynomial, rather than equally spaced points.\n\nExample 4.14: Gaussian quadrature with \\(n=1\\) on \\([-1,1]\\) and \\(w(x)=1\\) (again)To find the nodes \\(x_0\\), \\(x_1\\), we need to find the roots of the orthogonal polynomial \\(\\phi_2(x)\\). For this inner product, we already computed this (Legendre polynomial) in Chapter 3, where we found \\[\n\\phi_2(x) = x^2 - \\tfrac13.\n\\] Thus the nodes are \\(x_0= -1/\\sqrt{3}, x_1=1/\\sqrt{3}\\). Integrating the Lagrange polynomials gives the corresponding weights \\[\n\\begin{aligned}\n\\sigma_0 &= \\int_{-1}^1\\ell_0(x)\\,\\mathrm{d}x = \\int_{-1}^1\\frac{x-\\tfrac1{\\sqrt{3}}}{-\\tfrac2{\\sqrt{3}}}\\,\\mathrm{d}x = -\\tfrac{\\sqrt{3}}{2}\\left[\\tfrac12x^2 - \\tfrac1{\\sqrt{3}}x\\right]_{-1}^1 = 1,\\\\\n\\sigma_1 &= \\int_{-1}^1\\ell_1(x)\\,\\mathrm{d}x = \\int_{-1}^1\\frac{x+\\tfrac1{\\sqrt{3}}}{\\tfrac2{\\sqrt{3}}}\\,\\mathrm{d}x = \\tfrac{\\sqrt{3}}{2}\\left[\\tfrac12x^2 + \\tfrac1{\\sqrt{3}}x\\right]_{-1}^1 = 1,\n\\end{aligned}\n\\] as before.\n\n\n\n\n\n\n\n\nUsing an appropriate weight function \\(w(x)\\) can be useful for integrands with a singularity, since we can incorporate this in \\(w(x)\\) and still approximate the integral with \\(G_{n,w}\\).\n\n\n\n\nExample 4.15: Gaussian quadrature for \\(\\int_0^1 \\cos(x)x^{-1/2}\\,\\mathrm{d}x\\), with \\(n=0\\)\nThis is a Fresnel integral, with exact value \\(1.80905\\ldots\\) Let us compare the effect of using an appropriate weight function.\n\nUnweighted quadrature (\\(w(x)\\equiv 1\\)). The orthogonal polynomial of degree 1 is \\[\n\\phi_1(x) = x - \\frac{\\int_0^1x\\,\\mathrm{d}x}{\\int_0^1\\,\\mathrm{d}x} = x-\\tfrac12 \\implies x_0=\\tfrac12.\n\\] The corresponding weight may be found by imposing \\(G_{0}(1)=I(1)\\), which gives \\(\\sigma_0=\\int_0^1\\,\\mathrm{d}x = 1\\). Then our estimate is \\[\nG_0\\left(\\frac{\\cos(x)}{\\sqrt{x}}\\right) = \\frac{\\cos\\left(\\tfrac12\\right)}{\\sqrt{\\tfrac12}} = 1.2411\\ldots\n\\]\nWeighted quadrature with \\(w(x) = x^{-1/2}\\). This time we get \\[\n\\phi_1(x) = x - \\frac{\\int_0^1x^{1/2}\\,\\mathrm{d}x}{\\int_0^1x^{-1/2}\\,\\mathrm{d}x} = x-\\frac{2/3}{2} \\implies x_0=\\tfrac13.\n\\] The corresponding weight is \\(\\sigma_0 = \\int_0^1x^{-1/2}\\,\\mathrm{d}x = 2\\), so the new estimate is the more accurate \\[\nG_{0,w}\\big(\\cos(x)\\big) = 2\\cos\\left(\\tfrac13\\right) = 1.8899\\ldots\n\\]\n\n\n\nProof:\nFirst, recall that any interpolatory quadrature formula based on \\(n+1\\) nodes will be exact for all polynomials in \\({\\cal P}_n\\) (this follows from the Newton-Cotes theorem, which can be modified to include the weight function \\(w(x)\\)). So in particular, \\(G_{n,w}\\) is exact for \\(p_n\\in{\\cal P}_n\\).\nNow let \\(p_{2n+1}\\in{\\cal P}_{2n+1}\\). The trick is to divide this by the orthogonal polynomial \\(\\phi_{n+1}\\) whose roots are the nodes. This gives \\[\np_{2n+1}(x) = \\phi_{n+1}(x)q_n(x) + r_n(x) \\quad \\textrm{for some} \\quad q_n,r_n \\in{\\cal P}_n.\n\\] Then \\[\n\\begin{aligned}\nG_{n,w}(p_{2n+1}) &= \\sum_{k=0}^n\\sigma_kp_{2n+1}(x_k) = \\sum_{k=0}^n\\sigma_k\\Big[\\phi_{n+1}(x_k)q_n(x_k) + r_n(x_k) \\Big] \\\\ &= \\sum_{k=0}^n\\sigma_kr_n(x_k) = I_w(r_n),\n\\end{aligned}\n\\] where we have used the fact that \\(G_{n,w}\\) is exact for \\(r_n\\in{\\cal P}_n\\). Now, since \\(q_n\\) has lower degree than \\(\\phi_{n+1}\\), it must be orthogonal to \\(\\phi_{n+1}\\), so \\[\nI_w(\\phi_{n+1}q_n) = \\int_a^b\\phi_{n+1}(x)q_n(x)w(x)\\,\\mathrm{d}x = 0\n\\] and hence \\[\n\\begin{aligned}\nG_{n,w}(p_{2n+1}) &= I_w(r_n) + 0= I_w(r_n) + I_w(\\phi_{n+1}q_n) \\\\ &= I_w( \\phi_{n+1}q_n + r_n) = I_w(p_{2n+1}).\n\\end{aligned}\n\\]\nUnlike Newton-Cotes formulae with equally-spaced points, it can be shown that \\(G_{n,w}(f)\\to I_w(f)\\) as \\(n\\to\\infty\\), for any continuous function \\(f\\). This follows (with a bit of analysis) from the fact that all of the weights \\(\\sigma_k\\) are positive, along with the fact that they sum to a fixed number \\(\\int_a^bw(x)\\,\\mathrm{d}x\\). For Newton-Cotes, the signed weights still sum to a fixed number, but \\(\\sum_{k=0}^n|\\sigma_k|\\to\\infty\\), which destroys convergence.\nNot surprisingly, we can derive an error formula that depends on \\(f^{(2n+2)}(\\xi)\\) for some \\(\\xi\\in(a,b)\\). To do this, we will need the following result from calculus.\n\nTheorem 4.4: Mean value theorem for integralsIf \\(f,g\\) are continuous on \\([a,b]\\) and \\(g(x)\\geq 0\\) for all \\(x\\in[a,b]\\), then there exists \\(\\xi\\in(a,b)\\) such that \\[\n\\int_a^bf(x)g(x)\\,\\mathrm{d}x = f(\\xi)\\int_a^bg(x)\\,\\mathrm{d}x.\n\\]\n\n\nProof:\nLet \\(m\\) and \\(M\\) be the minimum and maximum values of \\(f\\) on \\([a,b]\\), respectively. Since \\(g(x)\\geq 0\\), we have that \\[\nm\\int_a^bg(x)\\,\\mathrm{d}x \\leq \\int_a^bf(x)g(x)\\,\\mathrm{d}x \\leq M\\int_a^bg(x)\\,\\mathrm{d}x.\n\\] Now let \\(I=\\int_a^bg(x)\\,\\mathrm{d}x\\). If \\(I=0\\) then \\(g(x)\\equiv 0\\), so \\(\\int_a^bf(x)g(x)\\,\\mathrm{d}x=0\\) and the theorem holds for every \\(\\xi\\in(a,b)\\). Otherwise, we have \\[\nm \\leq \\frac{1}{I}\\int_a^bf(x)g(x)\\,\\mathrm{d}x \\leq M.\n\\] By the Intermediate Value Theorem, \\(f(x)\\) attains every value between \\(m\\) and \\(M\\) somewhere in \\((a,b)\\), so in particular there exists \\(\\xi\\in(a,b)\\) with \\[\nf(\\xi) = \\frac{1}{I}\\int_a^bf(x)g(x)\\,\\mathrm{d}x.\n\\]\n\nTheorem 4.5: Error estimate for Gaussian quadratureLet \\(\\phi_{n+1}\\in{\\cal P}_{n+1}\\) be monic and orthogonal on \\([a,b]\\) to all polynomials in \\({\\cal P}_n\\), with respect to the weight function \\(w(x)\\). Let \\(x_0,x_1,\\ldots,x_n\\) be the roots of \\(\\phi_{n+1}\\), and let \\(G_{n,w}(f)\\) be the Gaussian quadrature formula defined above. If \\(f\\) has \\(2n+2\\) continuous derivatives on \\((a,b)\\), then there exists \\(\\xi\\in(a,b)\\) such that \\[\nI_w(f) - G_{n,w}(f) = \\frac{f^{(2n+2)}(\\xi)}{(2n+2)!}\\int_a^b\\phi_{n+1}^2(x)w(x)\\,\\mathrm{d}x.\n\\]\n\n\nProof:\nA neat trick is to use Hermite interpolation. Since the \\(x_k\\) are distinct, there exists a unique polynomial \\(p_{2n+1}\\) such that \\[\np_{2n+1}(x_k) = f(x_k), \\quad p_{2n+1}'(x_k) = f'(x_k) \\quad \\textrm{for $k=0,\\ldots,n$}.\n\\] In addition (see problem sheet), there exists \\(\\lambda\\in(a,b)\\), depending on \\(x\\), such that \\[\nf(x) - p_{2n+1}(x) = \\frac{f^{(2n+2)}(\\lambda)}{(2n+2)!}\\prod_{i=0}^n(x-x_i)^2.\n\\] Now we know that \\((x-x_0)(x-x_1)\\cdots(x-x_n)=\\phi_{n+1}(x)\\), since we fixed \\(\\phi_{n+1}\\) to be monic. Hence \\[\n\\int_a^bf(x)w(x)\\,\\mathrm{d}x - \\int_a^bp_{2n+1}(x)w(x)\\,\\mathrm{d}x = \\int_a^b\\frac{f^{(2n+2)}(\\lambda)}{(2n+2)!}\\phi_{n+1}^2(x)w(x)\\,\\mathrm{d}x.\n\\] Now we know that \\(G_{n,w}\\) must be exact for \\(p_{2n+1}\\), so \\[\n\\int_a^bp_{2n+1}(x)w(x)\\,\\mathrm{d}x = G_{n,w}(p_{2n+1}) = \\sum_{k=0}^n\\sigma_kp_{2n+1}(x_k) = \\sum_{k=0}^n\\sigma_kf(x_k)=G_{n,w}(f).\n\\] For the right-hand side, we can’t take \\(f^{(2n+2)}(\\lambda)\\) outside the integral since \\(\\lambda\\) depends on \\(x\\). But \\(\\phi_{n+1}^2(x)w(x)\\geq 0\\) on \\([a,b]\\), so we can apply the mean value theorem for integrals and get \\[\nI_w(f) - G_{n,w}(f) = \\frac{f^{(2n+2)}(\\xi)}{(2n+2)!}\\int_a^b\\phi_{n+1}^2(x)w(x)\\,\\mathrm{d}x\n\\] for some \\(\\xi\\in(a,b)\\) that does not depend on \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Calculus</span>"
    ]
  }
]