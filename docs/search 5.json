[
  {
    "objectID": "chap-five.html",
    "href": "chap-five.html",
    "title": "5  Differential Equations",
    "section": "",
    "text": "5.1 Basic Concepts\n$$ \\def\\eb{\\boldsymbol{e}} \\def\\fb{\\boldsymbol{f}} \\def\\hb{\\boldsymbol{h}} \\def\\xb{\\boldsymbol{x}} \\def\\Rb{\\boldsymbol{R}} \\def\\Real{\\mathbb{R}} \\def\\bfzero{\\boldsymbol{0}} \\newcommand{\\ddy}[2]{\\frac{\\partial{#1}}{\\partial{#2}}} $$\nAlmost all differential equations which arise in mathematics and its applications do not have exact analytical solutions. This is a central motivation for numerical analysis, as well as the historical development of increasingly powerful computers. Most scientific computing languages have pre-built packages for solving differential equations quickly and accurately using numerical approximations, and there are entire classes of software packages designed to do this for specialised industries, such as aerospace or finance. The goal of this Chapter is to understand the fundamentals of numerical timestepping, so that you can understand properties of these numerical methods, and hence so you can choose which to use for a given problem.\nWe will focus on general systems of (n) first-order differential equations of the form \\[\n\\dot{\\mathbf{u}} = \\frac{\\mathrm{d}\\mathbf{u}}{\\mathrm{d}t} = \\mathbf{f}(t,\\mathbf{u}), \\quad \\mathbf{u}(t_0)=\\mathbf{u}_0 \\in \\mathbb{R}^n.\n\\] Many general classes of equations can be written in this form, and such systems also arise when discretising partial differential equations, as well as other kinds of models involving derivatives. When ((t,)=()) (i.e. does not depend on time) we call the system autonomous.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Differential Equations</span>"
    ]
  },
  {
    "objectID": "chap-five.html#basic-concepts",
    "href": "chap-five.html#basic-concepts",
    "title": "5  Differential Equations",
    "section": "",
    "text": "5.1.0.1 Preliminary ODE theory\nBefore discussing practical aspects of solving such equations, recall the basic existence and uniqueness theory. For ODEs this theory is not too complicated, provided the right-hand side is sufficiently well behaved.\n\nTheorem 5.1Picard–Lindelöf theorem.\nLet \\(\\mathcal{D}\\subset\\mathbb{R}\\times\\mathbb{R}^n\\) be an open rectangle with interior point \\((t_0,\\mathbf{u}_0)\\). Suppose \\(\\mathbf{f}:\\mathcal{D}\\to\\mathbb{R}^n\\) is continuous in \\(t\\) and Lipschitz continuous in \\(\\mathbf{u}\\) for all \\((t,\\mathbf{u})\\in\\mathcal{D}\\). Then there exists \\(\\varepsilon&gt;0\\) such that the initial-value problem above has a unique solution \\(\\mathbf{u}(t)\\) for \\(t\\in[t_0-\\varepsilon,t_0+\\varepsilon]\\).\n\n\nEssentially, this says that if () is sufficiently nice then the ODE has a unique solution for each initial condition. The result follows from the contraction-mapping theorem or via an iterative scheme. If () is globally Lipschitz (same Lipschitz constant for all (^n)) and continuous for all (t), then the solution exists for all (t). For autonomous systems, solution curves ((t)^n) cannot cross.\nWhile this is theoretical, the theorem is important: there are ODEs without solutions or with non-unique solutions, and for PDEs existence and uniqueness are often much harder (and can fail).\n\nExample 5.1\nThe ODE [ = , u(0)=(0)=0, ] equivalently the first-order system [ =v,=,u(0)=v(0)=0, ] is a classical example with non-unique solutions. For any (T&gt;0) there are infinitely many solutions given by [ u(t)=\n\\[\\begin{cases}\n0 & t&lt;T,\\\\[4pt]\n\\frac{1}{144}(t-T)^4 & t\\ge T,\n\\end{cases}\\]\n] in addition to the trivial solution (u(t)=0). This example (Norton’s Dome) has an interpretation in Newtonian mechanics that breaks causality, since the particle can spontaneously start moving after an arbitrary time. See https://arxiv.org/abs/1801.01719v3.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Differential Equations</span>"
    ]
  },
  {
    "objectID": "chap-five.html#finite-difference-methods",
    "href": "chap-five.html#finite-difference-methods",
    "title": "5  Differential Equations",
    "section": "5.2 Finite Difference Methods",
    "text": "5.2 Finite Difference Methods\nMoving to practical questions, we now consider how to approximate solutions to the general ODE system using a computer. We have already seen in the previous chapter how to approximate the derivative using what are known as finite differences. These approximations are the main ingredients we will use to develop our numerical approaches.\nThe most well-known numerical method is commonly referred to as the forward Euler method, which is given by taking the forward difference operator on the left-hand side of the ODE system and rearranging the equation to obtain: \\[\n\\mathbf{u}(t + \\Delta t) = \\mathbf{u}(t) + \\Delta t \\mathbf{f}(\\mathbf{u}(t)),\n\\] where we are now using \\(\\Delta t\\) to denote a small time step, rather than \\(h\\). This is essentially the same approximation for the gradient of a function illustrated in Chapter 4, except now for the solution of an ODE. We can then iterate this formula starting at the initial condition to find an approximate solution at an arbitrary time \\(t\\).\nWe can use a more natural notation for this approximation by taking \\(t \\approx n\\Delta t\\) and letting \\(\\mathbf{u}_n \\approx \\mathbf{u}(n\\Delta t)\\). The forward Euler method can then be written as the iteration: \\[\n\\mathbf{u}_{n+1} = \\mathbf{u}_n + \\Delta t \\mathbf{f}(\\mathbf{u}_n).\n\\]\nHow accurate is this method for approximating the true solution? How can we know that this method is convergent; that is, if we take \\(\\Delta t \\to 0\\), can we ensure that \\(\\mathbf{u}_n \\to \\mathbf{u}(t)\\)? These are more subtle issues compared to numerical differentiation, as here we are using previous approximations for each subsequent timestep, and the dynamics of these schemes can play important roles in determining convergence. Let’s consider a simple example to illustrate these ideas.\n\n5.2.1 The van der Pol Oscillator\nThe van der Pol oscillator is given by \\[\n\\frac{\\mathrm{d}^2 u}{\\mathrm{d} t^2} - C\\left(1-u^2 \\right )\\frac{\\mathrm{d} u}{\\mathrm{d} t} + u = 0,\n\\] which can be converted to the first-order system: \\[\n\\frac{\\mathrm{d} u}{\\mathrm{d} t} = v, \\quad \\frac{\\mathrm{d}v}{\\mathrm{d} t} = C\\left(1-u^2\\right)\\frac{\\mathrm{d} u}{\\mathrm{d} t} - u.\n\\]\nFor \\(C=0\\), this is the simple harmonic oscillator. For \\(C&gt;0\\), the extra term means that for \\(u&gt;1\\), the oscillations are damped, but for \\(u&lt;1\\), there is an extra force due to “negative damping” driving the system away from \\(u=0\\).\nWe can first solve the simple case of \\(C=0\\) using the forward Euler scheme. The code for this can be compared with a high-order scheme that MATLAB has built-in called ode45.\n% Forward Euler implementation for the simple harmonic oscillator\nim/Forward_Euler.m\nRunning this code and plotting the outputs, we obtain the following graph:\n\n\n\n\n\nThe inset shows that the first few steps of the method match the solution reasonably well. However, over time it appears that the error builds up, and the amplitudes grow (despite the correct solution having the same amplitude for all time). Reducing the timestep will improve this, but eventually the amplitude will always begin to grow, at a rate which will become approximately exponential.\nWe can see how this scheme behaves with the timestep more clearly by considering the nonlinear van der Pol oscillator with \\(C=3\\), shown below for four different choices of timestep \\(\\Delta t\\):\n\n\n\n\n\nWe observe convergence towards a similar solution profile as \\(\\Delta t\\) decreases. However, there is still a buildup of error as \\(t\\) increases, meaning that we will need to consider local errors over one timestep, as well as global errors over iterative schemes for many timesteps.\nWe can formalize these ideas in terms of the local truncation error (LTE), which is essentially how much the approximation given in the forward Euler method fails to exactly satisfy the ODE. We can compute this by substituting in the true solution, \\(\\mathbf{u}(t)\\), and using Taylor series expansions to determine the error.\n\nExample 5.2LTE of forward Euler\nExpanding \\(\\mathbf{u}(t)\\) in a Taylor series, we find: \\[\n\\mathbf{u}_{n+1} = \\mathbf{u}(t+\\Delta t) = \\mathbf{u}(t) + \\Delta t \\frac{\\mathrm{d}\\mathbf{u}}{\\mathrm{d}t} + O(\\Delta t^2) = \\mathbf{u}_n + \\Delta t \\mathbf{f}(\\mathbf{u}_n),\n\\] which implies that this method has an LTE of \\(O(\\Delta t^2)\\). Note, however, that we can do precisely the same calculation before rearranging (via the approximation of the derivative directly) as: \\[\n\\frac{\\mathbf{u}_{n+1} - \\mathbf{u}_n}{\\Delta t} = \\frac{\\mathbf{u}(t+\\Delta t) - \\mathbf{u}(t)}{\\Delta t} = \\frac{\\mathrm{d}\\mathbf{u}}{\\mathrm{d}t} + O(\\Delta t) = \\mathbf{f}(\\mathbf{u}(t)),\n\\] which implies an LTE of \\(O(\\Delta t)\\). These two definitions are both used, despite being somewhat inconsistent. We will adopt the former definition, which is sometimes called the single-step error.\n\n\nAn integration scheme which has an LTE of the form \\(O(\\Delta t)\\) or smaller is called consistent. Essentially, a consistent scheme is one where the approximations are equivalent to a collection of Taylor series approximations, and hence, subject to various smoothness assumptions, we expect to be able to make the error tend to \\(0\\) for small enough time steps. However, consistency is not enough to ensure that a numerical scheme converges to the analytical solution of the original ODE.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Differential Equations</span>"
    ]
  }
]